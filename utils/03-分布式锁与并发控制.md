# 分布式锁与并发控制

## 背景
在分布式系统中，多个节点可能同时访问共享资源，如果没有适当的同步机制，就会出现数据竞争、重复处理等问题。分布式锁是解决分布式环境下资源同步访问的重要手段，它确保在任意时刻只有一个节点能够访问特定的共享资源。随着微服务架构的普及，分布式锁在防止重复执行、保证数据一致性、实现分布式协调等方面发挥着关键作用。

## 核心原理

### 1. 分布式锁的基本特性

#### 互斥性（Mutual Exclusion）
- **定义**：在任意时刻，只能有一个客户端持有锁
- **实现**：通过原子操作确保锁的唯一性
- **挑战**：网络分区、节点故障可能导致锁状态不一致

#### 防死锁（Deadlock Prevention）
- **超时机制**：锁具有过期时间，防止持有者故障导致死锁
- **可重入性**：同一个客户端可以多次获取同一把锁
- **公平性**：按照请求顺序分配锁，避免饥饿

#### 容错性（Fault Tolerance）
- **高可用**：锁服务本身需要高可用，避免单点故障
- **一致性**：在网络分区等异常情况下保证锁状态一致
- **自动恢复**：节点故障后能够自动释放锁

### 2. 分布式锁的实现模式

#### 基于数据库的分布式锁
```sql
-- 创建锁表
CREATE TABLE distributed_locks (
    lock_name VARCHAR(255) PRIMARY KEY,
    holder VARCHAR(255) NOT NULL,
    acquired_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    INDEX idx_expires_at (expires_at)
);

-- 获取锁
INSERT INTO distributed_locks (lock_name, holder, expires_at) 
VALUES ('resource_lock', 'node_1', DATE_ADD(NOW(), INTERVAL 30 SECOND))
ON DUPLICATE KEY UPDATE 
    holder = CASE 
        WHEN expires_at < NOW() THEN VALUES(holder)
        ELSE holder 
    END,
    expires_at = CASE 
        WHEN expires_at < NOW() THEN VALUES(expires_at)
        ELSE expires_at 
    END;

-- 释放锁
DELETE FROM distributed_locks 
WHERE lock_name = 'resource_lock' AND holder = 'node_1';
```

```go
// Go实现数据库分布式锁
type DBDistributedLock struct {
    db       *sql.DB
    lockName string
    holder   string
    ttl      time.Duration
    acquired bool
}

func NewDBDistributedLock(db *sql.DB, lockName, holder string, ttl time.Duration) *DBDistributedLock {
    return &DBDistributedLock{
        db:       db,
        lockName: lockName,
        holder:   holder,
        ttl:      ttl,
    }
}

func (dl *DBDistributedLock) TryLock() error {
    expiresAt := time.Now().Add(dl.ttl)
    
    query := `
        INSERT INTO distributed_locks (lock_name, holder, expires_at) 
        VALUES (?, ?, ?)
        ON DUPLICATE KEY UPDATE 
            holder = CASE 
                WHEN expires_at < NOW() THEN VALUES(holder)
                ELSE holder 
            END,
            expires_at = CASE 
                WHEN expires_at < NOW() THEN VALUES(expires_at)
                ELSE expires_at 
            END
    `
    
    _, err := dl.db.Exec(query, dl.lockName, dl.holder, expiresAt)
    if err != nil {
        return err
    }
    
    // 检查是否成功获取锁
    var currentHolder string
    var currentExpires time.Time
    
    checkQuery := "SELECT holder, expires_at FROM distributed_locks WHERE lock_name = ?"
    err = dl.db.QueryRow(checkQuery, dl.lockName).Scan(&currentHolder, &currentExpires)
    if err != nil {
        return err
    }
    
    if currentHolder == dl.holder && currentExpires.After(time.Now()) {
        dl.acquired = true
        return nil
    }
    
    return errors.New("failed to acquire lock")
}

func (dl *DBDistributedLock) Unlock() error {
    if !dl.acquired {
        return errors.New("lock not acquired")
    }
    
    query := "DELETE FROM distributed_locks WHERE lock_name = ? AND holder = ?"
    result, err := dl.db.Exec(query, dl.lockName, dl.holder)
    if err != nil {
        return err
    }
    
    rowsAffected, _ := result.RowsAffected()
    if rowsAffected == 0 {
        return errors.New("lock not held by this holder")
    }
    
    dl.acquired = false
    return nil
}

// 自动续期
func (dl *DBDistributedLock) KeepAlive(ctx context.Context) {
    ticker := time.NewTicker(dl.ttl / 3) // 每1/3 TTL续期一次
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            if dl.acquired {
                dl.renewLock()
            }
        }
    }
}

func (dl *DBDistributedLock) renewLock() error {
    expiresAt := time.Now().Add(dl.ttl)
    query := "UPDATE distributed_locks SET expires_at = ? WHERE lock_name = ? AND holder = ?"
    
    result, err := dl.db.Exec(query, expiresAt, dl.lockName, dl.holder)
    if err != nil {
        return err
    }
    
    rowsAffected, _ := result.RowsAffected()
    if rowsAffected == 0 {
        dl.acquired = false
        return errors.New("lock lost")
    }
    
    return nil
}
```

#### 基于Redis的分布式锁
```go
// Redis分布式锁实现
type RedisDistributedLock struct {
    client   *redis.Client
    key      string
    value    string
    ttl      time.Duration
    acquired bool
    stopCh   chan struct{}
}

func NewRedisDistributedLock(client *redis.Client, key string, ttl time.Duration) *RedisDistributedLock {
    return &RedisDistributedLock{
        client: client,
        key:    key,
        value:  generateUniqueValue(), // UUID + 节点ID
        ttl:    ttl,
        stopCh: make(chan struct{}),
    }
}

// Lua脚本确保原子性
const lockScript = `
    if redis.call('get', KEYS[1]) == ARGV[1] then
        return redis.call('del', KEYS[1])
    else
        return 0
    end
`

const renewScript = `
    if redis.call('get', KEYS[1]) == ARGV[1] then
        return redis.call('expire', KEYS[1], ARGV[2])
    else
        return 0
    end
`

func (rl *RedisDistributedLock) TryLock() error {
    // 使用SET NX EX命令原子性获取锁
    result := rl.client.SetNX(context.Background(), rl.key, rl.value, rl.ttl)
    if result.Err() != nil {
        return result.Err()
    }
    
    if result.Val() {
        rl.acquired = true
        go rl.keepAlive() // 启动自动续期
        return nil
    }
    
    return errors.New("failed to acquire lock")
}

func (rl *RedisDistributedLock) Lock() error {
    for {
        err := rl.TryLock()
        if err == nil {
            return nil
        }
        
        // 等待一段时间后重试
        time.Sleep(100 * time.Millisecond)
    }
}

func (rl *RedisDistributedLock) Unlock() error {
    if !rl.acquired {
        return errors.New("lock not acquired")
    }
    
    close(rl.stopCh) // 停止续期
    
    // 使用Lua脚本确保只有锁的持有者才能释放
    result := rl.client.Eval(context.Background(), lockScript, []string{rl.key}, rl.value)
    if result.Err() != nil {
        return result.Err()
    }
    
    if result.Val().(int64) == 1 {
        rl.acquired = false
        return nil
    }
    
    return errors.New("lock not held by this client")
}

func (rl *RedisDistributedLock) keepAlive() {
    ticker := time.NewTicker(rl.ttl / 3)
    defer ticker.Stop()
    
    for {
        select {
        case <-rl.stopCh:
            return
        case <-ticker.C:
            if rl.acquired {
                rl.renewLock()
            }
        }
    }
}

func (rl *RedisDistributedLock) renewLock() error {
    ttlSeconds := int(rl.ttl.Seconds())
    result := rl.client.Eval(context.Background(), renewScript, []string{rl.key}, rl.value, ttlSeconds)
    
    if result.Err() != nil {
        return result.Err()
    }
    
    if result.Val().(int64) == 0 {
        rl.acquired = false
        return errors.New("lock lost")
    }
    
    return nil
}

func generateUniqueValue() string {
    hostname, _ := os.Hostname()
    return fmt.Sprintf("%s-%s-%d", hostname, uuid.New().String(), time.Now().UnixNano())
}
```

#### 基于etcd的分布式锁
```go
// etcd分布式锁实现
type EtcdDistributedLock struct {
    client  *clientv3.Client
    key     string
    lease   clientv3.Lease
    leaseID clientv3.LeaseID
    ctx     context.Context
    cancel  context.CancelFunc
    session *concurrency.Session
    mutex   *concurrency.Mutex
}

func NewEtcdDistributedLock(client *clientv3.Client, key string, ttl int64) (*EtcdDistributedLock, error) {
    session, err := concurrency.NewSession(client, concurrency.WithTTL(int(ttl)))
    if err != nil {
        return nil, err
    }
    
    mutex := concurrency.NewMutex(session, key)
    
    return &EtcdDistributedLock{
        client:  client,
        key:     key,
        session: session,
        mutex:   mutex,
    }, nil
}

func (el *EtcdDistributedLock) Lock() error {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    return el.mutex.Lock(ctx)
}

func (el *EtcdDistributedLock) TryLock() error {
    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
    defer cancel()
    
    return el.mutex.TryLock(ctx)
}

func (el *EtcdDistributedLock) Unlock() error {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    err := el.mutex.Unlock(ctx)
    if err != nil {
        return err
    }
    
    return el.session.Close()
}

// 手动实现etcd分布式锁（更底层的控制）
type ManualEtcdLock struct {
    client   *clientv3.Client
    key      string
    lease    clientv3.Lease
    leaseID  clientv3.LeaseID
    revision int64
    ctx      context.Context
    cancel   context.CancelFunc
}

func NewManualEtcdLock(client *clientv3.Client, key string, ttl int64) *ManualEtcdLock {
    ctx, cancel := context.WithCancel(context.Background())
    return &ManualEtcdLock{
        client: client,
        key:    key,
        lease:  clientv3.NewLease(client),
        ctx:    ctx,
        cancel: cancel,
    }
}

func (mel *ManualEtcdLock) Lock() error {
    // 创建租约
    leaseResp, err := mel.lease.Grant(mel.ctx, 30)
    if err != nil {
        return err
    }
    mel.leaseID = leaseResp.ID
    
    // 自动续租
    keepAlive, err := mel.lease.KeepAlive(mel.ctx, mel.leaseID)
    if err != nil {
        return err
    }
    
    go func() {
        for range keepAlive {
            // 处理续租响应
        }
    }()
    
    // 创建锁键
    lockKey := fmt.Sprintf("%s/%d", mel.key, mel.leaseID)
    
    // 事务性地创建锁
    txn := mel.client.Txn(mel.ctx)
    txn.If(clientv3.Compare(clientv3.CreateRevision(mel.key), "=", 0)).
        Then(clientv3.OpPut(lockKey, "", clientv3.WithLease(mel.leaseID))).
        Else(clientv3.OpGet(mel.key, clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByCreateRevision, clientv3.SortAscend)))
    
    resp, err := txn.Commit()
    if err != nil {
        return err
    }
    
    if resp.Succeeded {
        // 成功获取锁
        return nil
    }
    
    // 需要等待前面的锁释放
    return mel.waitForLock(lockKey)
}

func (mel *ManualEtcdLock) waitForLock(lockKey string) error {
    // 获取当前所有锁
    resp, err := mel.client.Get(mel.ctx, mel.key, clientv3.WithPrefix(), clientv3.WithSort(clientv3.SortByCreateRevision, clientv3.SortAscend))
    if err != nil {
        return err
    }
    
    // 找到自己在队列中的位置
    var waitKey string
    for i, kv := range resp.Kvs {
        if string(kv.Key) == lockKey {
            if i == 0 {
                // 自己是第一个，获得锁
                return nil
            }
            // 等待前一个锁释放
            waitKey = string(resp.Kvs[i-1].Key)
            break
        }
    }
    
    if waitKey == "" {
        return errors.New("lock key not found")
    }
    
    // 监听前一个锁的删除事件
    watchCh := mel.client.Watch(mel.ctx, waitKey)
    for watchResp := range watchCh {
        for _, event := range watchResp.Events {
            if event.Type == mvccpb.DELETE {
                return nil // 前一个锁被释放，获得锁
            }
        }
    }
    
    return errors.New("watch cancelled")
}

func (mel *ManualEtcdLock) Unlock() error {
    mel.cancel()
    _, err := mel.lease.Revoke(context.Background(), mel.leaseID)
    return err
}
```

### 3. Redlock算法
```go
// Redlock算法实现（Redis集群分布式锁）
type Redlock struct {
    clients []*redis.Client
    quorum  int
    drift   time.Duration
}

func NewRedlock(addrs []string) *Redlock {
    clients := make([]*redis.Client, len(addrs))
    for i, addr := range addrs {
        clients[i] = redis.NewClient(&redis.Options{Addr: addr})
    }
    
    return &Redlock{
        clients: clients,
        quorum:  len(addrs)/2 + 1, // 多数派
        drift:   2 * time.Millisecond, // 时钟漂移
    }
}

type RedlockResult struct {
    Success   bool
    Value     string
    ValidTime time.Duration
}

func (r *Redlock) Lock(key string, ttl time.Duration) *RedlockResult {
    value := generateUniqueValue()
    start := time.Now()
    
    // 尝试在所有Redis实例上获取锁
    var wg sync.WaitGroup
    results := make(chan bool, len(r.clients))
    
    for _, client := range r.clients {
        wg.Add(1)
        go func(c *redis.Client) {
            defer wg.Done()
            
            result := c.SetNX(context.Background(), key, value, ttl)
            results <- result.Err() == nil && result.Val()
        }(client)
    }
    
    wg.Wait()
    close(results)
    
    // 统计成功获取锁的实例数量
    successCount := 0
    for success := range results {
        if success {
            successCount++
        }
    }
    
    elapsed := time.Since(start)
    validTime := ttl - elapsed - r.drift
    
    // 检查是否满足Redlock条件
    if successCount >= r.quorum && validTime > 0 {
        return &RedlockResult{
            Success:   true,
            Value:     value,
            ValidTime: validTime,
        }
    }
    
    // 获取锁失败，释放已获取的锁
    r.unlock(key, value)
    
    return &RedlockResult{Success: false}
}

func (r *Redlock) unlock(key, value string) {
    script := `
        if redis.call('get', KEYS[1]) == ARGV[1] then
            return redis.call('del', KEYS[1])
        else
            return 0
        end
    `
    
    var wg sync.WaitGroup
    for _, client := range r.clients {
        wg.Add(1)
        go func(c *redis.Client) {
            defer wg.Done()
            c.Eval(context.Background(), script, []string{key}, value)
        }(client)
    }
    wg.Wait()
}

func (r *Redlock) Unlock(key, value string) {
    r.unlock(key, value)
}
```

## 技术亮点

### 1. 锁的公平性实现
```go
// 公平锁实现（基于队列）
type FairDistributedLock struct {
    client    *redis.Client
    queueKey  string
    lockKey   string
    clientID  string
    ttl       time.Duration
    acquired  bool
}

func NewFairDistributedLock(client *redis.Client, lockName, clientID string, ttl time.Duration) *FairDistributedLock {
    return &FairDistributedLock{
        client:   client,
        queueKey: lockName + ":queue",
        lockKey:  lockName + ":lock",
        clientID: clientID,
        ttl:      ttl,
    }
}

const fairLockScript = `
    -- 加入队列
    local queue_key = KEYS[1]
    local lock_key = KEYS[2]
    local client_id = ARGV[1]
    local ttl = tonumber(ARGV[2])
    local timestamp = tonumber(ARGV[3])
    
    -- 检查是否已经在队列中
    local score = redis.call('zscore', queue_key, client_id)
    if not score then
        redis.call('zadd', queue_key, timestamp, client_id)
    end
    
    -- 检查是否轮到自己
    local first = redis.call('zrange', queue_key, 0, 0)
    if #first > 0 and first[1] == client_id then
        -- 尝试获取锁
        local lock_result = redis.call('set', lock_key, client_id, 'NX', 'EX', ttl)
        if lock_result then
            -- 获取锁成功，从队列中移除
            redis.call('zrem', queue_key, client_id)
            return 1
        end
    end
    
    return 0
`

func (fl *FairDistributedLock) Lock() error {
    for {
        timestamp := time.Now().UnixNano()
        result := fl.client.Eval(context.Background(), fairLockScript, 
            []string{fl.queueKey, fl.lockKey}, 
            fl.clientID, int(fl.ttl.Seconds()), timestamp)
        
        if result.Err() != nil {
            return result.Err()
        }
        
        if result.Val().(int64) == 1 {
            fl.acquired = true
            return nil
        }
        
        // 等待一段时间后重试
        time.Sleep(100 * time.Millisecond)
    }
}
```

### 2. 可重入锁实现
```go
// 可重入分布式锁
type ReentrantDistributedLock struct {
    client    *redis.Client
    key       string
    value     string
    ttl       time.Duration
    count     int32
    mu        sync.Mutex
}

const reentrantLockScript = `
    local key = KEYS[1]
    local value = ARGV[1]
    local ttl = tonumber(ARGV[2])
    
    local current = redis.call('hget', key, 'holder')
    if current == false then
        -- 锁不存在，获取锁
        redis.call('hset', key, 'holder', value)
        redis.call('hset', key, 'count', 1)
        redis.call('expire', key, ttl)
        return 1
    elseif current == value then
        -- 同一个持有者，增加计数
        local count = redis.call('hincrby', key, 'count', 1)
        redis.call('expire', key, ttl)
        return count
    else
        -- 其他持有者，获取失败
        return 0
    end
`

const reentrantUnlockScript = `
    local key = KEYS[1]
    local value = ARGV[1]
    
    local current = redis.call('hget', key, 'holder')
    if current == value then
        local count = redis.call('hincrby', key, 'count', -1)
        if count <= 0 then
            redis.call('del', key)
            return 1
        else
            return count
        end
    else
        return -1
    end
`

func (rl *ReentrantDistributedLock) Lock() error {
    rl.mu.Lock()
    defer rl.mu.Unlock()
    
    if rl.count > 0 {
        // 已经持有锁，直接增加计数
        atomic.AddInt32(&rl.count, 1)
        return nil
    }
    
    result := rl.client.Eval(context.Background(), reentrantLockScript,
        []string{rl.key}, rl.value, int(rl.ttl.Seconds()))
    
    if result.Err() != nil {
        return result.Err()
    }
    
    count := result.Val().(int64)
    if count > 0 {
        atomic.StoreInt32(&rl.count, int32(count))
        return nil
    }
    
    return errors.New("failed to acquire lock")
}

func (rl *ReentrantDistributedLock) Unlock() error {
    rl.mu.Lock()
    defer rl.mu.Unlock()
    
    if rl.count <= 0 {
        return errors.New("lock not held")
    }
    
    result := rl.client.Eval(context.Background(), reentrantUnlockScript,
        []string{rl.key}, rl.value)
    
    if result.Err() != nil {
        return result.Err()
    }
    
    count := result.Val().(int64)
    if count == -1 {
        return errors.New("lock not held by this client")
    }
    
    atomic.StoreInt32(&rl.count, int32(count))
    return nil
}
```

### 3. 读写锁实现
```go
// 分布式读写锁
type DistributedRWLock struct {
    client     *redis.Client
    key        string
    clientID   string
    ttl        time.Duration
    readCount  int32
    writeHeld  bool
    mu         sync.RWMutex
}

const readLockScript = `
    local key = KEYS[1]
    local client_id = ARGV[1]
    local ttl = tonumber(ARGV[2])
    
    -- 检查是否有写锁
    local write_lock = redis.call('hget', key, 'write_lock')
    if write_lock and write_lock ~= client_id then
        return 0
    end
    
    -- 增加读锁计数
    local read_count = redis.call('hincrby', key, 'read_count', 1)
    redis.call('hset', key, 'reader:' .. client_id, 1)
    redis.call('expire', key, ttl)
    
    return read_count
`

const writeLockScript = `
    local key = KEYS[1]
    local client_id = ARGV[1]
    local ttl = tonumber(ARGV[2])
    
    -- 检查是否有其他写锁
    local write_lock = redis.call('hget', key, 'write_lock')
    if write_lock and write_lock ~= client_id then
        return 0
    end
    
    -- 检查是否有读锁（除了自己的）
    local read_count = redis.call('hget', key, 'read_count')
    if read_count and tonumber(read_count) > 0 then
        local my_read = redis.call('hget', key, 'reader:' .. client_id)
        if not my_read or tonumber(read_count) > tonumber(my_read) then
            return 0
        end
    end
    
    -- 获取写锁
    redis.call('hset', key, 'write_lock', client_id)
    redis.call('expire', key, ttl)
    
    return 1
`

func (rw *DistributedRWLock) RLock() error {
    result := rw.client.Eval(context.Background(), readLockScript,
        []string{rw.key}, rw.clientID, int(rw.ttl.Seconds()))
    
    if result.Err() != nil {
        return result.Err()
    }
    
    if result.Val().(int64) > 0 {
        atomic.AddInt32(&rw.readCount, 1)
        return nil
    }
    
    return errors.New("failed to acquire read lock")
}

func (rw *DistributedRWLock) Lock() error {
    result := rw.client.Eval(context.Background(), writeLockScript,
        []string{rw.key}, rw.clientID, int(rw.ttl.Seconds()))
    
    if result.Err() != nil {
        return result.Err()
    }
    
    if result.Val().(int64) == 1 {
        rw.writeHeld = true
        return nil
    }
    
    return errors.New("failed to acquire write lock")
}
```

## 核心组件

### 1. 锁管理器
```go
// 分布式锁管理器
type LockManager struct {
    locks map[string]DistributedLock
    mu    sync.RWMutex
}

type DistributedLock interface {
    Lock() error
    TryLock() error
    Unlock() error
    IsLocked() bool
    GetHolder() string
}

func NewLockManager() *LockManager {
    return &LockManager{
        locks: make(map[string]DistributedLock),
    }
}

func (lm *LockManager) GetLock(name string, lockType string, config map[string]interface{}) (DistributedLock, error) {
    lm.mu.RLock()
    if lock, exists := lm.locks[name]; exists {
        lm.mu.RUnlock()
        return lock, nil
    }
    lm.mu.RUnlock()
    
    lm.mu.Lock()
    defer lm.mu.Unlock()
    
    // 双重检查
    if lock, exists := lm.locks[name]; exists {
        return lock, nil
    }
    
    // 创建新锁
    var lock DistributedLock
    var err error
    
    switch lockType {
    case "redis":
        lock, err = lm.createRedisLock(name, config)
    case "etcd":
        lock, err = lm.createEtcdLock(name, config)
    case "database":
        lock, err = lm.createDBLock(name, config)
    default:
        return nil, errors.New("unsupported lock type")
    }
    
    if err != nil {
        return nil, err
    }
    
    lm.locks[name] = lock
    return lock, nil
}

func (lm *LockManager) createRedisLock(name string, config map[string]interface{}) (DistributedLock, error) {
    client := config["client"].(*redis.Client)
    ttl := config["ttl"].(time.Duration)
    
    return NewRedisDistributedLock(client, name, ttl), nil
}

// 锁的健康检查
func (lm *LockManager) HealthCheck() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            lm.mu.RLock()
            for name, lock := range lm.locks {
                if !lock.IsLocked() {
                    // 清理未使用的锁
                    delete(lm.locks, name)
                }
            }
            lm.mu.RUnlock()
        }
    }
}
```

### 2. 锁监控和指标
```go
// 锁监控指标
type LockMetrics struct {
    AcquireCount    int64
    AcquireLatency  time.Duration
    HoldTime        time.Duration
    FailureCount    int64
    TimeoutCount    int64
    mu              sync.RWMutex
}

type MonitoredLock struct {
    lock    DistributedLock
    metrics *LockMetrics
    name    string
}

func NewMonitoredLock(lock DistributedLock, name string) *MonitoredLock {
    return &MonitoredLock{
        lock:    lock,
        metrics: &LockMetrics{},
        name:    name,
    }
}

func (ml *MonitoredLock) Lock() error {
    start := time.Now()
    
    err := ml.lock.Lock()
    
    ml.metrics.mu.Lock()
    ml.metrics.AcquireLatency = time.Since(start)
    if err != nil {
        ml.metrics.FailureCount++
    } else {
        ml.metrics.AcquireCount++
    }
    ml.metrics.mu.Unlock()
    
    return err
}

func (ml *MonitoredLock) Unlock() error {
    start := time.Now()
    
    err := ml.lock.Unlock()
    
    if err == nil {
        ml.metrics.mu.Lock()
        ml.metrics.HoldTime = time.Since(start)
        ml.metrics.mu.Unlock()
    }
    
    return err
}

func (ml *MonitoredLock) GetMetrics() LockMetrics {
    ml.metrics.mu.RLock()
    defer ml.metrics.mu.RUnlock()
    return *ml.metrics
}

// 锁状态监控
type LockMonitor struct {
    locks map[string]*MonitoredLock
    mu    sync.RWMutex
}

func (lm *LockMonitor) RegisterLock(name string, lock *MonitoredLock) {
    lm.mu.Lock()
    defer lm.mu.Unlock()
    lm.locks[name] = lock
}

func (lm *LockMonitor) GetReport() map[string]LockMetrics {
    lm.mu.RLock()
    defer lm.mu.RUnlock()
    
    report := make(map[string]LockMetrics)
    for name, lock := range lm.locks {
        report[name] = lock.GetMetrics()
    }
    
    return report
}

func (lm *LockMonitor) StartMonitoring() {
    ticker := time.NewTicker(1 * time.Minute)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            report := lm.GetReport()
            for name, metrics := range report {
                log.Printf("Lock %s: Acquires=%d, Failures=%d, AvgLatency=%v",
                    name, metrics.AcquireCount, metrics.FailureCount, metrics.AcquireLatency)
            }
        }
    }
}
```

## 使用场景

### 1. 防止重复执行
```go
// 防止定时任务重复执行
type ScheduledTask struct {
    name     string
    lockMgr  *LockManager
    task     func() error
    interval time.Duration
}

func NewScheduledTask(name string, lockMgr *LockManager, task func() error, interval time.Duration) *ScheduledTask {
    return &ScheduledTask{
        name:     name,
        lockMgr:  lockMgr,
        task:     task,
        interval: interval,
    }
}

func (st *ScheduledTask) Start() {
    ticker := time.NewTicker(st.interval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            st.executeWithLock()
        }
    }
}

func (st *ScheduledTask) executeWithLock() {
    lockName := fmt.Sprintf("task:%s", st.name)
    lock, err := st.lockMgr.GetLock(lockName, "redis", map[string]interface{}{
        "client": getRedisClient(),
        "ttl":    5 * time.Minute,
    })
    
    if err != nil {
        log.Printf("Failed to get lock: %v", err)
        return
    }
    
    err = lock.TryLock()
    if err != nil {
        log.Printf("Task %s is already running", st.name)
        return
    }
    
    defer func() {
        if unlockErr := lock.Unlock(); unlockErr != nil {
            log.Printf("Failed to unlock: %v", unlockErr)
        }
    }()
    
    log.Printf("Executing task %s", st.name)
    if err := st.task(); err != nil {
        log.Printf("Task %s failed: %v", st.name, err)
    } else {
        log.Printf("Task %s completed successfully", st.name)
    }
}
```

### 2. 分布式限流
```go
// 基于分布式锁的限流器
type DistributedRateLimiter struct {
    lockMgr   *LockManager
    key       string
    limit     int
    window    time.Duration
    client    *redis.Client
}

func NewDistributedRateLimiter(lockMgr *LockManager, client *redis.Client, key string, limit int, window time.Duration) *DistributedRateLimiter {
    return &DistributedRateLimiter{
        lockMgr: lockMgr,
        client:  client,
        key:     key,
        limit:   limit,
        window:  window,
    }
}

func (drl *DistributedRateLimiter) Allow() bool {
    lockName := fmt.Sprintf("ratelimit:%s", drl.key)
    lock, err := drl.lockMgr.GetLock(lockName, "redis", map[string]interface{}{
        "client": drl.client,
        "ttl":    1 * time.Second,
    })
    
    if err != nil {
        return false
    }
    
    err = lock.Lock()
    if err != nil {
        return false
    }
    
    defer lock.Unlock()
    
    // 使用滑动窗口算法
    now := time.Now().UnixNano()
    windowStart := now - drl.window.Nanoseconds()
    
    // 清理过期的请求记录
    drl.client.ZRemRangeByScore(context.Background(), drl.key, "0", fmt.Sprintf("%d", windowStart))
    
    // 检查当前窗口内的请求数量
    count := drl.client.ZCard(context.Background(), drl.key).Val()
    if int(count) >= drl.limit {
        return false
    }
    
    // 记录当前请求
    drl.client.ZAdd(context.Background(), drl.key, &redis.Z{
        Score:  float64(now),
        Member: fmt.Sprintf("%d", now),
    })
    
    // 设置过期时间
    drl.client.Expire(context.Background(), drl.key, drl.window)
    
    return true
}
```

### 3. 分布式缓存更新
```go
// 分布式缓存更新协调
type CacheUpdater struct {
    lockMgr *LockManager
    cache   Cache
    db      Database
}

type Cache interface {
    Get(key string) (interface{}, error)
    Set(key string, value interface{}, ttl time.Duration) error
    Delete(key string) error
}

type Database interface {
    Get(key string) (interface{}, error)
    Set(key string, value interface{}) error
}

func (cu *CacheUpdater) GetWithCacheAside(key string) (interface{}, error) {
    // 先从缓存获取
    value, err := cu.cache.Get(key)
    if err == nil {
        return value, nil
    }
    
    // 缓存未命中，使用分布式锁防止缓存击穿
    lockName := fmt.Sprintf("cache_update:%s", key)
    lock, err := cu.lockMgr.GetLock(lockName, "redis", map[string]interface{}{
        "client": getRedisClient(),
        "ttl":    10 * time.Second,
    })
    
    if err != nil {
        return nil, err
    }
    
    err = lock.Lock()
    if err != nil {
        // 获取锁失败，等待一下再从缓存读取
        time.Sleep(100 * time.Millisecond)
        return cu.cache.Get(key)
    }
    
    defer lock.Unlock()
    
    // 双重检查，可能其他线程已经更新了缓存
    value, err = cu.cache.Get(key)
    if err == nil {
        return value, nil
    }
    
    // 从数据库获取数据
    value, err = cu.db.Get(key)
    if err != nil {
        return nil, err
    }
    
    // 更新缓存
    cu.cache.Set(key, value, 5*time.Minute)
    
    return value, nil
}

func (cu *CacheUpdater) UpdateWithWriteThrough(key string, value interface{}) error {
    lockName := fmt.Sprintf("cache_write:%s", key)
    lock, err := cu.lockMgr.GetLock(lockName, "redis", map[string]interface{}{
        "client": getRedisClient(),
        "ttl":    5 * time.Second,
    })
    
    if err != nil {
        return err
    }
    
    err = lock.Lock()
    if err != nil {
        return err
    }
    
    defer lock.Unlock()
    
    // 先更新数据库
    err = cu.db.Set(key, value)
    if err != nil {
        return err
    }
    
    // 再更新缓存
    return cu.cache.Set(key, value, 5*time.Minute)
}
```

## 技术分析

### 优势
1. **简单易用**：提供统一的锁接口，屏蔽底层实现复杂性
2. **高可用性**：基于分布式存储，避免单点故障
3. **强一致性**：确保在分布式环境下的互斥访问
4. **自动过期**：防止死锁，提高系统健壮性
5. **可扩展性**：支持多种实现方式，可根据需求选择

### 挑战与限制
1. **性能开销**：网络通信和存储操作带来延迟
2. **复杂性**：实现正确的分布式锁需要考虑多种边界情况
3. **时钟依赖**：依赖系统时钟，时钟偏移可能影响正确性
4. **网络分区**：网络分区可能导致锁状态不一致
5. **调试困难**：分布式环境下的问题定位和调试较为复杂

### 最佳实践

#### 1. 锁的设计原则
- **最小锁粒度**：尽量减小锁的范围，提高并发性
- **合理超时**：设置合适的锁超时时间，平衡安全性和性能
- **幂等操作**：确保在锁保护下的操作是幂等的
- **快速失败**：获取锁失败时快速返回，避免长时间阻塞

#### 2. 错误处理
```go
func SafeLockOperation(lock DistributedLock, operation func() error) error {
    // 设置超时
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // 获取锁
    lockCh := make(chan error, 1)
    go func() {
        lockCh <- lock.Lock()
    }()
    
    select {
    case err := <-lockCh:
        if err != nil {
            return fmt.Errorf("failed to acquire lock: %w", err)
        }
    case <-ctx.Done():
        return fmt.Errorf("lock acquisition timeout")
    }
    
    // 确保释放锁
    defer func() {
        if unlockErr := lock.Unlock(); unlockErr != nil {
            log.Printf("Failed to unlock: %v", unlockErr)
        }
    }()
    
    // 执行操作
    return operation()
}
```

#### 3. 监控和告警
```go
func (lm *LockManager) SetupMonitoring() {
    // 锁获取延迟监控
    go func() {
        for {
            report := lm.GetReport()
            for name, metrics := range report {
                if metrics.AcquireLatency > 1*time.Second {
                    log.Printf("ALERT: Lock %s has high latency: %v", name, metrics.AcquireLatency)
                }
                
                if metrics.FailureCount > 10 {
                    log.Printf("ALERT: Lock %s has high failure rate: %d", name, metrics.FailureCount)
                }
            }
            time.Sleep(1 * time.Minute)
        }
    }()
}
```

## 面试常见问题

### 1. 分布式锁的实现方式有哪些？各有什么优缺点？
**回答要点**：
- **数据库锁**：简单易实现，但性能较差，存在单点故障风险
- **Redis锁**：性能好，实现简单，但需要考虑主从切换时的数据一致性
- **etcd锁**：强一致性，高可用，但相对复杂，性能不如Redis
- **Zookeeper锁**：强一致性，支持公平锁，但运维复杂度高

### 2. Redis分布式锁如何防止误删？
**回答要点**：
- **唯一标识**：每个锁设置唯一的value（UUID+节点ID）
- **Lua脚本**：使用Lua脚本确保检查和删除的原子性
- **示例代码**：
```go
const unlockScript = `
    if redis.call('get', KEYS[1]) == ARGV[1] then
        return redis.call('del', KEYS[1])
    else
        return 0
    end
`
```

### 3. 如何解决Redis主从切换导致的锁丢失问题？
**回答要点**：
- **Redlock算法**：在多个独立的Redis实例上获取锁，需要多数派成功
- **等待同步**：在主节点写入后等待一定时间确保同步到从节点
- **业务幂等**：确保业务操作是幂等的，即使锁丢失也不会造成问题
- **监控告警**：监控主从切换事件，及时发现问题

### 4. 分布式锁的性能如何优化？
**回答要点**：
- **减少网络往返**：使用Lua脚本减少网络通信次数
- **批量操作**：将多个锁操作合并为批量操作
- **本地缓存**：对于读多写少的场景，使用本地缓存减少远程调用
- **异步续期**：使用异步线程进行锁续期，避免阻塞业务逻辑
- **连接池**：使用连接池复用网络连接

### 5. 在Go语言中实现分布式锁需要注意什么？
**回答要点**：
```go
// 关键注意点

// 1. 并发安全
type SafeLock struct {
    mu sync.Mutex // 保护本地状态
    // ... 其他字段
}

// 2. 上下文控制
func (l *Lock) LockWithContext(ctx context.Context) error {
    select {
    case <-ctx.Done():
        return ctx.Err()
    default:
        return l.lock()
    }
}

// 3. 资源清理
func (l *Lock) Close() error {
    if l.acquired {
        return l.Unlock()
    }
    return nil
}

// 4. 错误处理
func (l *Lock) TryLock() error {
    if err := l.validate(); err != nil {
        return fmt.Errorf("lock validation failed: %w", err)
    }
    // ... 实现逻辑
}

// 5. 监控指标
func (l *Lock) recordMetrics(operation string, duration time.Duration, err error) {
    // 记录操作指标
}
```

### 6. 如何设计一个高可用的分布式锁服务？
**回答要点**：
- **多副本部署**：使用多个存储节点，避免单点故障
- **一致性协议**：使用Raft、Paxos等协议保证数据一致性
- **故障检测**：快速检测节点故障，及时进行故障转移
- **负载均衡**：合理分配锁请求，避免热点问题
- **监控告警**：全面的监控体系，及时发现和处理问题
- **降级策略**：在极端情况下的降级和恢复机制