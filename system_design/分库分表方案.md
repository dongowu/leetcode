# 分库分表方案设计

## 目录
- [概述与背景](#概述与背景)
- [底层原理](#底层原理)
- [技术实现](#技术实现)
- [解决的问题](#解决的问题)
- [方案设计](#方案设计)
- [MySQL实现细节](#mysql实现细节)
- [注意事项与坑点](#注意事项与坑点)
- [高频面试题](#高频面试题)
- [实战场景分析](#实战场景分析)
- [Go语言实现](#go语言实现)

## 概述与背景

### 什么是分库分表

分库分表是一种数据库水平扩展技术，通过将单一数据库的数据分散到多个数据库实例（分库）或将单一表的数据分散到多个表（分表）来解决单库单表的性能瓶颈。

### 实现背景

**业务发展驱动的技术需求：**

1. **数据量爆炸式增长**
   - 单表数据量超过千万级别
   - 存储空间不足
   - 索引效率急剧下降

2. **并发访问压力**
   - QPS/TPS超过单库承载能力
   - 连接数达到上限
   - 锁竞争激烈

3. **业务复杂度提升**
   - 多业务模块数据隔离需求
   - 不同业务的性能要求差异
   - 数据安全和合规要求

**技术演进路径：**
```
单库单表 → 读写分离 → 垂直分库 → 水平分库 → 分库分表
```

### 架构演进

**1. 从简单到复杂的演进路径：**

- **单体应用阶段：** 初期业务简单，数据量小，单库单表足以支撑。关注点在于快速迭代和功能实现。
- **读写分离阶段：** 随着读请求增多，数据库读压力增大，引入读写分离，将读请求分发到只读副本，提高并发能力。
- **垂直分库阶段：** 业务模块增多，不同模块数据耦合，通过垂直分库将不同业务的数据拆分到独立的数据库，实现业务解耦和数据隔离。
- **水平分库分表阶段：** 单一业务数据量持续增长，单表或单库性能达到瓶颈，引入水平分库分表，将数据分散到多个数据库和表，实现数据库的水平扩展。
- **分布式事务与数据一致性：** 随着分库分表的引入，跨库事务成为挑战，需要引入分布式事务解决方案（如2PC、TCC、SAGA）和最终一致性保障机制。
- **云原生与自动化运维：** 结合容器化（Docker）、容器编排（Kubernetes）和自动化运维工具，实现分库分表集群的弹性伸缩、高可用和自动化管理。

**2. 不同阶段的技术选型依据：**

- **业务需求：** 根据业务的QPS、TPS、数据量、并发量、数据一致性要求等核心指标进行技术选型。
- **团队能力：** 评估团队对特定技术的掌握程度和运维能力，选择团队熟悉且有能力驾驭的技术栈。
- **成本考量：** 综合考虑硬件成本、软件授权成本、人力成本和运维成本。
- **社区生态与成熟度：** 优先选择社区活跃、文档完善、有成功案例的成熟技术，降低风险。
- **可扩展性与可维护性：** 考虑未来业务发展，选择易于扩展和维护的架构和技术。

**3. 重构和优化的时机判断：**

- **性能瓶颈出现：** 当数据库CPU、IO、连接数等指标持续告警，或用户反馈响应时间变长时，是进行性能优化的时机。
- **业务快速增长：** 预判到业务即将迎来爆发式增长，提前进行架构升级和容量规划。
- **技术债务积累：** 当现有架构难以满足新业务需求，或维护成本过高时，考虑进行技术重构。
- **技术栈升级：** 当有更先进、更高效的技术出现，且能带来显著收益时，可以考虑技术栈升级。

**4. 技术债务的管理策略：**

- **定期评估：** 定期对系统进行技术债务评估，识别和量化技术债务。
- **优先级排序：** 根据业务影响、风险程度和修复成本，对技术债务进行优先级排序。
- **持续投入：** 将技术债务的偿还纳入日常开发计划，分配专门资源进行修复。
- **小步快跑：** 避免一次性大规模重构，采用小步迭代的方式逐步偿还债务。
- **文档化：** 详细记录技术债务的产生原因、影响和解决方案，避免重复犯错。
- **代码规范与Code Review：** 建立严格的代码规范和Code Review机制，从源头减少技术债务的产生。

## 底层原理

### 1. 分库原理

**垂直分库（按业务模块）：**
```
原始架构：
┌─────────────────┐
│   单一数据库    │
│  ┌─────────────┐│
│  │ 用户表      ││
│  │ 订单表      ││
│  │ 商品表      ││
│  │ 支付表      ││
│  └─────────────┘│
└─────────────────┘

垂直分库后：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│  用户数据库  │ │  订单数据库  │ │  商品数据库  │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 订单表  │ │ │ │ 商品表  │ │
│ │ 用户详情│ │ │ │ 订单项  │ │ │ │ 商品详情│ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

**水平分库（按数据特征）：**
```
按用户ID分库：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   DB_0      │ │   DB_1      │ │   DB_2      │
│ user_id%3=0 │ │ user_id%3=1 │ │ user_id%3=2 │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 用户表  │ │ │ │ 用户表  │ │
│ │ 订单表  │ │ │ │ 订单表  │ │ │ │ 订单表  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

### 2. 分表原理

**水平分表：**
```sql
-- 原始表
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

-- 分表后
CREATE TABLE orders_0 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

CREATE TABLE orders_1 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);
```

**垂直分表：**
```sql
-- 原始表（字段过多）
CREATE TABLE user_info (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    -- 基础信息
    real_name VARCHAR(50),
    id_card VARCHAR(18),
    -- 扩展信息（大字段）
    avatar LONGTEXT,
    description TEXT,
    preferences JSON
);

-- 垂直分表后
CREATE TABLE user_basic (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    real_name VARCHAR(50),
    id_card VARCHAR(18)
);

CREATE TABLE user_profile (
    user_id BIGINT PRIMARY KEY,
    avatar LONGTEXT,
    description TEXT,
    preferences JSON,
    FOREIGN KEY (user_id) REFERENCES user_basic(id)
);
```

### 3. 路由算法

**哈希取模算法：**
```go
func HashRoute(shardKey string, shardCount int) int {
    hash := crc32.ChecksumIEEE([]byte(shardKey))
    return int(hash) % shardCount
}

// 示例：用户ID路由
userID := "12345"
dbIndex := HashRoute(userID, 4) // 分4个库
tableIndex := HashRoute(userID, 8) // 每库8张表
```

**范围路由算法：**
```go
func RangeRoute(value int64, ranges []Range) int {
    for i, r := range ranges {
        if value >= r.Start && value < r.End {
            return i
        }
    }
    return -1
}

// 示例：按时间范围路由
type Range struct {
    Start int64
    End   int64
}

ranges := []Range{
    {Start: 0, End: 202401},      // 2024年1月
    {Start: 202401, End: 202402}, // 2024年2月
    {Start: 202402, End: 202403}, // 2024年3月
}
```

**一致性哈希算法：**
```go
type ConsistentHash struct {
    ring     map[uint32]string
    sortedKeys []uint32
    replicas int
}

func (ch *ConsistentHash) Add(nodes ...string) {
    for _, node := range nodes {
        for i := 0; i < ch.replicas; i++ {
            key := ch.hash(fmt.Sprintf("%s:%d", node, i))
            ch.ring[key] = node
            ch.sortedKeys = append(ch.sortedKeys, key)
        }
    }
    sort.Slice(ch.sortedKeys, func(i, j int) bool {
        return ch.sortedKeys[i] < ch.sortedKeys[j]
    })
}

func (ch *ConsistentHash) Get(key string) string {
    if len(ch.ring) == 0 {
        return ""
    }
    
    hash := ch.hash(key)
    idx := sort.Search(len(ch.sortedKeys), func(i int) bool {
        return ch.sortedKeys[i] >= hash
    })
    
    if idx == len(ch.sortedKeys) {
        idx = 0
    }
    
    return ch.ring[ch.sortedKeys[idx]]
}
```

## 技术实现

### 1. 中间件架构

**代理层架构：**
```
┌─────────────┐
│ 应用层      │
└─────────────┘
       │
┌─────────────┐
│ 分库分表    │
│ 中间件      │ ← SQL解析、路由、聚合
└─────────────┘
       │
┌─────────────┐
│ 数据库集群  │
└─────────────┘
```

**主流中间件对比：**

| 中间件 | 类型 | 语言 | 特点 | 适用场景 |
|--------|------|------|------|----------|
| **ShardingSphere** | 客户端 | Java | 功能全面、生态完善 | 企业级应用 |
| **MyCat** | 代理 | Java | 独立部署、透明化 | 中大型系统 |
| **Vitess** | 代理 | Go | 云原生、高性能 | 大规模分布式 |
| **TDDL** | 客户端 | Java | 阿里开源、稳定 | 电商场景 |
| **Cobar** | 代理 | Java | 轻量级、简单 | 中小型系统 |

### 2. 分片策略

**按业务维度分片：**
```yaml
# 配置示例
sharding:
  databases:
    user_db:
      shardingColumn: user_id
      algorithmExpression: user_db_${user_id % 4}
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 4}
  
  tables:
    orders:
      actualDataNodes: order_db_${0..3}.orders_${0..7}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 4}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 8}
```

**按时间维度分片：**
```sql
-- 按月分表
CREATE TABLE orders_202401 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2),
    INDEX idx_user_time (user_id, order_time)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404)
);
```

### 3. 实战案例

**案例一：电商订单系统分库分表实践**

**业务场景：** 某电商平台，日订单量千万级，用户量过亿。订单数据增长迅速，单库已无法支撑高并发写入和查询。

**技术方案：**
- **分片键：** 订单表以 `user_id` 作为分库键，以 `order_id` 作为分表键。用户表以 `user_id` 作为分库键。
- **分片策略：** 采用哈希取模，`user_id % 64` 分64个库，每个库内 `order_id % 16` 分16张表。
- **中间件：** 选用 ShardingSphere 作为数据分片中间件，以客户端模式集成到业务服务中。

**Go语言实现示例（简化版）：**
```go
package main

import (
	"database/sql"
	"fmt"
	"log"
	"strconv"
	"time"

	_ "github.com/go-sql-driver/mysql"
)

// Order represents a simplified order struct
type Order struct {
	ID        int64
	UserID    int64
	Amount    float64
	Status    int
	CreatedAt time.Time
}

// ShardConfig holds sharding configuration
type ShardConfig struct {
	DBCount    int
	TableCount int
	DSNs       map[string]string
}

// NewShardConfig creates a new ShardConfig
func NewShardConfig() *ShardConfig {
	// In a real application, DSNs would be loaded from config files or a discovery service
	dsns := make(map[string]string)
	for i := 0; i < 64; i++ {
		dsns[fmt.Sprintf("db_%d", i)] = fmt.Sprintf("root:password@tcp(127.0.0.1:330%d)/shard_db_%d?parseTime=true", i, i)
	}
	return &ShardConfig{
		DBCount:    64,
		TableCount: 16,
		DSNs:       dsns,
	}
}

// GetDBConnection gets a database connection for a given user ID
func (sc *ShardConfig) GetDBConnection(userID int64) (*sql.DB, error) {
	dbIndex := userID % int64(sc.DBCount)
	dbName := fmt.Sprintf("db_%d", dbIndex)
	dsn, ok := sc.DSNs[dbName]
	if !ok {
		return nil, fmt.Errorf("DSN not found for db: %s", dbName)
	}
	// In a real application, connection pools would be managed globally
	db, err := sql.Open("mysql", dsn)
	if err != nil {
		return nil, fmt.Errorf("failed to open database connection: %w", err)
	}
	db.SetMaxOpenConns(100)
	db.SetMaxIdleConns(10)
	db.SetConnMaxLifetime(5 * time.Minute)
	return db, nil
}

// GetTableName gets the sharded table name for a given order ID
func (sc *ShardConfig) GetTableName(orderID int64) string {
	tableIndex := orderID % int64(sc.TableCount)
	return fmt.Sprintf("orders_%d", tableIndex)
}

// CreateOrder inserts a new order into the sharded database
func (sc *ShardConfig) CreateOrder(order *Order) error {
	db, err := sc.GetDBConnection(order.UserID)
	if err != nil {
		return err
	}
	defer db.Close() // In real app, manage connection pool properly

	tableName := sc.GetTableName(order.ID)
	query := fmt.Sprintf("INSERT INTO %s (id, user_id, amount, status, created_at) VALUES (?, ?, ?, ?, ?)", tableName)

	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	_, err = stmt.Exec(order.ID, order.UserID, order.Amount, order.Status, order.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to execute insert: %w", err)
	}
	log.Printf("Order %d created in %s.%s\n", order.ID, db.Stats().DriverName, tableName)
	return nil
}

// GetOrderByID retrieves an order by its ID (requires knowing user_id for routing)
func (sc *ShardConfig) GetOrderByID(orderID, userID int64) (*Order, error) {
	db, err := sc.GetDBConnection(userID)
	if err != nil {
		return nil, err
	}
	defer db.Close()

	tableName := sc.GetTableName(orderID)
	query := fmt.Sprintf("SELECT id, user_id, amount, status, created_at FROM %s WHERE id = ? AND user_id = ?", tableName)

	row := db.QueryRow(query, orderID, userID)
	order := &Order{}
	err = row.Scan(&order.ID, &order.UserID, &order.Amount, &order.Status, &order.CreatedAt)
	if err == nil {
		log.Printf("Order %d retrieved from %s.%s\n", order.ID, db.Stats().DriverName, tableName)
	}
	return order, err
}

func main() {
	sc := NewShardConfig()

	// Simulate creating orders
	for i := 1; i <= 10; i++ {
		orderID := int64(1000 + i)
		userID := int64(i % 10) // Distribute users across shards
		order := &Order{
			ID:        orderID,
			UserID:    userID,
			Amount:    float64(100*i),
			Status:    1,
			CreatedAt: time.Now(),
		}
		err := sc.CreateOrder(order)
		if err != nil {
			log.Printf("Error creating order: %v\n", err)
		}
	}

	// Simulate retrieving an order
	retrievedOrder, err := sc.GetOrderByID(1005, 5) // Assuming order 1005 belongs to user 5
	if err != nil {
		log.Printf("Error retrieving order: %v\n", err)
	} else {
		log.Printf("Retrieved Order: %+v\n", retrievedOrder)
	}
}
```

**性能测试数据（模拟）：**
- **分片前：** 单库单表，1亿条订单数据，查询平均响应时间 800ms，写入 QPS 500。
- **分片后：** 64库16表，总计1024个物理分片，查询平均响应时间 50ms，写入 QPS 15000+。

**踩坑经验与解决方案：**
1. **跨库Join/事务问题：**
   - **问题：** 业务查询需要关联不同分库的表，或涉及跨库事务。
   - **解决方案：** 尽量避免跨库Join，通过业务层组装数据或数据冗余解决。分布式事务采用最终一致性方案（如消息队列、TCC），避免强一致性带来的性能损耗。
2. **热点数据问题：**
   - **问题：** 某些用户或特定时间段的数据访问量巨大，导致单个分片成为瓶颈。
   - **解决方案：** 针对热点数据进行特殊处理，如独立分片、读写分离、缓存预热等。分片键的选择至关重要，需确保数据分布均匀。
3. **数据迁移与扩容：**
   - **问题：** 随着数据量增长，需要进行分片扩容，数据迁移过程复杂且影响线上服务。
   - **解决方案：** 提前规划好分片数量，预留足够的扩容空间。采用平滑扩容方案，如双写、渐进式迁移，结合数据校验确保数据一致性。利用中间件的在线扩容能力。
4. **全局唯一ID生成：**
   - **问题：** 分库分表后，数据库自增ID无法保证全局唯一性。
   - **解决方案：** 引入独立ID生成服务（如Snowflake算法、UUID、Redis自增ID），保证ID的全局唯一性。
5. **复杂查询与聚合：**
   - **问题：** 跨分片的复杂查询（如聚合、排序、分页）性能低下。
   - **解决方案：** 针对复杂查询，考虑引入离线数仓（如Hadoop、Spark）或实时数仓（如ClickHouse、Elasticsearch）进行数据分析和报表生成。业务层进行结果归并和二次处理。

## 解决的问题

### 1. 性能问题

**单表性能瓶颈：**
- **查询性能下降**：千万级数据查询缓慢
- **索引效率降低**：B+树层级增加，IO次数增多
- **锁竞争加剧**：表级锁、行级锁冲突频繁

**解决效果：**
```
分片前：
- 单表1000万数据
- 查询响应时间：500ms-2s
- QPS：1000

分片后（8个分片）：
- 单表125万数据
- 查询响应时间：50ms-200ms
- QPS：8000（理论值）
```

### 2. 存储问题

**存储容量限制：**
- MySQL单表建议不超过2000万行
- 单库容量建议不超过500GB
- 磁盘IO成为瓶颈

**解决方案：**
```
垂直分库：按业务模块分离
- 用户库：100GB
- 订单库：200GB
- 商品库：150GB
- 日志库：300GB

水平分库：按数据量分散
- 4个分库，每库负载降低75%
- 存储压力分散
- IO并发能力提升
```

### 3. 可用性问题

**单点故障风险：**
- 单库故障影响全业务
- 备份恢复时间长
- 维护窗口影响大

**高可用架构：**
```
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   分片1     │ │   分片2     │ │   分片3     │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Master  │ │ │ │ Master  │ │ │ │ Master  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Slave   │ │ │ │ Slave   │ │ │ │ Slave   │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

## 方案设计

### 1. 垂直分库方案

**设计原则：**
- 按业务领域划分
- 减少跨库事务
- 便于团队协作

**实施步骤：**
```
1. 业务梳理
   ├── 用户域：用户信息、权限、认证
   ├── 商品域：商品信息、分类、库存
   ├── 订单域：订单、支付、物流
   └── 营销域：优惠券、活动、推荐

2. 数据库设计
   ├── user_db：用户相关表
   ├── product_db：商品相关表
   ├── order_db：订单相关表
   └── marketing_db：营销相关表

3. 应用改造
   ├── 数据源配置
   ├── DAO层改造
   ├── 事务处理
   └── 数据一致性
```

### 2. 水平分库分表方案

**分片键选择：**
```
常用分片键：
1. 用户ID：适合用户相关数据
2. 订单ID：适合订单相关数据
3. 时间：适合日志、流水数据
4. 地理位置：适合LBS应用
5. 业务ID：适合特定业务场景

选择原则：
- 数据分布均匀
- 查询路由简单
- 避免热点数据
- 支持范围查询
```

**分片数量规划：**
```go
// 分片数量计算
func CalculateShardCount(totalData, maxDataPerShard int64) int {
    shardCount := int(math.Ceil(float64(totalData) / float64(maxDataPerShard)))
    
    // 确保是2的幂次，便于扩容
    powerOf2 := 1
    for powerOf2 < shardCount {
        powerOf2 *= 2
    }
    
    return powerOf2
}

// 示例：
// 预计5年数据量：10亿
// 单表最大数据：1000万
// 计算结果：128个分片（2^7）
```

### 3. 混合分片方案

**垂直+水平分片：**
```
第一层：垂直分库（按业务）
├── user_cluster
│   ├── user_db_0 (user_id % 4 = 0)
│   ├── user_db_1 (user_id % 4 = 1)
│   ├── user_db_2 (user_id % 4 = 2)
│   └── user_db_3 (user_id % 4 = 3)
│
└── order_cluster
    ├── order_db_0 (user_id % 4 = 0)
    ├── order_db_1 (user_id % 4 = 1)
    ├── order_db_2 (user_id % 4 = 2)
    └── order_db_3 (user_id % 4 = 3)

第二层：水平分表（按数据量）
order_db_0:
├── orders_0 (order_id % 8 = 0)
├── orders_1 (order_id % 8 = 1)
├── ...
└── orders_7 (order_id % 8 = 7)
```

## MySQL实现细节

### 1. 分区表技术

**RANGE分区（按时间）：**
```sql
CREATE TABLE orders (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT DEFAULT 0,
    PRIMARY KEY (id, order_time),
    INDEX idx_user_id (user_id),
    INDEX idx_status (status)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- 自动分区管理
DELIMITER //
CREATE PROCEDURE CreateMonthlyPartition()
BEGIN
    DECLARE partition_name VARCHAR(20);
    DECLARE partition_value INT;
    
    SET partition_name = CONCAT('p', DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 1 MONTH), '%Y%m'));
    SET partition_value = DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 2 MONTH), '%Y%m');
    
    SET @sql = CONCAT('ALTER TABLE orders ADD PARTITION (PARTITION ', partition_name, 
                     ' VALUES LESS THAN (', partition_value, '))');
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END //
DELIMITER ;

-- 定期执行
CREATE EVENT CreatePartitionEvent
ON SCHEDULE EVERY 1 MONTH
STARTS '2024-01-01 00:00:00'
DO CALL CreateMonthlyPartition();
```

**HASH分区（按ID）：**
```sql
CREATE TABLE user_logs (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    action VARCHAR(50),
    log_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, user_id),
    INDEX idx_user_time (user_id, log_time)
) PARTITION BY HASH(user_id)
PARTITIONS 16;

-- 查看分区信息
SELECT 
    PARTITION_NAME,
    TABLE_ROWS,
    DATA_LENGTH,
    INDEX_LENGTH
FROM INFORMATION_SCHEMA.PARTITIONS 
WHERE TABLE_NAME = 'user_logs';
```

**LIST分区（按地区）：**
```sql
CREATE TABLE regional_data (
    id BIGINT AUTO_INCREMENT,
    region_code VARCHAR(10),
    data_value TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, region_code)
) PARTITION BY LIST COLUMNS(region_code) (
    PARTITION p_north VALUES IN ('BJ', 'TJ', 'HE', 'SX', 'NM'),
    PARTITION p_east VALUES IN ('SH', 'JS', 'ZJ', 'AH', 'FJ', 'JX', 'SD'),
    PARTITION p_south VALUES IN ('GD', 'GX', 'HI'),
    PARTITION p_west VALUES IN ('CQ', 'SC', 'GZ', 'YN', 'XZ', 'SN', 'GS', 'QH', 'NX', 'XJ')
);
```

### 2. 分库分表实现

**数据库连接管理：**
```go
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
}

type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
}

type HashRouter struct {
    dbCount    int
    tableCount int
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    dbIndex := int(hash) % r.dbCount
    return fmt.Sprintf("db_%d", dbIndex)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    tableIndex := int(hash) % r.tableCount
    return fmt.Sprintf("orders_%d", tableIndex)
}

// 使用示例
func (ds *ShardingDataSource) Insert(userID string, order *Order) error {
    dbName := ds.router.RouteDatabase(userID)
    tableName := ds.router.RouteTable(userID)
    
    db := ds.dataSources[dbName]
    query := fmt.Sprintf("INSERT INTO %s (user_id, amount, status) VALUES (?, ?, ?)", tableName)
    
    _, err := db.Exec(query, order.UserID, order.Amount, order.Status)
    return err
}
```

### 3. 跨分片查询

**分片聚合查询：**
```go
type AggregateResult struct {
    TotalCount int64
    TotalAmount float64
    AvgAmount   float64
}

func (ds *ShardingDataSource) AggregateQuery(startTime, endTime time.Time) (*AggregateResult, error) {
    var wg sync.WaitGroup
    resultChan := make(chan *AggregateResult, len(ds.dataSources))
    errorChan := make(chan error, len(ds.dataSources))
    
    // 并行查询所有分片
    for dbName, db := range ds.dataSources {
        wg.Add(1)
        go func(dbName string, db *sql.DB) {
            defer wg.Done()
            
            result, err := ds.queryShardAggregate(db, startTime, endTime)
            if err != nil {
                errorChan <- err
                return
            }
            resultChan <- result
        }(dbName, db)
    }
    
    wg.Wait()
    close(resultChan)
    close(errorChan)
    
    // 检查错误
    select {
    case err := <-errorChan:
        return nil, err
    default:
    }
    
    // 聚合结果
    finalResult := &AggregateResult{}
    for result := range resultChan {
        finalResult.TotalCount += result.TotalCount
        finalResult.TotalAmount += result.TotalAmount
    }
    
    if finalResult.TotalCount > 0 {
        finalResult.AvgAmount = finalResult.TotalAmount / float64(finalResult.TotalCount)
    }
    
    return finalResult, nil
}

func (ds *ShardingDataSource) queryShardAggregate(db *sql.DB, startTime, endTime time.Time) (*AggregateResult, error) {
    // 查询所有分表
    tables := []string{"orders_0", "orders_1", "orders_2", "orders_3"}
    
    var totalCount int64
    var totalAmount float64
    
    for _, table := range tables {
        query := fmt.Sprintf(`
            SELECT COUNT(*), COALESCE(SUM(amount), 0)
            FROM %s 
            WHERE order_time >= ? AND order_time < ?
        `, table)
        
        var count int64
        var amount float64
        err := db.QueryRow(query, startTime, endTime).Scan(&count, &amount)
        if err != nil {
            return nil, err
        }
        
        totalCount += count
        totalAmount += amount
    }
    
    return &AggregateResult{
        TotalCount:  totalCount,
        TotalAmount: totalAmount,
    }, nil
}
```

### 4. 分布式事务

**两阶段提交（2PC）：**
```go
type DistributedTransaction struct {
    dataSources map[string]*sql.DB
    transactions map[string]*sql.Tx
}

func (dt *DistributedTransaction) Begin() error {
    dt.transactions = make(map[string]*sql.Tx)
    
    for name, db := range dt.dataSources {
        tx, err := db.Begin()
        if err != nil {
            // 回滚已开始的事务
            dt.Rollback()
            return err
        }
        dt.transactions[name] = tx
    }
    
    return nil
}

func (dt *DistributedTransaction) Commit() error {
    // 第一阶段：准备提交
    for name, tx := range dt.transactions {
        _, err := tx.Exec("XA PREPARE ?;", name)
        if err != nil {
            dt.Rollback()
            return err
        }
    }
    
    // 第二阶段：提交
    for name, tx := range dt.transactions {
        err := tx.Commit()
        if err != nil {
            // 记录日志，需要人工干预
            log.Printf("Failed to commit transaction %s: %v", name, err)
            return err
        }
    }
    
    return nil
}

func (dt *DistributedTransaction) Rollback() {
    for _, tx := range dt.transactions {
        tx.Rollback()
    }
}
```

**TCC模式（Try-Confirm-Cancel）：**
```go
type TCCTransaction struct {
    operations []TCCOperation
}

type TCCOperation interface {
    Try() error
    Confirm() error
    Cancel() error
}

type TransferOperation struct {
    fromAccount string
    toAccount   string
    amount      float64
    ds          *ShardingDataSource
}

func (op *TransferOperation) Try() error {
    // 冻结转出账户金额
    return op.ds.FreezeAmount(op.fromAccount, op.amount)
}

func (op *TransferOperation) Confirm() error {
    // 确认转账
    err := op.ds.DeductAmount(op.fromAccount, op.amount)
    if err != nil {
        return err
    }
    return op.ds.AddAmount(op.toAccount, op.amount)
}

func (op *TransferOperation) Cancel() error {
    // 解冻金额
    return op.ds.UnfreezeAmount(op.fromAccount, op.amount)
}

func (tcc *TCCTransaction) Execute() error {
    // Try阶段
    for _, op := range tcc.operations {
        if err := op.Try(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    // Confirm阶段
    for _, op := range tcc.operations {
        if err := op.Confirm(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    return nil
}
```

## 注意事项与坑点

### 1. 分片键选择陷阱

**热点数据问题：**
```go
// 错误示例：按时间分片导致热点
func BadTimeSharding(timestamp time.Time) string {
    // 所有当前数据都写入同一分片
    return fmt.Sprintf("shard_%s", timestamp.Format("2006-01-02"))
}

// 正确示例：组合分片键
func GoodSharding(userID int64, timestamp time.Time) string {
    // 用户ID保证分散，时间用于查询优化
    shardIndex := userID % 16
    return fmt.Sprintf("shard_%d", shardIndex)
}
```

**数据倾斜问题：**
```sql
-- 监控分片数据分布
SELECT 
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb
FROM information_schema.tables 
WHERE table_schema = 'order_db' 
AND table_name LIKE 'orders_%'
ORDER BY table_rows DESC;

-- 结果分析
-- orders_0: 1,500,000 rows (数据倾斜)
-- orders_1: 800,000 rows
-- orders_2: 900,000 rows
-- orders_3: 750,000 rows
```

### 2. 跨分片查询复杂性

**JOIN查询限制：**
```sql
-- 无法执行的跨分片JOIN
SELECT u.username, o.amount
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.order_time >= '2024-01-01';

-- 解决方案1：应用层JOIN
-- 1. 先查询orders表获取user_id列表
-- 2. 再查询users表获取用户信息
-- 3. 应用层组装数据

-- 解决方案2：数据冗余
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    username VARCHAR(50), -- 冗余用户名
    amount DECIMAL(10,2),
    order_time DATETIME
);
```

**分页查询问题：**
```go
// 错误的分页实现
func BadPagination(page, size int) ([]Order, error) {
    var allOrders []Order
    
    // 从每个分片查询
    for _, db := range dataSources {
        orders, err := queryFromShard(db, page, size)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 问题：数据量不准确，排序错误
    return allOrders, nil
}

// 正确的分页实现
func GoodPagination(page, size int) ([]Order, error) {
    // 1. 从每个分片查询更多数据
    shardSize := size * len(dataSources)
    
    var allOrders []Order
    for _, db := range dataSources {
        orders, err := queryFromShard(db, 1, shardSize)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 2. 全局排序
    sort.Slice(allOrders, func(i, j int) bool {
        return allOrders[i].OrderTime.After(allOrders[j].OrderTime)
    })
    
    // 3. 分页截取
    start := (page - 1) * size
    end := start + size
    if start >= len(allOrders) {
        return []Order{}, nil
    }
    if end > len(allOrders) {
        end = len(allOrders)
    }
    
    return allOrders[start:end], nil
}
```

### 3. 数据一致性问题

**分布式事务性能问题：**
```go
// 避免分布式事务的设计
type OrderService struct {
    orderDB   *sql.DB
    accountDB *sql.DB
    stockDB   *sql.DB
}

// 错误：强一致性要求导致分布式事务
func (s *OrderService) CreateOrderWithTransaction(order *Order) error {
    // 需要跨3个数据库的事务
    // 1. 扣减库存 (stockDB)
    // 2. 扣减余额 (accountDB)
    // 3. 创建订单 (orderDB)
    
    // 分布式事务复杂且性能差
    return s.distributedTransaction(order)
}

// 正确：最终一致性设计
func (s *OrderService) CreateOrderEventually(order *Order) error {
    // 1. 先创建订单（状态：待支付）
    err := s.createPendingOrder(order)
    if err != nil {
        return err
    }
    
    // 2. 异步处理库存和支付
    s.publishOrderCreatedEvent(order)
    
    return nil
}

func (s *OrderService) handleOrderCreatedEvent(order *Order) {
    // 异步处理，允许重试
    if err := s.reserveStock(order); err != nil {
        s.retryLater(order, "reserve_stock")
        return
    }
    
    if err := s.processPayment(order); err != nil {
        s.releaseStock(order)
        s.retryLater(order, "process_payment")
        return
    }
    
    // 更新订单状态为已支付
    s.updateOrderStatus(order.ID, "paid")
}
```

### 4. 扩容缩容问题

**数据迁移复杂性：**
```go
// 扩容方案：双写+迁移
type MigrationManager struct {
    oldShards []ShardInfo
    newShards []ShardInfo
    migrating bool
}

func (m *MigrationManager) Write(key string, data interface{}) error {
    if !m.migrating {
        // 正常写入
        return m.writeToShard(m.getOldShard(key), data)
    }
    
    // 迁移期间双写
    oldShard := m.getOldShard(key)
    newShard := m.getNewShard(key)
    
    // 先写新分片
    if err := m.writeToShard(newShard, data); err != nil {
        return err
    }
    
    // 再写旧分片（允许失败）
    m.writeToShard(oldShard, data)
    
    return nil
}

func (m *MigrationManager) Read(key string) (interface{}, error) {
    if !m.migrating {
        return m.readFromShard(m.getOldShard(key), key)
    }
    
    // 迁移期间优先读新分片
    newShard := m.getNewShard(key)
    data, err := m.readFromShard(newShard, key)
    if err == nil {
        return data, nil
    }
    
    // 新分片没有则读旧分片
    oldShard := m.getOldShard(key)
    return m.readFromShard(oldShard, key)
}
```

### 5. 运维复杂性

**监控指标：**
```sql
-- 分片性能监控
SELECT 
    table_schema,
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb,
    ROUND((data_length + index_length)/1024/1024, 2) as total_mb
FROM information_schema.tables 
WHERE table_schema LIKE '%_db_%'
ORDER BY total_mb DESC;

-- 慢查询监控
SELECT 
    db,
    sql_text,
    exec_count,
    avg_timer_wait/1000000000 as avg_time_sec,
    max_timer_wait/1000000000 as max_time_sec
FROM performance_schema.events_statements_summary_by_digest
WHERE avg_timer_wait > 1000000000 -- 超过1秒
ORDER BY avg_timer_wait DESC
LIMIT 10;
```

**备份策略：**
```bash
#!/bin/bash
# 分片备份脚本

SHARD_COUNT=4
BACKUP_DIR="/backup/$(date +%Y%m%d)"

mkdir -p $BACKUP_DIR

# 并行备份所有分片
for i in $(seq 0 $((SHARD_COUNT-1))); do
    {
        echo "Backing up shard $i..."
        mysqldump -h db_$i.example.com -u backup_user -p$BACKUP_PASSWORD \
            --single-transaction \
            --routines \
            --triggers \
            order_db_$i > $BACKUP_DIR/order_db_$i.sql
        
        if [ $? -eq 0 ]; then
            echo "Shard $i backup completed"
            gzip $BACKUP_DIR/order_db_$i.sql
        else
            echo "Shard $i backup failed"
        fi
    } &
done

wait
echo "All shards backup completed"
```

## 高频面试题

### 1. 基础概念题

**Q1: 什么是分库分表？为什么需要分库分表？**

**答案：**
分库分表是一种数据库水平扩展技术，通过将数据分散到多个数据库实例或表中来解决单库单表的性能瓶颈。

**需要分库分表的原因：**
1. **数据量过大**：单表超过千万级别，查询性能下降
2. **并发压力**：单库QPS/TPS达到瓶颈
3. **存储限制**：单库容量接近上限
4. **可用性要求**：避免单点故障

**Q2: 垂直分库和水平分库的区别是什么？**

**答案：**

| 维度 | 垂直分库 | 水平分库 |
|------|----------|----------|
| **分割方式** | 按业务模块分割 | 按数据特征分割 |
| **数据分布** | 不同业务在不同库 | 同业务数据分散到多库 |
| **查询特点** | 减少跨库查询 | 需要路由到正确分片 |
| **扩展性** | 受业务模块限制 | 可无限水平扩展 |
| **复杂度** | 相对简单 | 路由和聚合复杂 |

**Q3: 常见的分片算法有哪些？各有什么优缺点？**

**答案：**

1. **哈希取模算法**
   - 优点：分布均匀、实现简单
   - 缺点：扩容困难、无法范围查询

2. **范围分片算法**
   - 优点：支持范围查询、扩容相对容易
   - 缺点：可能数据倾斜、热点问题

3. **一致性哈希算法**
   - 优点：扩容影响小、负载均衡好
   - 缺点：实现复杂、可能数据倾斜

4. **目录映射算法**
   - 优点：灵活性高、支持复杂路由
   - 缺点：需要额外存储、单点风险

### 2. 技术实现题

**Q4: 如何解决分库分表后的跨分片查询问题？**

**答案：**

1. **应用层聚合**
   ```go
   func CrossShardQuery(condition QueryCondition) ([]Result, error) {
       var results []Result
       var wg sync.WaitGroup
       resultChan := make(chan []Result, len(shards))
       
       // 并行查询所有分片
       for _, shard := range shards {
           wg.Add(1)
           go func(s Shard) {
               defer wg.Done()
               result := s.Query(condition)
               resultChan <- result
           }(shard)
       }
       
       wg.Wait()
       close(resultChan)
       
       // 聚合结果
       for result := range resultChan {
           results = append(results, result...)
       }
       
       return results, nil
   }
   ```

2. **数据冗余**
   - 在需要JOIN的表中冗余关联字段
   - 使用宽表设计减少关联查询

3. **搜索引擎**
   - 使用Elasticsearch等搜索引擎
   - 异步同步数据到搜索引擎

**Q5: 分库分表后如何保证数据一致性？**

**答案：**

1. **避免分布式事务**
   - 通过业务设计避免跨分片事务
   - 使用最终一致性代替强一致性

2. **分布式事务方案**
   ```go
   // TCC模式示例
   type TCCManager struct {
       operations []TCCOperation
   }
   
   func (m *TCCManager) Execute() error {
       // Try阶段
       for _, op := range m.operations {
           if err := op.Try(); err != nil {
               m.cancelAll()
               return err
           }
       }
       
       // Confirm阶段
       for _, op := range m.operations {
           if err := op.Confirm(); err != nil {
               // 需要补偿机制
               return err
           }
       }
       
       return nil
   }
   ```

3. **消息队列保证最终一致性**
   - 使用可靠消息投递
   - 实现幂等性处理
   - 补偿机制处理异常

### 3. 架构设计题

**Q6: 设计一个电商订单系统的分库分表方案**

**答案：**

**业务分析：**
- 订单数据量大，增长快
- 查询模式：按用户查询、按时间范围查询
- 写入频繁，读取也频繁

**分片策略：**
```
1. 垂直分库：
   - user_db：用户相关数据
   - order_db：订单相关数据
   - product_db：商品相关数据
   - payment_db：支付相关数据

2. 水平分库分表：
   - 分片键：user_id（保证用户相关数据在同一分片）
   - 分库：4个order_db实例
   - 分表：每库8张orders表
   - 路由算法：user_id % 4 确定库，order_id % 8 确定表

3. 数据分布：
   order_db_0:
   ├── orders_0, orders_1, ..., orders_7
   ├── order_items_0, order_items_1, ..., order_items_7
   └── order_logs_0, order_logs_1, ..., order_logs_7
```

**技术架构：**
```go
type OrderShardingService struct {
    dataSources map[string]*sql.DB
    router      *OrderRouter
}

type OrderRouter struct {
    dbCount    int
    tableCount int
}

func (r *OrderRouter) Route(userID int64) (string, string) {
    dbIndex := userID % int64(r.dbCount)
    tableIndex := userID % int64(r.tableCount)
    
    dbName := fmt.Sprintf("order_db_%d", dbIndex)
    tableName := fmt.Sprintf("orders_%d", tableIndex)
    
    return dbName, tableName
}
```

**Q7: 如何设计分库分表的扩容方案？**

**答案：**

**扩容策略：**
1. **倍数扩容**：从N个分片扩容到2N个分片
2. **数据迁移**：只需迁移50%的数据
3. **双写方案**：保证服务不中断

**实施步骤：**
```
1. 准备阶段：
   - 部署新的数据库实例
   - 创建新的分片表结构
   - 配置数据同步工具

2. 迁移阶段：
   - 开启双写模式（新旧分片同时写入）
   - 历史数据迁移（按分片键重新路由）
   - 数据一致性校验

3. 切换阶段：
   - 更新路由配置
   - 停止旧分片写入
   - 验证新分片数据完整性

4. 清理阶段：
   - 删除旧分片冗余数据
   - 回收旧分片资源
```

**双写实现：**
```go
type MigrationWriter struct {
    oldRouter ShardingRouter
    newRouter ShardingRouter
    migrating bool
}

func (w *MigrationWriter) Write(key string, data interface{}) error {
    if !w.migrating {
        return w.writeToOldShard(key, data)
    }
    
    // 双写模式
    newShard := w.newRouter.Route(key)
    if err := w.writeToShard(newShard, data); err != nil {
        return err
    }
    
    oldShard := w.oldRouter.Route(key)
    w.writeToShard(oldShard, data) // 允许失败
    
    return nil
}
```

### 4. 性能优化题

**Q8: 分库分表后如何优化查询性能？**

**答案：**

1. **索引优化**
   ```sql
   -- 分片键必须是索引的前缀
   CREATE INDEX idx_user_time ON orders(user_id, order_time);
   
   -- 覆盖索引减少回表
   CREATE INDEX idx_user_status_amount ON orders(user_id, status, amount);
   ```

2. **查询路由优化**
   ```go
   func OptimizedQuery(userID int64, status string) ([]Order, error) {
       // 精确路由到单个分片
       dbName, tableName := router.Route(userID)
       db := dataSources[dbName]
       
       query := fmt.Sprintf(
           "SELECT * FROM %s WHERE user_id = ? AND status = ?",
           tableName)
       
       return queryOrders(db, query, userID, status)
   }
   ```

3. **缓存策略**
   ```go
   func QueryWithCache(userID int64) ([]Order, error) {
       // 先查缓存
       cacheKey := fmt.Sprintf("user_orders:%d", userID)
       if orders := cache.Get(cacheKey); orders != nil {
           return orders.([]Order), nil
       }
       
       // 缓存未命中，查询数据库
       orders, err := queryFromDB(userID)
       if err != nil {
           return nil, err
       }
       
       // 写入缓存
       cache.Set(cacheKey, orders, 5*time.Minute)
       return orders, nil
   }
   ```

4. **读写分离**
   ```go
   type ShardCluster struct {
       master *sql.DB
       slaves []*sql.DB
   }
   
   func (c *ShardCluster) Read(query string, args ...interface{}) (*sql.Rows, error) {
       // 负载均衡选择从库
       slave := c.slaves[rand.Intn(len(c.slaves))]
       return slave.Query(query, args...)
   }
   
   func (c *ShardCluster) Write(query string, args ...interface{}) (sql.Result, error) {
       // 写入主库
       return c.master.Exec(query, args...)
   }
   ```

### 5. 故障处理题

**Q9: 分库分表环境下如何处理单个分片故障？**

**答案：**

1. **故障检测**
   ```go
   type HealthChecker struct {
       shards map[string]*ShardInfo
       mutex  sync.RWMutex
   }
   
   func (hc *HealthChecker) CheckHealth() {
       for name, shard := range hc.shards {
           go func(name string, shard *ShardInfo) {
               if err := shard.Ping(); err != nil {
                   hc.markUnhealthy(name)
                   log.Printf("Shard %s is unhealthy: %v", name, err)
               } else {
                   hc.markHealthy(name)
               }
           }(name, shard)
       }
   }
   ```

2. **故障转移**
   ```go
   func (ds *ShardingDataSource) Query(shardKey string, query string) (*sql.Rows, error) {
       primaryShard := ds.router.Route(shardKey)
       
       // 尝试主分片
       if ds.isHealthy(primaryShard) {
           return ds.queryFromShard(primaryShard, query)
       }
       
       // 主分片故障，尝试备份分片
       backupShard := ds.getBackupShard(primaryShard)
       if backupShard != "" && ds.isHealthy(backupShard) {
           return ds.queryFromShard(backupShard, query)
       }
       
       return nil, errors.New("all shards for this key are unavailable")
   }
   ```

3. **数据恢复**
   ```bash
   # 从备份恢复分片
   #!/bin/bash
   FAILED_SHARD="order_db_2"
   BACKUP_FILE="/backup/order_db_2_20240115.sql.gz"
   
   # 1. 停止应用对该分片的写入
   echo "Marking shard $FAILED_SHARD as readonly"
   
   # 2. 恢复数据
   echo "Restoring shard $FAILED_SHARD from backup"
   gunzip < $BACKUP_FILE | mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 3. 同步增量数据（从binlog）
   echo "Syncing incremental data"
   mysqlbinlog --start-datetime="2024-01-15 10:00:00" \
               --database=$FAILED_SHARD \
               /var/log/mysql/mysql-bin.000001 | \
               mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 4. 验证数据一致性
   echo "Verifying data consistency"
   
   # 5. 恢复服务
   echo "Marking shard $FAILED_SHARD as healthy"
   ```

## 实战场景分析

### 场景1：电商订单系统

**业务特点：**
- 订单数据量：日增100万+
- 查询模式：用户查询自己订单、商家查询店铺订单
- 峰值特点：促销期间写入量暴增

**分片方案：**
```yaml
# 分片配置
sharding:
  databases:
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 8}
      
  tables:
    orders:
      actualDataNodes: order_db_${0..7}.orders_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 16}
        
    order_items:
      actualDataNodes: order_db_${0..7}.order_items_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: order_items_${order_id % 16}
```

**Go实现：**
```go
type OrderService struct {
    shardingDS *ShardingDataSource
    cache      *redis.Client
}

// 创建订单
func (s *OrderService) CreateOrder(ctx context.Context, order *Order) error {
    // 生成分布式ID
    order.ID = s.generateOrderID()
    
    // 开启事务
    tx, err := s.shardingDS.BeginTx(ctx, order.UserID)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // 插入订单主表
    err = s.insertOrder(ctx, tx, order)
    if err != nil {
        return err
    }
    
    // 插入订单明细
    for _, item := range order.Items {
        err = s.insertOrderItem(ctx, tx, item)
        if err != nil {
            return err
        }
    }
    
    // 提交事务
    if err = tx.Commit(); err != nil {
        return err
    }
    
    // 异步更新缓存
    go s.updateOrderCache(order)
    
    return nil
}

// 查询用户订单
func (s *OrderService) GetUserOrders(ctx context.Context, userID int64, page, size int) ([]Order, error) {
    // 先查缓存
    cacheKey := fmt.Sprintf("user_orders:%d:%d:%d", userID, page, size)
    if cached := s.cache.Get(ctx, cacheKey).Val(); cached != "" {
        var orders []Order
        json.Unmarshal([]byte(cached), &orders)
        return orders, nil
    }
    
    // 缓存未命中，查询数据库
    orders, err := s.queryUserOrders(ctx, userID, page, size)
    if err != nil {
        return nil, err
    }
    
    // 写入缓存
    data, _ := json.Marshal(orders)
    s.cache.Set(ctx, cacheKey, data, 5*time.Minute)
    
    return orders, nil
}
```

### 场景2：金融交易系统

**业务特点：**
- 交易数据：强一致性要求
- 查询模式：按用户、按时间、按交易类型
- 监管要求：数据不可篡改、完整审计

**分片策略：**
```go
// 按用户+时间双维度分片
type FinanceRouter struct {
    userShardCount int
    timeShardCount int
}

func (r *FinanceRouter) Route(userID int64, timestamp time.Time) (string, string) {
    // 用户维度分库
    userShard := userID % int64(r.userShardCount)
    
    // 时间维度分表
    timeKey := timestamp.Format("200601") // 按月分表
    
    dbName := fmt.Sprintf("finance_db_%d", userShard)
    tableName := fmt.Sprintf("transactions_%s", timeKey)
    
    return dbName, tableName
}
```

**事务处理：**
```go
type TransactionService struct {
    shardingDS *ShardingDataSource
    sagaManager *SagaManager
}

// 转账操作（Saga模式）
func (s *TransactionService) Transfer(ctx context.Context, req *TransferRequest) error {
    saga := s.sagaManager.NewSaga()
    
    // 步骤1：冻结转出账户金额
    saga.AddStep(
        func() error { return s.freezeAmount(req.FromAccount, req.Amount) },
        func() error { return s.unfreezeAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤2：扣减转出账户
    saga.AddStep(
        func() error { return s.deductAmount(req.FromAccount, req.Amount) },
        func() error { return s.addAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤3：增加转入账户
    saga.AddStep(
        func() error { return s.addAmount(req.ToAccount, req.Amount) },
        func() error { return s.deductAmount(req.ToAccount, req.Amount) },
    )
    
    // 步骤4：记录交易流水
    saga.AddStep(
        func() error { return s.recordTransaction(req) },
        func() error { return s.cancelTransaction(req.TransactionID) },
    )
    
    return saga.Execute(ctx)
}
```

### 场景3：日志分析系统

**业务特点：**
- 数据量：TB级别日志数据
- 写入：高并发写入，批量导入
- 查询：按时间范围、按关键字搜索

**分片策略：**
```sql
-- 按时间分表（天级别）
CREATE TABLE access_logs_20240115 (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT,
    ip_address VARCHAR(45),
    user_agent TEXT,
    request_url VARCHAR(500),
    response_code INT,
    response_time INT,
    log_time DATETIME,
    PRIMARY KEY (id, log_time),
    INDEX idx_user_time (user_id, log_time),
    INDEX idx_ip_time (ip_address, log_time),
    INDEX idx_response_code (response_code)
) PARTITION BY RANGE (TO_DAYS(log_time)) (
    PARTITION p20240115 VALUES LESS THAN (TO_DAYS('2024-01-16')),
    PARTITION p20240116 VALUES LESS THAN (TO_DAYS('2024-01-17')),
    PARTITION p20240117 VALUES LESS THAN (TO_DAYS('2024-01-18'))
);
```

**批量写入优化：**
```go
type LogService struct {
    shardingDS *ShardingDataSource
    buffer     *LogBuffer
}

type LogBuffer struct {
    logs   []AccessLog
    mutex  sync.Mutex
    ticker *time.Ticker
}

// 异步批量写入
func (s *LogService) WriteLog(log *AccessLog) {
    s.buffer.Add(log)
}

func (b *LogBuffer) Add(log *AccessLog) {
    b.mutex.Lock()
    defer b.mutex.Unlock()
    
    b.logs = append(b.logs, *log)
    
    // 达到批量大小或时间间隔，触发写入
    if len(b.logs) >= 1000 {
        go b.flush()
    }
}

func (b *LogBuffer) flush() {
    b.mutex.Lock()
    logs := make([]AccessLog, len(b.logs))
    copy(logs, b.logs)
    b.logs = b.logs[:0] // 清空缓冲区
    b.mutex.Unlock()
    
    // 按分片分组
    shardGroups := make(map[string][]AccessLog)
    for _, log := range logs {
        shard := b.getShardKey(log.LogTime)
        shardGroups[shard] = append(shardGroups[shard], log)
    }
    
    // 并行写入各分片
    var wg sync.WaitGroup
    for shard, shardLogs := range shardGroups {
        wg.Add(1)
        go func(shard string, logs []AccessLog) {
            defer wg.Done()
            b.batchInsert(shard, logs)
        }(shard, shardLogs)
    }
    wg.Wait()
}
```

## Go语言实现

### 1. Go语言分库分表中间件选型

在Go语言生态中，目前没有像Java生态中ShardingSphere那样成熟且功能全面的分库分表中间件。通常需要结合以下方式实现：

- **自研路由层：** 在应用层实现分片键解析、路由计算、SQL改写和结果归并。
- **数据库连接池管理：** 使用Go原生的 `database/sql` 结合连接池管理，或者使用第三方库如 `gorm`、`xorm` 等ORM框架。
- **ShardingSphere-Proxy：** 如果团队有Java背景或希望使用成熟的代理层方案，可以部署 ShardingSphere-Proxy，Go应用通过标准MySQL协议连接。
- **Vitess：** 对于超大规模的MySQL集群，Vitess 是一个云原生的分库分表解决方案，但引入成本较高。

本节主要关注自研路由层的实现思路。

### 2. 连接管理与路由实现

```go
package sharding

import (
	"database/sql"
	"fmt"
	"hash/crc32"
	"log"
	"sync"

	_ "github.com/go-sql-driver/mysql" // MySQL driver
)

// ShardConfig defines the sharding topology
type ShardConfig struct {
	DBCount    int                 // Total number of sharded databases
	TableCount int                 // Total number of sharded tables per database
	DSNMap     map[string]string // Map of logical DB name to DSN
	DBPools    map[string]*sql.DB // Connection pools for each database
	mu         sync.RWMutex      // Mutex for protecting DBPools
}

// NewShardConfig initializes a new ShardConfig
func NewShardConfig(dbCount, tableCount int, dsnMap map[string]string) (*ShardConfig, error) {
	sc := &ShardConfig{
		DBCount:    dbCount,
		TableCount: tableCount,
		DSNMap:     dsnMap,
		DBPools:    make(map[string]*sql.DB),
	}

	// Initialize connection pools for all databases
	for dbName, dsn := range dsnMap {
		db, err := sql.Open("mysql", dsn)
		if err != nil {
			return nil, fmt.Errorf("failed to open DB %s: %w", dbName, err)
		}
		db.SetMaxOpenConns(100) // Max open connections
		db.SetMaxIdleConns(10)  // Max idle connections
		db.SetConnMaxLifetime(5 * time.Minute) // Connection max lifetime
		sc.DBPools[dbName] = db
		log.Printf("Initialized DB pool for %s\n", dbName)
	}
	return sc, nil
}

// Close closes all database connections
func (sc *ShardConfig) Close() {
	sc.mu.Lock()
	defer sc.mu.Unlock()
	for dbName, db := range sc.DBPools {
		if err := db.Close(); err != nil {
			log.Printf("Error closing DB %s: %v\n", dbName, err)
		}
	}
}

// RouteDB calculates the database index based on sharding key
func (sc *ShardConfig) RouteDB(shardingKey string) string {
	hash := crc32.ChecksumIEEE([]byte(shardingKey))
	dbIndex := int(hash) % sc.DBCount
	return fmt.Sprintf("db_%d", dbIndex) // Assuming logical DB names are db_0, db_1, ...
}

// RouteTable calculates the table index based on sharding key
func (sc *ShardConfig) RouteTable(shardingKey string) string {
	hash := crc32.ChecksumIEEE([]byte(shardingKey))
	tableIndex := int(hash) % sc.TableCount
	return fmt.Sprintf("orders_%d", tableIndex) // Assuming table names are orders_0, orders_1, ...
}

// GetDB retrieves a database connection pool for a given sharding key
func (sc *ShardConfig) GetDB(shardingKey string) (*sql.DB, error) {
	dbName := sc.RouteDB(shardingKey)
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	db, ok := sc.DBPools[dbName]
	if !ok {
		return nil, fmt.Errorf("database pool not found for %s", dbName)
	}
	return db, nil
}

// GetActualTableName retrieves the actual table name for a given sharding key
func (sc *ShardConfig) GetActualTableName(baseTableName, shardingKey string) string {
	return fmt.Sprintf("%s_%s", baseTableName, sc.RouteTable(shardingKey))
}
```

### 3. CRUD操作示例

```go
package sharding

import (
	"database/sql"
	"fmt"
	"time"
)

// Order represents a simplified order struct
type Order struct {
	ID        int64
	UserID    int64
	Amount    float64
	Status    int
	CreatedAt time.Time
}

// OrderRepository handles CRUD operations for Order
type OrderRepository struct {
	sc *ShardConfig
}

// NewOrderRepository creates a new OrderRepository
func NewOrderRepository(sc *ShardConfig) *OrderRepository {
	return &OrderRepository{sc: sc}
}

// CreateOrder inserts a new order into the sharded database
func (r *OrderRepository) CreateOrder(order *Order) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", order.UserID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", order.ID))

	query := fmt.Sprintf("INSERT INTO %s (id, user_id, amount, status, created_at) VALUES (?, ?, ?, ?, ?)", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	_, err = stmt.Exec(order.ID, order.UserID, order.Amount, order.Status, order.CreatedAt)
	if err != nil {
		return fmt.Errorf("failed to execute insert: %w", err)
	}
	return nil
}

// GetOrderByID retrieves an order by its ID and UserID (requires sharding key for routing)
func (r *OrderRepository) GetOrderByID(orderID, userID int64) (*Order, error) {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return nil, err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("SELECT id, user_id, amount, status, created_at FROM %s WHERE id = ? AND user_id = ?", tableName)
	row := db.QueryRow(query, orderID, userID)

	order := &Order{}
	err = row.Scan(&order.ID, &order.UserID, &order.Amount, &order.Status, &order.CreatedAt)
	if err != nil {
		if err == sql.ErrNoRows {
			return nil, fmt.Errorf("order not found")
		}
		return nil, fmt.Errorf("failed to scan order: %w", err)
	}
	return order, nil
}

// UpdateOrderStatus updates the status of an order
func (r *OrderRepository) UpdateOrderStatus(orderID, userID int64, status int) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("UPDATE %s SET status = ? WHERE id = ? AND user_id = ?", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	res, err := stmt.Exec(status, orderID, userID)
	if err != nil {
		return fmt.Errorf("failed to execute update: %w", err)
	}
	rowsAffected, _ := res.RowsAffected()
	if rowsAffected == 0 {
		return fmt.Errorf("order %d not found or no changes made", orderID)
	}
	return nil
}

// DeleteOrder deletes an order
func (r *OrderRepository) DeleteOrder(orderID, userID int64) error {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return err
	}
	tableName := r.sc.GetActualTableName("orders", fmt.Sprintf("%d", orderID))

	query := fmt.Sprintf("DELETE FROM %s WHERE id = ? AND user_id = ?", tableName)
	stmt, err := db.Prepare(query)
	if err != nil {
		return fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	res, err := stmt.Exec(orderID, userID)
	if err != nil {
		return fmt.Errorf("failed to execute delete: %w", err)
	}
	rowsAffected, _ := res.RowsAffected()
	if rowsAffected == 0 {
		return fmt.Errorf("order %d not found", orderID)
	}
	return nil
}

// AggregateOrdersByUserID aggregates orders for a specific user across all their tables
func (r *OrderRepository) AggregateOrdersByUserID(userID int64) (int64, float64, error) {
	db, err := r.sc.GetDB(fmt.Sprintf("%d", userID))
	if err != nil {
		return 0, 0, err
	}

	var totalCount int64
	var totalAmount float64

	// Iterate through all possible tables for this user's DB shard
	for i := 0; i < r.sc.TableCount; i++ {
		tableName := fmt.Sprintf("orders_%d", i)
		query := fmt.Sprintf("SELECT COUNT(*), COALESCE(SUM(amount), 0) FROM %s WHERE user_id = ?", tableName)
		
		var count int64
		var amount float64
		err = db.QueryRow(query, userID).Scan(&count, &amount)
		if err != nil {
			return 0, 0, fmt.Errorf("failed to query aggregate for table %s: %w", tableName, err)
		}
		totalCount += count
		totalAmount += amount
	}
	return totalCount, totalAmount, nil
}

// GlobalAggregateOrders aggregates orders across all databases and tables (expensive operation)
func (r *OrderRepository) GlobalAggregateOrders() (int64, float64, error) {
	var totalCount int64
	var totalAmount float64
	var wg sync.WaitGroup
	resultChan := make(chan struct{ Count int64; Amount float64 }, r.sc.DBCount)
	errorChan := make(chan error, r.sc.DBCount)

	for dbName, db := range r.sc.DBPools {
		wg.Add(1)
		go func(db *sql.DB, dbName string) {
			defer wg.Done()
			var dbCount int64
			var dbAmount float64
			for i := 0; i < r.sc.TableCount; i++ {
				tableName := fmt.Sprintf("orders_%d", i)
				query := fmt.Sprintf("SELECT COUNT(*), COALESCE(SUM(amount), 0) FROM %s", tableName)
				var count int64
				var amount float64
				err := db.QueryRow(query).Scan(&count, &amount)
				if err != nil {
					errorChan <- fmt.Errorf("failed to query global aggregate for %s.%s: %w", dbName, tableName, err)
					return
				}
				dbCount += count
				dbAmount += amount
			}
			resultChan <- struct{ Count int64; Amount float64 }{Count: dbCount, Amount: dbAmount}
		}(db, dbName)
	}

	wg.Wait()
	close(resultChan)
	close(errorChan)

	select {
	case err := <-errorChan:
		return 0, 0, err
	default:
		// No error
	}

	for res := range resultChan {
		totalCount += res.Count
		totalAmount += res.Amount
	}
	return totalCount, totalAmount, nil
}
```

### 4. 分布式事务与全局唯一ID

**分布式事务：**

在分库分表场景下，跨库事务是复杂且难以避免的问题。Go语言中通常采用以下策略：

- **最终一致性：** 大多数互联网应用会选择最终一致性，通过消息队列（如Kafka, RabbitMQ）或事务消息（如RocketMQ事务消息）来实现。例如，订单创建后发送消息，库存服务消费消息扣减库存。
  - **优点：** 性能高，系统吞吐量大。
  - **缺点：** 业务需要处理数据不一致的中间状态，对业务侵入性强。
- **TCC (Try-Confirm-Cancel) 模式：** 适用于对一致性要求较高，但又不能接受2PC性能损耗的场景。业务层面需要实现Try、Confirm、Cancel三个操作。
  - **优点：** 解决了2PC的阻塞问题，业务侵入性适中。
  - **缺点：** 业务开发复杂，需要为每个操作实现TCC接口。
- **SAGA 模式：** 一系列本地事务的组合，每个本地事务都有一个对应的补偿操作。当某个本地事务失败时，通过执行之前已成功事务的补偿操作来回滚。
  - **优点：** 解决了长事务问题，无需锁定资源。
  - **缺点：** 补偿逻辑复杂，需要保证补偿操作的幂等性。

**全局唯一ID生成：**

分库分表后，数据库的自增ID无法保证全局唯一性。常见的解决方案有：

- **UUID：** 简单易用，但无序，作为主键会影响数据库索引性能。
- **Snowflake算法：** Twitter开源的分布式ID生成算法，生成的是趋势递增的64位整数ID，包含时间戳、机器ID、序列号等信息，适用于分布式环境。
  - **Go语言实现示例：**
    ```go
    package main
    
    import (
    	"fmt"
    	"sync"
    	"time"
    )
    
    const (
    	workerIDBits     = uint(5)  // 机器ID占5位
    	dataCenterIDBits = uint(5)  // 数据中心ID占5位
    	sequenceBits     = uint(12) // 序列号占12位
    
    	maxWorkerID     = int64(-1) ^ (int64(-1) << workerIDBits)
    	maxDataCenterID = int64(-1) ^ (int64(-1) << dataCenterIDBits)
    	maxSequence     = int64(-1) ^ (int64(-1) << sequenceBits)
    
    	workerIDShift     = sequenceBits
    	dataCenterIDShift = sequenceBits + workerIDBits
    	timestampShift    = sequenceBits + workerIDBits + dataCenterIDBits
    
    	epoch = int64(1672531200000) // 2023-01-01 00:00:00 UTC in milliseconds
    )
    
    // Snowflake struct
    type Snowflake struct {
    	mu           sync.Mutex
    	lastTimestamp int64
    	workerID      int64
    	dataCenterID  int64
    	sequence      int64
    }
    
    // NewSnowflake creates a new Snowflake instance
    func NewSnowflake(workerID, dataCenterID int64) (*Snowflake, error) {
    	if workerID < 0 || workerID > maxWorkerID {
    		return nil, fmt.Errorf("worker ID must be between 0 and %d", maxWorkerID)
    	}
    	if dataCenterID < 0 || dataCenterID > maxDataCenterID {
    		return nil, fmt.Errorf("data center ID must be between 0 and %d", maxDataCenterID)
    	}
    	return &Snowflake{
    		workerID:     workerID,
    		dataCenterID: dataCenterID,
    		lastTimestamp: -1,
    		sequence:     0,
    	}, nil
    }
    
    // Generate generates a unique ID
    func (sf *Snowflake) Generate() (int64, error) {
    	sf.mu.Lock()
    	defer sf.mu.Unlock()
    
    	now := time.Now().UnixNano() / 1e6 // Milliseconds
    
    	if now < sf.lastTimestamp {
    		return 0, fmt.Errorf("clock moved backwards. Refusing to generate id for %d milliseconds", sf.lastTimestamp-now)
    	}
    
    	if now == sf.lastTimestamp {
    		sf.sequence = (sf.sequence + 1) & maxSequence
    		if sf.sequence == 0 {
    			// Sequence exhausted, wait until next millisecond
    			for now <= sf.lastTimestamp {
    				now = time.Now().UnixNano() / 1e6
    			}
    		}
    	} else {
    		sf.sequence = 0
    	}
    
    	sf.lastTimestamp = now
    
    	id := ((now - epoch) << timestampShift) |
    		(sf.dataCenterID << dataCenterIDShift) |
    		(sf.workerID << workerIDShift) |
    		sf.sequence
    
    	return id, nil
    }
    
    // Example usage:
    // func main() {
    // 	sf, err := NewSnowflake(1, 1) // workerID=1, dataCenterID=1
    // 	if err != nil {
    // 		log.Fatalf("Error creating snowflake: %v", err)
    // 	}
    // 	for i := 0; i < 10; i++ {
    // 		id, err := sf.Generate()
    // 		if err != nil {
    // 			log.Printf("Error generating ID: %v", err)
    // 			continue
    // 		}
    // 		fmt.Printf("Generated ID: %d\n", id)
    // 	}
    // }
    ```
- **Redis自增ID：** 利用Redis的 `INCR` 命令生成，简单高效，但需要考虑Redis的持久化和高可用。
- **数据库序列/号段模式：** 预先在数据库中生成一段ID，服务每次取一段ID使用，用完再取。减少数据库压力，但需要保证号段服务的可用性。

### 5. 性能优化要点

- **合理选择分片键：** 确保数据分布均匀，避免热点。尽量选择查询条件中经常出现的字段作为分片键。
- **SQL优化：** 避免全表扫描，确保SQL语句能命中索引。针对跨库查询，尽量通过业务层聚合或引入数据仓库解决。
- **连接池优化：** 合理配置数据库连接池大小，避免频繁创建和销毁连接。
- **缓存策略：** 引入Redis等缓存，减少对数据库的直接访问，特别是针对热点数据和读多写少的场景。
- **批量操作：** 批量插入、批量更新，减少网络IO和数据库交互次数。
- **读写分离：** 在分库分表的基础上，每个分库再进行读写分离，进一步提升读并发能力。
- **异步化：** 将非核心、耗时的操作异步化，如日志记录、消息通知等，减少主流程的响应时间。
- **监控与告警：** 建立完善的监控体系，实时监控数据库性能指标（CPU、内存、IO、连接数、慢查询等），及时发现并解决问题。

### 6. 生产实践经验

- **灰度发布与A/B测试：** 对于分库分表这种核心架构变更，务必进行灰度发布，逐步验证新架构的稳定性和性能。
- **数据迁移方案：** 制定详细的数据迁移计划，包括全量迁移、增量同步、数据校验、回滚方案等。推荐双写模式平滑过渡。
- **容量规划与压测：** 提前进行容量规划，并进行充分的压力测试，验证系统在高并发下的表现，发现潜在瓶颈。
- **应急预案：** 针对可能出现的故障（如分片故障、中间件故障、数据不一致等）制定详细的应急预案和恢复流程。
- **自动化运维：** 结合Kubernetes、Ansible等工具，实现分库分表集群的自动化部署、扩容、缩容、备份和恢复。
- **团队协作与知识共享：** 分库分表涉及多个团队和模块，需要加强团队间的沟通协作，沉淀和共享知识。

### 7. 面试要点总结

- **分库分表解决了什么问题？** (性能、存储、可用性)
- **分库分表的策略有哪些？** (垂直分库、水平分库、水平分表、混合分片)
- **分片键如何选择？** (均匀分布、避免热点、查询路由)
- **全局唯一ID如何生成？** (UUID、Snowflake、Redis、号段模式)
- **分布式事务如何处理？** (2PC、TCC、SAGA、最终一致性)
- **分库分表带来的挑战及解决方案？** (跨库Join、跨库事务、扩容、热点数据、复杂查询)
- **常用的分库分表中间件有哪些？** (ShardingSphere、MyCat、Vitess等)
- **如何进行数据迁移和扩容？** (双写、渐进式迁移)
- **Go语言中如何实现分库分表？** (自研路由、ShardingSphere-Proxy、Vitess)

通过以上内容的补充，<mcfile name="分库分表方案.md" path="d:\ownCode\leetcode\system_design\分库分表方案.md"></mcfile>文档将更加全面和深入，不仅包含理论知识，还提供了Go语言的实战代码和生产实践经验，更符合资深架构师的视角。

### 1. 分片路由器

```go
package sharding

import (
    "crypto/md5"
    "fmt"
    "hash/crc32"
    "sort"
    "strconv"
    "time"
)

// ShardingRouter 分片路由接口
type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
    GetAllShards() []string
}

// HashRouter 哈希路由器
type HashRouter struct {
    DatabaseCount int
    TableCount    int
    DatabasePrefix string
    TablePrefix   string
}

func NewHashRouter(dbCount, tableCount int, dbPrefix, tablePrefix string) *HashRouter {
    return &HashRouter{
        DatabaseCount:  dbCount,
        TableCount:     tableCount,
        DatabasePrefix: dbPrefix,
        TablePrefix:    tablePrefix,
    }
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.DatabaseCount
    return fmt.Sprintf("%s_%d", r.DatabasePrefix, index)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.TableCount
    return fmt.Sprintf("%s_%d", r.TablePrefix, index)
}

func (r *HashRouter) GetAllShards() []string {
    var shards []string
    for i := 0; i < r.DatabaseCount; i++ {
        shards = append(shards, fmt.Sprintf("%s_%d", r.DatabasePrefix, i))
    }
    return shards
}

// RangeRouter 范围路由器
type RangeRouter struct {
    Ranges []ShardRange
}

type ShardRange struct {
    Start    int64
    End      int64
    Database string
    Table    string
}

func (r *RangeRouter) RouteDatabase(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Database
        }
    }
    return r.Ranges[len(r.Ranges)-1].Database // 默认最后一个
}

func (r *RangeRouter) RouteTable(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Table
        }
    }
    return r.Ranges[len(r.Ranges)-1].Table
}

// TimeRouter 时间路由器
type TimeRouter struct {
    DatabasePrefix string
    TablePrefix    string
    TimeFormat     string // "200601" 按月, "20060102" 按天
}

func (r *TimeRouter) RouteDatabase(shardingKey string) string {
    // 时间路由通常不分库，只分表
    return r.DatabasePrefix
}

func (r *TimeRouter) RouteTable(shardingKey string) string {
    timestamp, _ := strconv.ParseInt(shardingKey, 10, 64)
    t := time.Unix(timestamp, 0)
    suffix := t.Format(r.TimeFormat)
    return fmt.Sprintf("%s_%s", r.TablePrefix, suffix)
}
```

### 2. 分片数据源

```go
package sharding

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// ShardingDataSource 分片数据源
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
    healthCheck *HealthChecker
    mutex       sync.RWMutex
}

func NewShardingDataSource(router ShardingRouter) *ShardingDataSource {
    return &ShardingDataSource{
        dataSources: make(map[string]*sql.DB),
        router:      router,
        healthCheck: NewHealthChecker(),
    }
}

// AddDataSource 添加数据源
func (ds *ShardingDataSource) AddDataSource(name string, db *sql.DB) {
    ds.mutex.Lock()
    defer ds.mutex.Unlock()
    
    ds.dataSources[name] = db
    ds.healthCheck.AddShard(name, db)
}

// GetDataSource 获取数据源
func (ds *ShardingDataSource) GetDataSource(shardingKey string) (*sql.DB, string, error) {
    dbName := ds.router.RouteDatabase(shardingKey)
    tableName := ds.router.RouteTable(shardingKey)
    
    ds.mutex.RLock()
    db, exists := ds.dataSources[dbName]
    ds.mutex.RUnlock()
    
    if !exists {
        return nil, "", fmt.Errorf("database %s not found", dbName)
    }
    
    if !ds.healthCheck.IsHealthy(dbName) {
        return nil, "", fmt.Errorf("database %s is unhealthy", dbName)
    }
    
    return db, tableName, nil
}

// Execute 执行SQL
func (ds *ShardingDataSource) Execute(ctx context.Context, shardingKey, query string, args ...interface{}) (sql.Result, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    // 替换表名占位符
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.ExecContext(ctx, finalQuery, args...)
}

// Query 查询数据
func (ds *ShardingDataSource) Query(ctx context.Context, shardingKey, query string, args ...interface{}) (*sql.Rows, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.QueryContext(ctx, finalQuery, args...)
}

// CrossShardQuery 跨分片查询
func (ds *ShardingDataSource) CrossShardQuery(ctx context.Context, query string, args ...interface{}) ([]map[string]interface{}, error) {
    shards := ds.router.GetAllShards()
    
    type shardResult struct {
        data []map[string]interface{}
        err  error
    }
    
    resultChan := make(chan shardResult, len(shards))
    
    // 并行查询所有分片
    for _, shard := range shards {
        go func(shardName string) {
            db := ds.dataSources[shardName]
            if db == nil || !ds.healthCheck.IsHealthy(shardName) {
                resultChan <- shardResult{nil, fmt.Errorf("shard %s unavailable", shardName)}
                return
            }
            
            rows, err := db.QueryContext(ctx, query, args...)
            if err != nil {
                resultChan <- shardResult{nil, err}
                return
            }
            defer rows.Close()
            
            data, err := scanRowsToMap(rows)
            resultChan <- shardResult{data, err}
        }(shard)
    }
    
    // 收集结果
    var allResults []map[string]interface{}
    for i := 0; i < len(shards); i++ {
        result := <-resultChan
        if result.err != nil {
            return nil, result.err
        }
        allResults = append(allResults, result.data...)
    }
    
    return allResults, nil
}

// 健康检查器
type HealthChecker struct {
    shards map[string]*ShardHealth
    mutex  sync.RWMutex
}

type ShardHealth struct {
    DB        *sql.DB
    Healthy   bool
    LastCheck time.Time
}

func NewHealthChecker() *HealthChecker {
    hc := &HealthChecker{
        shards: make(map[string]*ShardHealth),
    }
    
    // 启动健康检查
    go hc.startHealthCheck()
    
    return hc
}

func (hc *HealthChecker) AddShard(name string, db *sql.DB) {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    hc.shards[name] = &ShardHealth{
        DB:        db,
        Healthy:   true,
        LastCheck: time.Now(),
    }
}

func (hc *HealthChecker) IsHealthy(name string) bool {
    hc.mutex.RLock()
    defer hc.mutex.RUnlock()
    
    shard, exists := hc.shards[name]
    return exists && shard.Healthy
}

func (hc *HealthChecker) startHealthCheck() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        hc.checkAllShards()
    }
}

func (hc *HealthChecker) checkAllShards() {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    for name, shard := range hc.shards {
        healthy := hc.pingShard(shard.DB)
        shard.Healthy = healthy
        shard.LastCheck = time.Now()
        
        if !healthy {
            fmt.Printf("Shard %s is unhealthy\n", name)
        }
    }
}

func (hc *HealthChecker) pingShard(db *sql.DB) bool {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    return db.PingContext(ctx) == nil
}

// 辅助函数：将查询结果转换为map
func scanRowsToMap(rows *sql.Rows) ([]map[string]interface{}, error) {
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var results []map[string]interface{}
    
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, err
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            row[col] = values[i]
        }
        
        results = append(results, row)
    }
    
    return results, rows.Err()
}
```

### 3. 分布式事务管理

```go
package transaction

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// TCC事务管理器
type TCCManager struct {
    transactions map[string]*TCCTransaction
    mutex        sync.RWMutex
}

type TCCTransaction struct {
    ID         string
    Operations []TCCOperation
    Status     TransactionStatus
    CreatedAt  time.Time
    UpdatedAt  time.Time
}

type TransactionStatus int

const (
    StatusPending TransactionStatus = iota
    StatusTrying
    StatusConfirming
    StatusCanceling
    StatusConfirmed
    StatusCanceled
    StatusFailed
)

type TCCOperation interface {
    Try(ctx context.Context) error
    Confirm(ctx context.Context) error
    Cancel(ctx context.Context) error
    GetResourceID() string
}

func NewTCCManager() *TCCManager {
    return &TCCManager{
        transactions: make(map[string]*TCCTransaction),
    }
}

func (tm *TCCManager) BeginTransaction(id string) *TCCTransaction {
    tm.mutex.Lock()
    defer tm.mutex.Unlock()
    
    tx := &TCCTransaction{
        ID:        id,
        Status:    StatusPending,
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }
    
    tm.transactions[id] = tx
    return tx
}

func (tx *TCCTransaction) AddOperation(op TCCOperation) {
    tx.Operations = append(tx.Operations, op)
}

func (tm *TCCManager) ExecuteTransaction(ctx context.Context, txID string) error {
    tm.mutex.Lock()
    tx, exists := tm.transactions[txID]
    if !exists {
        tm.mutex.Unlock()
        return fmt.Errorf("transaction %s not found", txID)
    }
    tx.Status = StatusTrying
    tx.UpdatedAt = time.Now()
    tm.mutex.Unlock()
    
    // Try阶段
    for i, op := range tx.Operations {
        if err := op.Try(ctx); err != nil {
            // Try失败，执行Cancel
            tm.cancelTransaction(ctx, tx, i)
            return err
        }
    }
    
    // Confirm阶段
    tx.Status = StatusConfirming
    for i, op := range tx.Operations {
        if err := op.Confirm(ctx); err != nil {
            // Confirm失败，需要人工干预
            tx.Status = StatusFailed
            return fmt.Errorf("confirm failed at operation %d: %v", i, err)
        }
    }
    
    tx.Status = StatusConfirmed
    tx.UpdatedAt = time.Now()
    
    return nil
}

func (tm *TCCManager) cancelTransaction(ctx context.Context, tx *TCCTransaction, failedIndex int) {
    tx.Status = StatusCanceling
    
    // 逆序取消已执行的操作
    for i := failedIndex - 1; i >= 0; i-- {
        if err := tx.Operations[i].Cancel(ctx); err != nil {
            // 记录取消失败的操作，需要人工干预
            fmt.Printf("Cancel operation %d failed: %v\n", i, err)
        }
    }
    
    tx.Status = StatusCanceled
    tx.UpdatedAt = time.Now()
}

// 账户转账TCC操作示例
type TransferOperation struct {
    FromAccount string
    ToAccount   string
    Amount      float64
    DS          *ShardingDataSource
    TxID        string
}

func (op *TransferOperation) GetResourceID() string {
    return fmt.Sprintf("%s-%s", op.FromAccount, op.ToAccount)
}

func (op *TransferOperation) Try(ctx context.Context) error {
    // 冻结转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount + ?, version = version + 1 WHERE account_id = ? AND balance >= ?"
    
    result, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount, op.Amount)
    if err != nil {
        return err
    }
    
    affected, _ := result.RowsAffected()
    if affected == 0 {
        return fmt.Errorf("insufficient balance or account not found")
    }
    
    // 记录TCC事务日志
    return op.recordTCCLog(ctx, "TRY", "SUCCESS")
}

func (op *TransferOperation) Confirm(ctx context.Context) error {
    // 扣减转出账户，增加转入账户
    
    // 1. 扣减转出账户
    deductQuery := "UPDATE accounts SET balance = balance - ?, frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    _, err := op.DS.Execute(ctx, op.FromAccount, deductQuery, op.Amount, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    // 2. 增加转入账户
    addQuery := "UPDATE accounts SET balance = balance + ?, version = version + 1 WHERE account_id = ?"
    _, err = op.DS.Execute(ctx, op.ToAccount, addQuery, op.Amount, op.ToAccount)
    if err != nil {
        return err
    }
    
    // 3. 记录转账流水
    err = op.recordTransferLog(ctx)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CONFIRM", "SUCCESS")
}

func (op *TransferOperation) Cancel(ctx context.Context) error {
    // 解冻转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    
    _, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CANCEL", "SUCCESS")
}

func (op *TransferOperation) recordTCCLog(ctx context.Context, action, status string) error {
    query := "INSERT INTO tcc_logs (tx_id, resource_id, action, status, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.GetResourceID(), action, status, time.Now())
    return err
}

func (op *TransferOperation) recordTransferLog(ctx context.Context) error {
    query := "INSERT INTO transfer_logs (tx_id, from_account, to_account, amount, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.FromAccount, op.ToAccount, op.Amount, time.Now())
    return err
}
```

## 总结

分库分表是解决大规模数据存储和高并发访问的重要技术手段，但也带来了系统复杂性的显著增加。在实施分库分表方案时，需要综合考虑业务特点、技术架构、运维成本等多个因素。

**关键要点：**

1. **合理的分片策略**：选择合适的分片键和算法
2. **数据一致性**：在性能和一致性之间找到平衡
3. **查询优化**：避免跨分片查询，合理使用缓存
4. **运维监控**：完善的监控和故障处理机制
5. **扩容规划**：提前规划扩容方案，避免后期重构

**最佳实践：**

- 优先考虑垂直分库，再考虑水平分库
- 分片数量选择2的幂次，便于扩容
- 避免分布式事务，采用最终一致性
- 建立完善的监控和告警机制
- 制定详细的数据迁移和扩容方案

通过合理的架构设计和技术选型，分库分表能够有效解决大规模系统的数据存储和访问问题，为业务的快速发展提供强有力的技术支撑。