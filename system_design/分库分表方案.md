# 分库分表方案设计

## 目录
- [概述与背景](#概述与背景)
- [底层原理](#底层原理)
- [技术实现](#技术实现)
- [解决的问题](#解决的问题)
- [方案设计](#方案设计)
- [MySQL实现细节](#mysql实现细节)
- [注意事项与坑点](#注意事项与坑点)
- [高频面试题](#高频面试题)
- [实战场景分析](#实战场景分析)
- [Go语言实现](#go语言实现)

## 概述与背景

### 什么是分库分表

分库分表是一种数据库水平扩展技术，通过将单一数据库的数据分散到多个数据库实例（分库）或将单一表的数据分散到多个表（分表）来解决单库单表的性能瓶颈。

### 实现背景

**业务发展驱动的技术需求：**

1. **数据量爆炸式增长**
   - 单表数据量超过千万级别
   - 存储空间不足
   - 索引效率急剧下降

2. **并发访问压力**
   - QPS/TPS超过单库承载能力
   - 连接数达到上限
   - 锁竞争激烈

3. **业务复杂度提升**
   - 多业务模块数据隔离需求
   - 不同业务的性能要求差异
   - 数据安全和合规要求

**技术演进路径：**
```
单库单表 → 读写分离 → 垂直分库 → 水平分库 → 分库分表
```

## 底层原理

### 1. 分库原理

**垂直分库（按业务模块）：**
```
原始架构：
┌─────────────────┐
│   单一数据库    │
│  ┌─────────────┐│
│  │ 用户表      ││
│  │ 订单表      ││
│  │ 商品表      ││
│  │ 支付表      ││
│  └─────────────┘│
└─────────────────┘

垂直分库后：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│  用户数据库  │ │  订单数据库  │ │  商品数据库  │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 订单表  │ │ │ │ 商品表  │ │
│ │ 用户详情│ │ │ │ 订单项  │ │ │ │ 商品详情│ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

**水平分库（按数据特征）：**
```
按用户ID分库：
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   DB_0      │ │   DB_1      │ │   DB_2      │
│ user_id%3=0 │ │ user_id%3=1 │ │ user_id%3=2 │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ 用户表  │ │ │ │ 用户表  │ │ │ │ 用户表  │ │
│ │ 订单表  │ │ │ │ 订单表  │ │ │ │ 订单表  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

### 2. 分表原理

**水平分表：**
```sql
-- 原始表
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

-- 分表后
CREATE TABLE orders_0 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);

CREATE TABLE orders_1 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2)
);
```

**垂直分表：**
```sql
-- 原始表（字段过多）
CREATE TABLE user_info (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    -- 基础信息
    real_name VARCHAR(50),
    id_card VARCHAR(18),
    -- 扩展信息（大字段）
    avatar LONGTEXT,
    description TEXT,
    preferences JSON
);

-- 垂直分表后
CREATE TABLE user_basic (
    id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    real_name VARCHAR(50),
    id_card VARCHAR(18)
);

CREATE TABLE user_profile (
    user_id BIGINT PRIMARY KEY,
    avatar LONGTEXT,
    description TEXT,
    preferences JSON,
    FOREIGN KEY (user_id) REFERENCES user_basic(id)
);
```

### 3. 路由算法

**哈希取模算法：**
```go
func HashRoute(shardKey string, shardCount int) int {
    hash := crc32.ChecksumIEEE([]byte(shardKey))
    return int(hash) % shardCount
}

// 示例：用户ID路由
userID := "12345"
dbIndex := HashRoute(userID, 4) // 分4个库
tableIndex := HashRoute(userID, 8) // 每库8张表
```

**范围路由算法：**
```go
func RangeRoute(value int64, ranges []Range) int {
    for i, r := range ranges {
        if value >= r.Start && value < r.End {
            return i
        }
    }
    return -1
}

// 示例：按时间范围路由
type Range struct {
    Start int64
    End   int64
}

ranges := []Range{
    {Start: 0, End: 202401},      // 2024年1月
    {Start: 202401, End: 202402}, // 2024年2月
    {Start: 202402, End: 202403}, // 2024年3月
}
```

**一致性哈希算法：**
```go
type ConsistentHash struct {
    ring     map[uint32]string
    sortedKeys []uint32
    replicas int
}

func (ch *ConsistentHash) Add(nodes ...string) {
    for _, node := range nodes {
        for i := 0; i < ch.replicas; i++ {
            key := ch.hash(fmt.Sprintf("%s:%d", node, i))
            ch.ring[key] = node
            ch.sortedKeys = append(ch.sortedKeys, key)
        }
    }
    sort.Slice(ch.sortedKeys, func(i, j int) bool {
        return ch.sortedKeys[i] < ch.sortedKeys[j]
    })
}

func (ch *ConsistentHash) Get(key string) string {
    if len(ch.ring) == 0 {
        return ""
    }
    
    hash := ch.hash(key)
    idx := sort.Search(len(ch.sortedKeys), func(i int) bool {
        return ch.sortedKeys[i] >= hash
    })
    
    if idx == len(ch.sortedKeys) {
        idx = 0
    }
    
    return ch.ring[ch.sortedKeys[idx]]
}
```

## 技术实现

### 1. 中间件架构

**代理层架构：**
```
┌─────────────┐
│ 应用层      │
└─────────────┘
       │
┌─────────────┐
│ 分库分表    │
│ 中间件      │ ← SQL解析、路由、聚合
└─────────────┘
       │
┌─────────────┐
│ 数据库集群  │
└─────────────┘
```

**主流中间件对比：**

| 中间件 | 类型 | 语言 | 特点 | 适用场景 |
|--------|------|------|------|----------|
| **ShardingSphere** | 客户端 | Java | 功能全面、生态完善 | 企业级应用 |
| **MyCat** | 代理 | Java | 独立部署、透明化 | 中大型系统 |
| **Vitess** | 代理 | Go | 云原生、高性能 | 大规模分布式 |
| **TDDL** | 客户端 | Java | 阿里开源、稳定 | 电商场景 |
| **Cobar** | 代理 | Java | 轻量级、简单 | 中小型系统 |

### 2. 分片策略

**按业务维度分片：**
```yaml
# 配置示例
sharding:
  databases:
    user_db:
      shardingColumn: user_id
      algorithmExpression: user_db_${user_id % 4}
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 4}
  
  tables:
    orders:
      actualDataNodes: order_db_${0..3}.orders_${0..7}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 4}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 8}
```

**按时间维度分片：**
```sql
-- 按月分表
CREATE TABLE orders_202401 (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    order_time DATETIME,
    amount DECIMAL(10,2),
    INDEX idx_user_time (user_id, order_time)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404)
);
```

## 解决的问题

### 1. 性能问题

**单表性能瓶颈：**
- **查询性能下降**：千万级数据查询缓慢
- **索引效率降低**：B+树层级增加，IO次数增多
- **锁竞争加剧**：表级锁、行级锁冲突频繁

**解决效果：**
```
分片前：
- 单表1000万数据
- 查询响应时间：500ms-2s
- QPS：1000

分片后（8个分片）：
- 单表125万数据
- 查询响应时间：50ms-200ms
- QPS：8000（理论值）
```

### 2. 存储问题

**存储容量限制：**
- MySQL单表建议不超过2000万行
- 单库容量建议不超过500GB
- 磁盘IO成为瓶颈

**解决方案：**
```
垂直分库：按业务模块分离
- 用户库：100GB
- 订单库：200GB
- 商品库：150GB
- 日志库：300GB

水平分库：按数据量分散
- 4个分库，每库负载降低75%
- 存储压力分散
- IO并发能力提升
```

### 3. 可用性问题

**单点故障风险：**
- 单库故障影响全业务
- 备份恢复时间长
- 维护窗口影响大

**高可用架构：**
```
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   分片1     │ │   分片2     │ │   分片3     │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Master  │ │ │ │ Master  │ │ │ │ Master  │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
│ ┌─────────┐ │ │ ┌─────────┐ │ │ ┌─────────┐ │
│ │ Slave   │ │ │ │ Slave   │ │ │ │ Slave   │ │
│ └─────────┘ │ │ └─────────┘ │ │ └─────────┘ │
└─────────────┘ └─────────────┘ └─────────────┘
```

## 方案设计

### 1. 垂直分库方案

**设计原则：**
- 按业务领域划分
- 减少跨库事务
- 便于团队协作

**实施步骤：**
```
1. 业务梳理
   ├── 用户域：用户信息、权限、认证
   ├── 商品域：商品信息、分类、库存
   ├── 订单域：订单、支付、物流
   └── 营销域：优惠券、活动、推荐

2. 数据库设计
   ├── user_db：用户相关表
   ├── product_db：商品相关表
   ├── order_db：订单相关表
   └── marketing_db：营销相关表

3. 应用改造
   ├── 数据源配置
   ├── DAO层改造
   ├── 事务处理
   └── 数据一致性
```

### 2. 水平分库分表方案

**分片键选择：**
```
常用分片键：
1. 用户ID：适合用户相关数据
2. 订单ID：适合订单相关数据
3. 时间：适合日志、流水数据
4. 地理位置：适合LBS应用
5. 业务ID：适合特定业务场景

选择原则：
- 数据分布均匀
- 查询路由简单
- 避免热点数据
- 支持范围查询
```

**分片数量规划：**
```go
// 分片数量计算
func CalculateShardCount(totalData, maxDataPerShard int64) int {
    shardCount := int(math.Ceil(float64(totalData) / float64(maxDataPerShard)))
    
    // 确保是2的幂次，便于扩容
    powerOf2 := 1
    for powerOf2 < shardCount {
        powerOf2 *= 2
    }
    
    return powerOf2
}

// 示例：
// 预计5年数据量：10亿
// 单表最大数据：1000万
// 计算结果：128个分片（2^7）
```

### 3. 混合分片方案

**垂直+水平分片：**
```
第一层：垂直分库（按业务）
├── user_cluster
│   ├── user_db_0 (user_id % 4 = 0)
│   ├── user_db_1 (user_id % 4 = 1)
│   ├── user_db_2 (user_id % 4 = 2)
│   └── user_db_3 (user_id % 4 = 3)
│
└── order_cluster
    ├── order_db_0 (user_id % 4 = 0)
    ├── order_db_1 (user_id % 4 = 1)
    ├── order_db_2 (user_id % 4 = 2)
    └── order_db_3 (user_id % 4 = 3)

第二层：水平分表（按数据量）
order_db_0:
├── orders_0 (order_id % 8 = 0)
├── orders_1 (order_id % 8 = 1)
├── ...
└── orders_7 (order_id % 8 = 7)
```

## MySQL实现细节

### 1. 分区表技术

**RANGE分区（按时间）：**
```sql
CREATE TABLE orders (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT DEFAULT 0,
    PRIMARY KEY (id, order_time),
    INDEX idx_user_id (user_id),
    INDEX idx_status (status)
) PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);

-- 自动分区管理
DELIMITER //
CREATE PROCEDURE CreateMonthlyPartition()
BEGIN
    DECLARE partition_name VARCHAR(20);
    DECLARE partition_value INT;
    
    SET partition_name = CONCAT('p', DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 1 MONTH), '%Y%m'));
    SET partition_value = DATE_FORMAT(DATE_ADD(NOW(), INTERVAL 2 MONTH), '%Y%m');
    
    SET @sql = CONCAT('ALTER TABLE orders ADD PARTITION (PARTITION ', partition_name, 
                     ' VALUES LESS THAN (', partition_value, '))');
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END //
DELIMITER ;

-- 定期执行
CREATE EVENT CreatePartitionEvent
ON SCHEDULE EVERY 1 MONTH
STARTS '2024-01-01 00:00:00'
DO CALL CreateMonthlyPartition();
```

**HASH分区（按ID）：**
```sql
CREATE TABLE user_logs (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    action VARCHAR(50),
    log_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, user_id),
    INDEX idx_user_time (user_id, log_time)
) PARTITION BY HASH(user_id)
PARTITIONS 16;

-- 查看分区信息
SELECT 
    PARTITION_NAME,
    TABLE_ROWS,
    DATA_LENGTH,
    INDEX_LENGTH
FROM INFORMATION_SCHEMA.PARTITIONS 
WHERE TABLE_NAME = 'user_logs';
```

**LIST分区（按地区）：**
```sql
CREATE TABLE regional_data (
    id BIGINT AUTO_INCREMENT,
    region_code VARCHAR(10),
    data_value TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (id, region_code)
) PARTITION BY LIST COLUMNS(region_code) (
    PARTITION p_north VALUES IN ('BJ', 'TJ', 'HE', 'SX', 'NM'),
    PARTITION p_east VALUES IN ('SH', 'JS', 'ZJ', 'AH', 'FJ', 'JX', 'SD'),
    PARTITION p_south VALUES IN ('GD', 'GX', 'HI'),
    PARTITION p_west VALUES IN ('CQ', 'SC', 'GZ', 'YN', 'XZ', 'SN', 'GS', 'QH', 'NX', 'XJ')
);
```

### 2. 分库分表实现

**数据库连接管理：**
```go
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
}

type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
}

type HashRouter struct {
    dbCount    int
    tableCount int
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    dbIndex := int(hash) % r.dbCount
    return fmt.Sprintf("db_%d", dbIndex)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    tableIndex := int(hash) % r.tableCount
    return fmt.Sprintf("orders_%d", tableIndex)
}

// 使用示例
func (ds *ShardingDataSource) Insert(userID string, order *Order) error {
    dbName := ds.router.RouteDatabase(userID)
    tableName := ds.router.RouteTable(userID)
    
    db := ds.dataSources[dbName]
    query := fmt.Sprintf("INSERT INTO %s (user_id, amount, status) VALUES (?, ?, ?)", tableName)
    
    _, err := db.Exec(query, order.UserID, order.Amount, order.Status)
    return err
}
```

### 3. 跨分片查询

**分片聚合查询：**
```go
type AggregateResult struct {
    TotalCount int64
    TotalAmount float64
    AvgAmount   float64
}

func (ds *ShardingDataSource) AggregateQuery(startTime, endTime time.Time) (*AggregateResult, error) {
    var wg sync.WaitGroup
    resultChan := make(chan *AggregateResult, len(ds.dataSources))
    errorChan := make(chan error, len(ds.dataSources))
    
    // 并行查询所有分片
    for dbName, db := range ds.dataSources {
        wg.Add(1)
        go func(dbName string, db *sql.DB) {
            defer wg.Done()
            
            result, err := ds.queryShardAggregate(db, startTime, endTime)
            if err != nil {
                errorChan <- err
                return
            }
            resultChan <- result
        }(dbName, db)
    }
    
    wg.Wait()
    close(resultChan)
    close(errorChan)
    
    // 检查错误
    select {
    case err := <-errorChan:
        return nil, err
    default:
    }
    
    // 聚合结果
    finalResult := &AggregateResult{}
    for result := range resultChan {
        finalResult.TotalCount += result.TotalCount
        finalResult.TotalAmount += result.TotalAmount
    }
    
    if finalResult.TotalCount > 0 {
        finalResult.AvgAmount = finalResult.TotalAmount / float64(finalResult.TotalCount)
    }
    
    return finalResult, nil
}

func (ds *ShardingDataSource) queryShardAggregate(db *sql.DB, startTime, endTime time.Time) (*AggregateResult, error) {
    // 查询所有分表
    tables := []string{"orders_0", "orders_1", "orders_2", "orders_3"}
    
    var totalCount int64
    var totalAmount float64
    
    for _, table := range tables {
        query := fmt.Sprintf(`
            SELECT COUNT(*), COALESCE(SUM(amount), 0)
            FROM %s 
            WHERE order_time >= ? AND order_time < ?
        `, table)
        
        var count int64
        var amount float64
        err := db.QueryRow(query, startTime, endTime).Scan(&count, &amount)
        if err != nil {
            return nil, err
        }
        
        totalCount += count
        totalAmount += amount
    }
    
    return &AggregateResult{
        TotalCount:  totalCount,
        TotalAmount: totalAmount,
    }, nil
}
```

### 4. 分布式事务

**两阶段提交（2PC）：**
```go
type DistributedTransaction struct {
    dataSources map[string]*sql.DB
    transactions map[string]*sql.Tx
}

func (dt *DistributedTransaction) Begin() error {
    dt.transactions = make(map[string]*sql.Tx)
    
    for name, db := range dt.dataSources {
        tx, err := db.Begin()
        if err != nil {
            // 回滚已开始的事务
            dt.Rollback()
            return err
        }
        dt.transactions[name] = tx
    }
    
    return nil
}

func (dt *DistributedTransaction) Commit() error {
    // 第一阶段：准备提交
    for name, tx := range dt.transactions {
        _, err := tx.Exec("XA PREPARE ?;", name)
        if err != nil {
            dt.Rollback()
            return err
        }
    }
    
    // 第二阶段：提交
    for name, tx := range dt.transactions {
        err := tx.Commit()
        if err != nil {
            // 记录日志，需要人工干预
            log.Printf("Failed to commit transaction %s: %v", name, err)
            return err
        }
    }
    
    return nil
}

func (dt *DistributedTransaction) Rollback() {
    for _, tx := range dt.transactions {
        tx.Rollback()
    }
}
```

**TCC模式（Try-Confirm-Cancel）：**
```go
type TCCTransaction struct {
    operations []TCCOperation
}

type TCCOperation interface {
    Try() error
    Confirm() error
    Cancel() error
}

type TransferOperation struct {
    fromAccount string
    toAccount   string
    amount      float64
    ds          *ShardingDataSource
}

func (op *TransferOperation) Try() error {
    // 冻结转出账户金额
    return op.ds.FreezeAmount(op.fromAccount, op.amount)
}

func (op *TransferOperation) Confirm() error {
    // 确认转账
    err := op.ds.DeductAmount(op.fromAccount, op.amount)
    if err != nil {
        return err
    }
    return op.ds.AddAmount(op.toAccount, op.amount)
}

func (op *TransferOperation) Cancel() error {
    // 解冻金额
    return op.ds.UnfreezeAmount(op.fromAccount, op.amount)
}

func (tcc *TCCTransaction) Execute() error {
    // Try阶段
    for _, op := range tcc.operations {
        if err := op.Try(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    // Confirm阶段
    for _, op := range tcc.operations {
        if err := op.Confirm(); err != nil {
            tcc.cancelAll()
            return err
        }
    }
    
    return nil
}
```

## 注意事项与坑点

### 1. 分片键选择陷阱

**热点数据问题：**
```go
// 错误示例：按时间分片导致热点
func BadTimeSharding(timestamp time.Time) string {
    // 所有当前数据都写入同一分片
    return fmt.Sprintf("shard_%s", timestamp.Format("2006-01-02"))
}

// 正确示例：组合分片键
func GoodSharding(userID int64, timestamp time.Time) string {
    // 用户ID保证分散，时间用于查询优化
    shardIndex := userID % 16
    return fmt.Sprintf("shard_%d", shardIndex)
}
```

**数据倾斜问题：**
```sql
-- 监控分片数据分布
SELECT 
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb
FROM information_schema.tables 
WHERE table_schema = 'order_db' 
AND table_name LIKE 'orders_%'
ORDER BY table_rows DESC;

-- 结果分析
-- orders_0: 1,500,000 rows (数据倾斜)
-- orders_1: 800,000 rows
-- orders_2: 900,000 rows
-- orders_3: 750,000 rows
```

### 2. 跨分片查询复杂性

**JOIN查询限制：**
```sql
-- 无法执行的跨分片JOIN
SELECT u.username, o.amount
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.order_time >= '2024-01-01';

-- 解决方案1：应用层JOIN
-- 1. 先查询orders表获取user_id列表
-- 2. 再查询users表获取用户信息
-- 3. 应用层组装数据

-- 解决方案2：数据冗余
CREATE TABLE orders (
    id BIGINT PRIMARY KEY,
    user_id BIGINT,
    username VARCHAR(50), -- 冗余用户名
    amount DECIMAL(10,2),
    order_time DATETIME
);
```

**分页查询问题：**
```go
// 错误的分页实现
func BadPagination(page, size int) ([]Order, error) {
    var allOrders []Order
    
    // 从每个分片查询
    for _, db := range dataSources {
        orders, err := queryFromShard(db, page, size)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 问题：数据量不准确，排序错误
    return allOrders, nil
}

// 正确的分页实现
func GoodPagination(page, size int) ([]Order, error) {
    // 1. 从每个分片查询更多数据
    shardSize := size * len(dataSources)
    
    var allOrders []Order
    for _, db := range dataSources {
        orders, err := queryFromShard(db, 1, shardSize)
        if err != nil {
            return nil, err
        }
        allOrders = append(allOrders, orders...)
    }
    
    // 2. 全局排序
    sort.Slice(allOrders, func(i, j int) bool {
        return allOrders[i].OrderTime.After(allOrders[j].OrderTime)
    })
    
    // 3. 分页截取
    start := (page - 1) * size
    end := start + size
    if start >= len(allOrders) {
        return []Order{}, nil
    }
    if end > len(allOrders) {
        end = len(allOrders)
    }
    
    return allOrders[start:end], nil
}
```

### 3. 数据一致性问题

**分布式事务性能问题：**
```go
// 避免分布式事务的设计
type OrderService struct {
    orderDB   *sql.DB
    accountDB *sql.DB
    stockDB   *sql.DB
}

// 错误：强一致性要求导致分布式事务
func (s *OrderService) CreateOrderWithTransaction(order *Order) error {
    // 需要跨3个数据库的事务
    // 1. 扣减库存 (stockDB)
    // 2. 扣减余额 (accountDB)
    // 3. 创建订单 (orderDB)
    
    // 分布式事务复杂且性能差
    return s.distributedTransaction(order)
}

// 正确：最终一致性设计
func (s *OrderService) CreateOrderEventually(order *Order) error {
    // 1. 先创建订单（状态：待支付）
    err := s.createPendingOrder(order)
    if err != nil {
        return err
    }
    
    // 2. 异步处理库存和支付
    s.publishOrderCreatedEvent(order)
    
    return nil
}

func (s *OrderService) handleOrderCreatedEvent(order *Order) {
    // 异步处理，允许重试
    if err := s.reserveStock(order); err != nil {
        s.retryLater(order, "reserve_stock")
        return
    }
    
    if err := s.processPayment(order); err != nil {
        s.releaseStock(order)
        s.retryLater(order, "process_payment")
        return
    }
    
    // 更新订单状态为已支付
    s.updateOrderStatus(order.ID, "paid")
}
```

### 4. 扩容缩容问题

**数据迁移复杂性：**
```go
// 扩容方案：双写+迁移
type MigrationManager struct {
    oldShards []ShardInfo
    newShards []ShardInfo
    migrating bool
}

func (m *MigrationManager) Write(key string, data interface{}) error {
    if !m.migrating {
        // 正常写入
        return m.writeToShard(m.getOldShard(key), data)
    }
    
    // 迁移期间双写
    oldShard := m.getOldShard(key)
    newShard := m.getNewShard(key)
    
    // 先写新分片
    if err := m.writeToShard(newShard, data); err != nil {
        return err
    }
    
    // 再写旧分片（允许失败）
    m.writeToShard(oldShard, data)
    
    return nil
}

func (m *MigrationManager) Read(key string) (interface{}, error) {
    if !m.migrating {
        return m.readFromShard(m.getOldShard(key), key)
    }
    
    // 迁移期间优先读新分片
    newShard := m.getNewShard(key)
    data, err := m.readFromShard(newShard, key)
    if err == nil {
        return data, nil
    }
    
    // 新分片没有则读旧分片
    oldShard := m.getOldShard(key)
    return m.readFromShard(oldShard, key)
}
```

### 5. 运维复杂性

**监控指标：**
```sql
-- 分片性能监控
SELECT 
    table_schema,
    table_name,
    table_rows,
    ROUND(data_length/1024/1024, 2) as data_mb,
    ROUND(index_length/1024/1024, 2) as index_mb,
    ROUND((data_length + index_length)/1024/1024, 2) as total_mb
FROM information_schema.tables 
WHERE table_schema LIKE '%_db_%'
ORDER BY total_mb DESC;

-- 慢查询监控
SELECT 
    db,
    sql_text,
    exec_count,
    avg_timer_wait/1000000000 as avg_time_sec,
    max_timer_wait/1000000000 as max_time_sec
FROM performance_schema.events_statements_summary_by_digest
WHERE avg_timer_wait > 1000000000 -- 超过1秒
ORDER BY avg_timer_wait DESC
LIMIT 10;
```

**备份策略：**
```bash
#!/bin/bash
# 分片备份脚本

SHARD_COUNT=4
BACKUP_DIR="/backup/$(date +%Y%m%d)"

mkdir -p $BACKUP_DIR

# 并行备份所有分片
for i in $(seq 0 $((SHARD_COUNT-1))); do
    {
        echo "Backing up shard $i..."
        mysqldump -h db_$i.example.com -u backup_user -p$BACKUP_PASSWORD \
            --single-transaction \
            --routines \
            --triggers \
            order_db_$i > $BACKUP_DIR/order_db_$i.sql
        
        if [ $? -eq 0 ]; then
            echo "Shard $i backup completed"
            gzip $BACKUP_DIR/order_db_$i.sql
        else
            echo "Shard $i backup failed"
        fi
    } &
done

wait
echo "All shards backup completed"
```

## 高频面试题

### 1. 基础概念题

**Q1: 什么是分库分表？为什么需要分库分表？**

**答案：**
分库分表是一种数据库水平扩展技术，通过将数据分散到多个数据库实例或表中来解决单库单表的性能瓶颈。

**需要分库分表的原因：**
1. **数据量过大**：单表超过千万级别，查询性能下降
2. **并发压力**：单库QPS/TPS达到瓶颈
3. **存储限制**：单库容量接近上限
4. **可用性要求**：避免单点故障

**Q2: 垂直分库和水平分库的区别是什么？**

**答案：**

| 维度 | 垂直分库 | 水平分库 |
|------|----------|----------|
| **分割方式** | 按业务模块分割 | 按数据特征分割 |
| **数据分布** | 不同业务在不同库 | 同业务数据分散到多库 |
| **查询特点** | 减少跨库查询 | 需要路由到正确分片 |
| **扩展性** | 受业务模块限制 | 可无限水平扩展 |
| **复杂度** | 相对简单 | 路由和聚合复杂 |

**Q3: 常见的分片算法有哪些？各有什么优缺点？**

**答案：**

1. **哈希取模算法**
   - 优点：分布均匀、实现简单
   - 缺点：扩容困难、无法范围查询

2. **范围分片算法**
   - 优点：支持范围查询、扩容相对容易
   - 缺点：可能数据倾斜、热点问题

3. **一致性哈希算法**
   - 优点：扩容影响小、负载均衡好
   - 缺点：实现复杂、可能数据倾斜

4. **目录映射算法**
   - 优点：灵活性高、支持复杂路由
   - 缺点：需要额外存储、单点风险

### 2. 技术实现题

**Q4: 如何解决分库分表后的跨分片查询问题？**

**答案：**

1. **应用层聚合**
   ```go
   func CrossShardQuery(condition QueryCondition) ([]Result, error) {
       var results []Result
       var wg sync.WaitGroup
       resultChan := make(chan []Result, len(shards))
       
       // 并行查询所有分片
       for _, shard := range shards {
           wg.Add(1)
           go func(s Shard) {
               defer wg.Done()
               result := s.Query(condition)
               resultChan <- result
           }(shard)
       }
       
       wg.Wait()
       close(resultChan)
       
       // 聚合结果
       for result := range resultChan {
           results = append(results, result...)
       }
       
       return results, nil
   }
   ```

2. **数据冗余**
   - 在需要JOIN的表中冗余关联字段
   - 使用宽表设计减少关联查询

3. **搜索引擎**
   - 使用Elasticsearch等搜索引擎
   - 异步同步数据到搜索引擎

**Q5: 分库分表后如何保证数据一致性？**

**答案：**

1. **避免分布式事务**
   - 通过业务设计避免跨分片事务
   - 使用最终一致性代替强一致性

2. **分布式事务方案**
   ```go
   // TCC模式示例
   type TCCManager struct {
       operations []TCCOperation
   }
   
   func (m *TCCManager) Execute() error {
       // Try阶段
       for _, op := range m.operations {
           if err := op.Try(); err != nil {
               m.cancelAll()
               return err
           }
       }
       
       // Confirm阶段
       for _, op := range m.operations {
           if err := op.Confirm(); err != nil {
               // 需要补偿机制
               return err
           }
       }
       
       return nil
   }
   ```

3. **消息队列保证最终一致性**
   - 使用可靠消息投递
   - 实现幂等性处理
   - 补偿机制处理异常

### 3. 架构设计题

**Q6: 设计一个电商订单系统的分库分表方案**

**答案：**

**业务分析：**
- 订单数据量大，增长快
- 查询模式：按用户查询、按时间范围查询
- 写入频繁，读取也频繁

**分片策略：**
```
1. 垂直分库：
   - user_db：用户相关数据
   - order_db：订单相关数据
   - product_db：商品相关数据
   - payment_db：支付相关数据

2. 水平分库分表：
   - 分片键：user_id（保证用户相关数据在同一分片）
   - 分库：4个order_db实例
   - 分表：每库8张orders表
   - 路由算法：user_id % 4 确定库，order_id % 8 确定表

3. 数据分布：
   order_db_0:
   ├── orders_0, orders_1, ..., orders_7
   ├── order_items_0, order_items_1, ..., order_items_7
   └── order_logs_0, order_logs_1, ..., order_logs_7
```

**技术架构：**
```go
type OrderShardingService struct {
    dataSources map[string]*sql.DB
    router      *OrderRouter
}

type OrderRouter struct {
    dbCount    int
    tableCount int
}

func (r *OrderRouter) Route(userID int64) (string, string) {
    dbIndex := userID % int64(r.dbCount)
    tableIndex := userID % int64(r.tableCount)
    
    dbName := fmt.Sprintf("order_db_%d", dbIndex)
    tableName := fmt.Sprintf("orders_%d", tableIndex)
    
    return dbName, tableName
}
```

**Q7: 如何设计分库分表的扩容方案？**

**答案：**

**扩容策略：**
1. **倍数扩容**：从N个分片扩容到2N个分片
2. **数据迁移**：只需迁移50%的数据
3. **双写方案**：保证服务不中断

**实施步骤：**
```
1. 准备阶段：
   - 部署新的数据库实例
   - 创建新的分片表结构
   - 配置数据同步工具

2. 迁移阶段：
   - 开启双写模式（新旧分片同时写入）
   - 历史数据迁移（按分片键重新路由）
   - 数据一致性校验

3. 切换阶段：
   - 更新路由配置
   - 停止旧分片写入
   - 验证新分片数据完整性

4. 清理阶段：
   - 删除旧分片冗余数据
   - 回收旧分片资源
```

**双写实现：**
```go
type MigrationWriter struct {
    oldRouter ShardingRouter
    newRouter ShardingRouter
    migrating bool
}

func (w *MigrationWriter) Write(key string, data interface{}) error {
    if !w.migrating {
        return w.writeToOldShard(key, data)
    }
    
    // 双写模式
    newShard := w.newRouter.Route(key)
    if err := w.writeToShard(newShard, data); err != nil {
        return err
    }
    
    oldShard := w.oldRouter.Route(key)
    w.writeToShard(oldShard, data) // 允许失败
    
    return nil
}
```

### 4. 性能优化题

**Q8: 分库分表后如何优化查询性能？**

**答案：**

1. **索引优化**
   ```sql
   -- 分片键必须是索引的前缀
   CREATE INDEX idx_user_time ON orders(user_id, order_time);
   
   -- 覆盖索引减少回表
   CREATE INDEX idx_user_status_amount ON orders(user_id, status, amount);
   ```

2. **查询路由优化**
   ```go
   func OptimizedQuery(userID int64, status string) ([]Order, error) {
       // 精确路由到单个分片
       dbName, tableName := router.Route(userID)
       db := dataSources[dbName]
       
       query := fmt.Sprintf(
           "SELECT * FROM %s WHERE user_id = ? AND status = ?",
           tableName)
       
       return queryOrders(db, query, userID, status)
   }
   ```

3. **缓存策略**
   ```go
   func QueryWithCache(userID int64) ([]Order, error) {
       // 先查缓存
       cacheKey := fmt.Sprintf("user_orders:%d", userID)
       if orders := cache.Get(cacheKey); orders != nil {
           return orders.([]Order), nil
       }
       
       // 缓存未命中，查询数据库
       orders, err := queryFromDB(userID)
       if err != nil {
           return nil, err
       }
       
       // 写入缓存
       cache.Set(cacheKey, orders, 5*time.Minute)
       return orders, nil
   }
   ```

4. **读写分离**
   ```go
   type ShardCluster struct {
       master *sql.DB
       slaves []*sql.DB
   }
   
   func (c *ShardCluster) Read(query string, args ...interface{}) (*sql.Rows, error) {
       // 负载均衡选择从库
       slave := c.slaves[rand.Intn(len(c.slaves))]
       return slave.Query(query, args...)
   }
   
   func (c *ShardCluster) Write(query string, args ...interface{}) (sql.Result, error) {
       // 写入主库
       return c.master.Exec(query, args...)
   }
   ```

### 5. 故障处理题

**Q9: 分库分表环境下如何处理单个分片故障？**

**答案：**

1. **故障检测**
   ```go
   type HealthChecker struct {
       shards map[string]*ShardInfo
       mutex  sync.RWMutex
   }
   
   func (hc *HealthChecker) CheckHealth() {
       for name, shard := range hc.shards {
           go func(name string, shard *ShardInfo) {
               if err := shard.Ping(); err != nil {
                   hc.markUnhealthy(name)
                   log.Printf("Shard %s is unhealthy: %v", name, err)
               } else {
                   hc.markHealthy(name)
               }
           }(name, shard)
       }
   }
   ```

2. **故障转移**
   ```go
   func (ds *ShardingDataSource) Query(shardKey string, query string) (*sql.Rows, error) {
       primaryShard := ds.router.Route(shardKey)
       
       // 尝试主分片
       if ds.isHealthy(primaryShard) {
           return ds.queryFromShard(primaryShard, query)
       }
       
       // 主分片故障，尝试备份分片
       backupShard := ds.getBackupShard(primaryShard)
       if backupShard != "" && ds.isHealthy(backupShard) {
           return ds.queryFromShard(backupShard, query)
       }
       
       return nil, errors.New("all shards for this key are unavailable")
   }
   ```

3. **数据恢复**
   ```bash
   # 从备份恢复分片
   #!/bin/bash
   FAILED_SHARD="order_db_2"
   BACKUP_FILE="/backup/order_db_2_20240115.sql.gz"
   
   # 1. 停止应用对该分片的写入
   echo "Marking shard $FAILED_SHARD as readonly"
   
   # 2. 恢复数据
   echo "Restoring shard $FAILED_SHARD from backup"
   gunzip < $BACKUP_FILE | mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 3. 同步增量数据（从binlog）
   echo "Syncing incremental data"
   mysqlbinlog --start-datetime="2024-01-15 10:00:00" \
               --database=$FAILED_SHARD \
               /var/log/mysql/mysql-bin.000001 | \
               mysql -h new_db_host -u admin -p $FAILED_SHARD
   
   # 4. 验证数据一致性
   echo "Verifying data consistency"
   
   # 5. 恢复服务
   echo "Marking shard $FAILED_SHARD as healthy"
   ```

## 实战场景分析

### 场景1：电商订单系统

**业务特点：**
- 订单数据量：日增100万+
- 查询模式：用户查询自己订单、商家查询店铺订单
- 峰值特点：促销期间写入量暴增

**分片方案：**
```yaml
# 分片配置
sharding:
  databases:
    order_db:
      shardingColumn: user_id
      algorithmExpression: order_db_${user_id % 8}
      
  tables:
    orders:
      actualDataNodes: order_db_${0..7}.orders_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: orders_${order_id % 16}
        
    order_items:
      actualDataNodes: order_db_${0..7}.order_items_${0..15}
      databaseStrategy:
        shardingColumn: user_id
        algorithmExpression: order_db_${user_id % 8}
      tableStrategy:
        shardingColumn: order_id
        algorithmExpression: order_items_${order_id % 16}
```

**Go实现：**
```go
type OrderService struct {
    shardingDS *ShardingDataSource
    cache      *redis.Client
}

// 创建订单
func (s *OrderService) CreateOrder(ctx context.Context, order *Order) error {
    // 生成分布式ID
    order.ID = s.generateOrderID()
    
    // 开启事务
    tx, err := s.shardingDS.BeginTx(ctx, order.UserID)
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    // 插入订单主表
    err = s.insertOrder(ctx, tx, order)
    if err != nil {
        return err
    }
    
    // 插入订单明细
    for _, item := range order.Items {
        err = s.insertOrderItem(ctx, tx, item)
        if err != nil {
            return err
        }
    }
    
    // 提交事务
    if err = tx.Commit(); err != nil {
        return err
    }
    
    // 异步更新缓存
    go s.updateOrderCache(order)
    
    return nil
}

// 查询用户订单
func (s *OrderService) GetUserOrders(ctx context.Context, userID int64, page, size int) ([]Order, error) {
    // 先查缓存
    cacheKey := fmt.Sprintf("user_orders:%d:%d:%d", userID, page, size)
    if cached := s.cache.Get(ctx, cacheKey).Val(); cached != "" {
        var orders []Order
        json.Unmarshal([]byte(cached), &orders)
        return orders, nil
    }
    
    // 缓存未命中，查询数据库
    orders, err := s.queryUserOrders(ctx, userID, page, size)
    if err != nil {
        return nil, err
    }
    
    // 写入缓存
    data, _ := json.Marshal(orders)
    s.cache.Set(ctx, cacheKey, data, 5*time.Minute)
    
    return orders, nil
}
```

### 场景2：金融交易系统

**业务特点：**
- 交易数据：强一致性要求
- 查询模式：按用户、按时间、按交易类型
- 监管要求：数据不可篡改、完整审计

**分片策略：**
```go
// 按用户+时间双维度分片
type FinanceRouter struct {
    userShardCount int
    timeShardCount int
}

func (r *FinanceRouter) Route(userID int64, timestamp time.Time) (string, string) {
    // 用户维度分库
    userShard := userID % int64(r.userShardCount)
    
    // 时间维度分表
    timeKey := timestamp.Format("200601") // 按月分表
    
    dbName := fmt.Sprintf("finance_db_%d", userShard)
    tableName := fmt.Sprintf("transactions_%s", timeKey)
    
    return dbName, tableName
}
```

**事务处理：**
```go
type TransactionService struct {
    shardingDS *ShardingDataSource
    sagaManager *SagaManager
}

// 转账操作（Saga模式）
func (s *TransactionService) Transfer(ctx context.Context, req *TransferRequest) error {
    saga := s.sagaManager.NewSaga()
    
    // 步骤1：冻结转出账户金额
    saga.AddStep(
        func() error { return s.freezeAmount(req.FromAccount, req.Amount) },
        func() error { return s.unfreezeAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤2：扣减转出账户
    saga.AddStep(
        func() error { return s.deductAmount(req.FromAccount, req.Amount) },
        func() error { return s.addAmount(req.FromAccount, req.Amount) },
    )
    
    // 步骤3：增加转入账户
    saga.AddStep(
        func() error { return s.addAmount(req.ToAccount, req.Amount) },
        func() error { return s.deductAmount(req.ToAccount, req.Amount) },
    )
    
    // 步骤4：记录交易流水
    saga.AddStep(
        func() error { return s.recordTransaction(req) },
        func() error { return s.cancelTransaction(req.TransactionID) },
    )
    
    return saga.Execute(ctx)
}
```

### 场景3：日志分析系统

**业务特点：**
- 数据量：TB级别日志数据
- 写入：高并发写入，批量导入
- 查询：按时间范围、按关键字搜索

**分片策略：**
```sql
-- 按时间分表（天级别）
CREATE TABLE access_logs_20240115 (
    id BIGINT AUTO_INCREMENT,
    user_id BIGINT,
    ip_address VARCHAR(45),
    user_agent TEXT,
    request_url VARCHAR(500),
    response_code INT,
    response_time INT,
    log_time DATETIME,
    PRIMARY KEY (id, log_time),
    INDEX idx_user_time (user_id, log_time),
    INDEX idx_ip_time (ip_address, log_time),
    INDEX idx_response_code (response_code)
) PARTITION BY RANGE (TO_DAYS(log_time)) (
    PARTITION p20240115 VALUES LESS THAN (TO_DAYS('2024-01-16')),
    PARTITION p20240116 VALUES LESS THAN (TO_DAYS('2024-01-17')),
    PARTITION p20240117 VALUES LESS THAN (TO_DAYS('2024-01-18'))
);
```

**批量写入优化：**
```go
type LogService struct {
    shardingDS *ShardingDataSource
    buffer     *LogBuffer
}

type LogBuffer struct {
    logs   []AccessLog
    mutex  sync.Mutex
    ticker *time.Ticker
}

// 异步批量写入
func (s *LogService) WriteLog(log *AccessLog) {
    s.buffer.Add(log)
}

func (b *LogBuffer) Add(log *AccessLog) {
    b.mutex.Lock()
    defer b.mutex.Unlock()
    
    b.logs = append(b.logs, *log)
    
    // 达到批量大小或时间间隔，触发写入
    if len(b.logs) >= 1000 {
        go b.flush()
    }
}

func (b *LogBuffer) flush() {
    b.mutex.Lock()
    logs := make([]AccessLog, len(b.logs))
    copy(logs, b.logs)
    b.logs = b.logs[:0] // 清空缓冲区
    b.mutex.Unlock()
    
    // 按分片分组
    shardGroups := make(map[string][]AccessLog)
    for _, log := range logs {
        shard := b.getShardKey(log.LogTime)
        shardGroups[shard] = append(shardGroups[shard], log)
    }
    
    // 并行写入各分片
    var wg sync.WaitGroup
    for shard, shardLogs := range shardGroups {
        wg.Add(1)
        go func(shard string, logs []AccessLog) {
            defer wg.Done()
            b.batchInsert(shard, logs)
        }(shard, shardLogs)
    }
    wg.Wait()
}
```

## Go语言实现

### 1. 分片路由器

```go
package sharding

import (
    "crypto/md5"
    "fmt"
    "hash/crc32"
    "sort"
    "strconv"
    "time"
)

// ShardingRouter 分片路由接口
type ShardingRouter interface {
    RouteDatabase(shardingKey string) string
    RouteTable(shardingKey string) string
    GetAllShards() []string
}

// HashRouter 哈希路由器
type HashRouter struct {
    DatabaseCount int
    TableCount    int
    DatabasePrefix string
    TablePrefix   string
}

func NewHashRouter(dbCount, tableCount int, dbPrefix, tablePrefix string) *HashRouter {
    return &HashRouter{
        DatabaseCount:  dbCount,
        TableCount:     tableCount,
        DatabasePrefix: dbPrefix,
        TablePrefix:    tablePrefix,
    }
}

func (r *HashRouter) RouteDatabase(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.DatabaseCount
    return fmt.Sprintf("%s_%d", r.DatabasePrefix, index)
}

func (r *HashRouter) RouteTable(shardingKey string) string {
    hash := crc32.ChecksumIEEE([]byte(shardingKey))
    index := int(hash) % r.TableCount
    return fmt.Sprintf("%s_%d", r.TablePrefix, index)
}

func (r *HashRouter) GetAllShards() []string {
    var shards []string
    for i := 0; i < r.DatabaseCount; i++ {
        shards = append(shards, fmt.Sprintf("%s_%d", r.DatabasePrefix, i))
    }
    return shards
}

// RangeRouter 范围路由器
type RangeRouter struct {
    Ranges []ShardRange
}

type ShardRange struct {
    Start    int64
    End      int64
    Database string
    Table    string
}

func (r *RangeRouter) RouteDatabase(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Database
        }
    }
    return r.Ranges[len(r.Ranges)-1].Database // 默认最后一个
}

func (r *RangeRouter) RouteTable(shardingKey string) string {
    value, _ := strconv.ParseInt(shardingKey, 10, 64)
    for _, rang := range r.Ranges {
        if value >= rang.Start && value < rang.End {
            return rang.Table
        }
    }
    return r.Ranges[len(r.Ranges)-1].Table
}

// TimeRouter 时间路由器
type TimeRouter struct {
    DatabasePrefix string
    TablePrefix    string
    TimeFormat     string // "200601" 按月, "20060102" 按天
}

func (r *TimeRouter) RouteDatabase(shardingKey string) string {
    // 时间路由通常不分库，只分表
    return r.DatabasePrefix
}

func (r *TimeRouter) RouteTable(shardingKey string) string {
    timestamp, _ := strconv.ParseInt(shardingKey, 10, 64)
    t := time.Unix(timestamp, 0)
    suffix := t.Format(r.TimeFormat)
    return fmt.Sprintf("%s_%s", r.TablePrefix, suffix)
}
```

### 2. 分片数据源

```go
package sharding

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// ShardingDataSource 分片数据源
type ShardingDataSource struct {
    dataSources map[string]*sql.DB
    router      ShardingRouter
    healthCheck *HealthChecker
    mutex       sync.RWMutex
}

func NewShardingDataSource(router ShardingRouter) *ShardingDataSource {
    return &ShardingDataSource{
        dataSources: make(map[string]*sql.DB),
        router:      router,
        healthCheck: NewHealthChecker(),
    }
}

// AddDataSource 添加数据源
func (ds *ShardingDataSource) AddDataSource(name string, db *sql.DB) {
    ds.mutex.Lock()
    defer ds.mutex.Unlock()
    
    ds.dataSources[name] = db
    ds.healthCheck.AddShard(name, db)
}

// GetDataSource 获取数据源
func (ds *ShardingDataSource) GetDataSource(shardingKey string) (*sql.DB, string, error) {
    dbName := ds.router.RouteDatabase(shardingKey)
    tableName := ds.router.RouteTable(shardingKey)
    
    ds.mutex.RLock()
    db, exists := ds.dataSources[dbName]
    ds.mutex.RUnlock()
    
    if !exists {
        return nil, "", fmt.Errorf("database %s not found", dbName)
    }
    
    if !ds.healthCheck.IsHealthy(dbName) {
        return nil, "", fmt.Errorf("database %s is unhealthy", dbName)
    }
    
    return db, tableName, nil
}

// Execute 执行SQL
func (ds *ShardingDataSource) Execute(ctx context.Context, shardingKey, query string, args ...interface{}) (sql.Result, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    // 替换表名占位符
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.ExecContext(ctx, finalQuery, args...)
}

// Query 查询数据
func (ds *ShardingDataSource) Query(ctx context.Context, shardingKey, query string, args ...interface{}) (*sql.Rows, error) {
    db, tableName, err := ds.GetDataSource(shardingKey)
    if err != nil {
        return nil, err
    }
    
    finalQuery := fmt.Sprintf(query, tableName)
    
    return db.QueryContext(ctx, finalQuery, args...)
}

// CrossShardQuery 跨分片查询
func (ds *ShardingDataSource) CrossShardQuery(ctx context.Context, query string, args ...interface{}) ([]map[string]interface{}, error) {
    shards := ds.router.GetAllShards()
    
    type shardResult struct {
        data []map[string]interface{}
        err  error
    }
    
    resultChan := make(chan shardResult, len(shards))
    
    // 并行查询所有分片
    for _, shard := range shards {
        go func(shardName string) {
            db := ds.dataSources[shardName]
            if db == nil || !ds.healthCheck.IsHealthy(shardName) {
                resultChan <- shardResult{nil, fmt.Errorf("shard %s unavailable", shardName)}
                return
            }
            
            rows, err := db.QueryContext(ctx, query, args...)
            if err != nil {
                resultChan <- shardResult{nil, err}
                return
            }
            defer rows.Close()
            
            data, err := scanRowsToMap(rows)
            resultChan <- shardResult{data, err}
        }(shard)
    }
    
    // 收集结果
    var allResults []map[string]interface{}
    for i := 0; i < len(shards); i++ {
        result := <-resultChan
        if result.err != nil {
            return nil, result.err
        }
        allResults = append(allResults, result.data...)
    }
    
    return allResults, nil
}

// 健康检查器
type HealthChecker struct {
    shards map[string]*ShardHealth
    mutex  sync.RWMutex
}

type ShardHealth struct {
    DB        *sql.DB
    Healthy   bool
    LastCheck time.Time
}

func NewHealthChecker() *HealthChecker {
    hc := &HealthChecker{
        shards: make(map[string]*ShardHealth),
    }
    
    // 启动健康检查
    go hc.startHealthCheck()
    
    return hc
}

func (hc *HealthChecker) AddShard(name string, db *sql.DB) {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    hc.shards[name] = &ShardHealth{
        DB:        db,
        Healthy:   true,
        LastCheck: time.Now(),
    }
}

func (hc *HealthChecker) IsHealthy(name string) bool {
    hc.mutex.RLock()
    defer hc.mutex.RUnlock()
    
    shard, exists := hc.shards[name]
    return exists && shard.Healthy
}

func (hc *HealthChecker) startHealthCheck() {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for range ticker.C {
        hc.checkAllShards()
    }
}

func (hc *HealthChecker) checkAllShards() {
    hc.mutex.Lock()
    defer hc.mutex.Unlock()
    
    for name, shard := range hc.shards {
        healthy := hc.pingShard(shard.DB)
        shard.Healthy = healthy
        shard.LastCheck = time.Now()
        
        if !healthy {
            fmt.Printf("Shard %s is unhealthy\n", name)
        }
    }
}

func (hc *HealthChecker) pingShard(db *sql.DB) bool {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    return db.PingContext(ctx) == nil
}

// 辅助函数：将查询结果转换为map
func scanRowsToMap(rows *sql.Rows) ([]map[string]interface{}, error) {
    columns, err := rows.Columns()
    if err != nil {
        return nil, err
    }
    
    var results []map[string]interface{}
    
    for rows.Next() {
        values := make([]interface{}, len(columns))
        valuePtrs := make([]interface{}, len(columns))
        
        for i := range values {
            valuePtrs[i] = &values[i]
        }
        
        if err := rows.Scan(valuePtrs...); err != nil {
            return nil, err
        }
        
        row := make(map[string]interface{})
        for i, col := range columns {
            row[col] = values[i]
        }
        
        results = append(results, row)
    }
    
    return results, rows.Err()
}
```

### 3. 分布式事务管理

```go
package transaction

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

// TCC事务管理器
type TCCManager struct {
    transactions map[string]*TCCTransaction
    mutex        sync.RWMutex
}

type TCCTransaction struct {
    ID         string
    Operations []TCCOperation
    Status     TransactionStatus
    CreatedAt  time.Time
    UpdatedAt  time.Time
}

type TransactionStatus int

const (
    StatusPending TransactionStatus = iota
    StatusTrying
    StatusConfirming
    StatusCanceling
    StatusConfirmed
    StatusCanceled
    StatusFailed
)

type TCCOperation interface {
    Try(ctx context.Context) error
    Confirm(ctx context.Context) error
    Cancel(ctx context.Context) error
    GetResourceID() string
}

func NewTCCManager() *TCCManager {
    return &TCCManager{
        transactions: make(map[string]*TCCTransaction),
    }
}

func (tm *TCCManager) BeginTransaction(id string) *TCCTransaction {
    tm.mutex.Lock()
    defer tm.mutex.Unlock()
    
    tx := &TCCTransaction{
        ID:        id,
        Status:    StatusPending,
        CreatedAt: time.Now(),
        UpdatedAt: time.Now(),
    }
    
    tm.transactions[id] = tx
    return tx
}

func (tx *TCCTransaction) AddOperation(op TCCOperation) {
    tx.Operations = append(tx.Operations, op)
}

func (tm *TCCManager) ExecuteTransaction(ctx context.Context, txID string) error {
    tm.mutex.Lock()
    tx, exists := tm.transactions[txID]
    if !exists {
        tm.mutex.Unlock()
        return fmt.Errorf("transaction %s not found", txID)
    }
    tx.Status = StatusTrying
    tx.UpdatedAt = time.Now()
    tm.mutex.Unlock()
    
    // Try阶段
    for i, op := range tx.Operations {
        if err := op.Try(ctx); err != nil {
            // Try失败，执行Cancel
            tm.cancelTransaction(ctx, tx, i)
            return err
        }
    }
    
    // Confirm阶段
    tx.Status = StatusConfirming
    for i, op := range tx.Operations {
        if err := op.Confirm(ctx); err != nil {
            // Confirm失败，需要人工干预
            tx.Status = StatusFailed
            return fmt.Errorf("confirm failed at operation %d: %v", i, err)
        }
    }
    
    tx.Status = StatusConfirmed
    tx.UpdatedAt = time.Now()
    
    return nil
}

func (tm *TCCManager) cancelTransaction(ctx context.Context, tx *TCCTransaction, failedIndex int) {
    tx.Status = StatusCanceling
    
    // 逆序取消已执行的操作
    for i := failedIndex - 1; i >= 0; i-- {
        if err := tx.Operations[i].Cancel(ctx); err != nil {
            // 记录取消失败的操作，需要人工干预
            fmt.Printf("Cancel operation %d failed: %v\n", i, err)
        }
    }
    
    tx.Status = StatusCanceled
    tx.UpdatedAt = time.Now()
}

// 账户转账TCC操作示例
type TransferOperation struct {
    FromAccount string
    ToAccount   string
    Amount      float64
    DS          *ShardingDataSource
    TxID        string
}

func (op *TransferOperation) GetResourceID() string {
    return fmt.Sprintf("%s-%s", op.FromAccount, op.ToAccount)
}

func (op *TransferOperation) Try(ctx context.Context) error {
    // 冻结转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount + ?, version = version + 1 WHERE account_id = ? AND balance >= ?"
    
    result, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount, op.Amount)
    if err != nil {
        return err
    }
    
    affected, _ := result.RowsAffected()
    if affected == 0 {
        return fmt.Errorf("insufficient balance or account not found")
    }
    
    // 记录TCC事务日志
    return op.recordTCCLog(ctx, "TRY", "SUCCESS")
}

func (op *TransferOperation) Confirm(ctx context.Context) error {
    // 扣减转出账户，增加转入账户
    
    // 1. 扣减转出账户
    deductQuery := "UPDATE accounts SET balance = balance - ?, frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    _, err := op.DS.Execute(ctx, op.FromAccount, deductQuery, op.Amount, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    // 2. 增加转入账户
    addQuery := "UPDATE accounts SET balance = balance + ?, version = version + 1 WHERE account_id = ?"
    _, err = op.DS.Execute(ctx, op.ToAccount, addQuery, op.Amount, op.ToAccount)
    if err != nil {
        return err
    }
    
    // 3. 记录转账流水
    err = op.recordTransferLog(ctx)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CONFIRM", "SUCCESS")
}

func (op *TransferOperation) Cancel(ctx context.Context) error {
    // 解冻转出账户金额
    query := "UPDATE accounts SET frozen_amount = frozen_amount - ?, version = version + 1 WHERE account_id = ?"
    
    _, err := op.DS.Execute(ctx, op.FromAccount, query, op.Amount, op.FromAccount)
    if err != nil {
        return err
    }
    
    return op.recordTCCLog(ctx, "CANCEL", "SUCCESS")
}

func (op *TransferOperation) recordTCCLog(ctx context.Context, action, status string) error {
    query := "INSERT INTO tcc_logs (tx_id, resource_id, action, status, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.GetResourceID(), action, status, time.Now())
    return err
}

func (op *TransferOperation) recordTransferLog(ctx context.Context) error {
    query := "INSERT INTO transfer_logs (tx_id, from_account, to_account, amount, created_at) VALUES (?, ?, ?, ?, ?)"
    
    _, err := op.DS.Execute(ctx, op.TxID, query, op.TxID, op.FromAccount, op.ToAccount, op.Amount, time.Now())
    return err
}
```

## 总结

分库分表是解决大规模数据存储和高并发访问的重要技术手段，但也带来了系统复杂性的显著增加。在实施分库分表方案时，需要综合考虑业务特点、技术架构、运维成本等多个因素。

**关键要点：**

1. **合理的分片策略**：选择合适的分片键和算法
2. **数据一致性**：在性能和一致性之间找到平衡
3. **查询优化**：避免跨分片查询，合理使用缓存
4. **运维监控**：完善的监控和故障处理机制
5. **扩容规划**：提前规划扩容方案，避免后期重构

**最佳实践：**

- 优先考虑垂直分库，再考虑水平分库
- 分片数量选择2的幂次，便于扩容
- 避免分布式事务，采用最终一致性
- 建立完善的监控和告警机制
- 制定详细的数据迁移和扩容方案

通过合理的架构设计和技术选型，分库分表能够有效解决大规模系统的数据存储和访问问题，为业务的快速发展提供强有力的技术支撑。