### Redis部署架构与生产环境最佳实践

#### 1. 主从复制（Master-Slave）深度解析

##### 1.1 复制机制核心原理
**背景**：单机Redis存在单点故障风险，数据丢失和服务不可用问题。
**解决问题**：数据备份、读写分离、故障转移基础。

**复制流程详细分析**：
```go
// Redis复制状态机模拟
type ReplicationState int

const (
    REPL_STATE_NONE ReplicationState = iota
    REPL_STATE_CONNECT
    REPL_STATE_CONNECTING
    REPL_STATE_RECEIVE_PONG
    REPL_STATE_SEND_AUTH
    REPL_STATE_RECEIVE_AUTH
    REPL_STATE_SEND_PORT
    REPL_STATE_RECEIVE_PORT
    REPL_STATE_SEND_IP
    REPL_STATE_RECEIVE_IP
    REPL_STATE_SEND_CAPA
    REPL_STATE_RECEIVE_CAPA
    REPL_STATE_SEND_PSYNC
    REPL_STATE_RECEIVE_PSYNC
    REPL_STATE_TRANSFER
    REPL_STATE_CONNECTED
)

// 主从复制管理器
type MasterSlaveManager struct {
    master     *RedisNode
    slaves     []*RedisNode
    replBuffer *ReplicationBuffer
    replOffset int64
}

func (msm *MasterSlaveManager) HandleSlaveConnection(slave *RedisNode) error {
    // 1. 握手阶段
    if err := msm.performHandshake(slave); err != nil {
        return err
    }
    
    // 2. 判断复制类型
    if slave.ReplOffset == 0 || !msm.canPartialResync(slave) {
        return msm.performFullResync(slave)
    } else {
        return msm.performPartialResync(slave)
    }
}

// 全量复制实现
func (msm *MasterSlaveManager) performFullResync(slave *RedisNode) error {
    // 1. 生成RDB快照
    rdbFile, err := msm.master.CreateRDBSnapshot()
    if err != nil {
        return err
    }
    
    // 2. 发送RDB文件
    if err := msm.sendRDBToSlave(slave, rdbFile); err != nil {
        return err
    }
    
    // 3. 发送复制缓冲区中的增量数据
    return msm.sendReplicationBuffer(slave)
}

// 增量复制实现
func (msm *MasterSlaveManager) performPartialResync(slave *RedisNode) error {
    // 计算需要发送的数据范围
    startOffset := slave.ReplOffset
    endOffset := msm.replOffset
    
    // 从复制缓冲区获取增量数据
    commands, err := msm.replBuffer.GetCommands(startOffset, endOffset)
    if err != nil {
        // 缓冲区数据不足，降级为全量复制
        return msm.performFullResync(slave)
    }
    
    // 发送增量命令
    return msm.sendCommandsToSlave(slave, commands)
}
```

##### 1.2 生产环境配置优化
**主节点配置**：
```bash
# redis-master.conf
# 复制相关配置
repl-diskless-sync yes                    # 无盘复制，直接通过网络发送RDB
repl-diskless-sync-delay 5                # 无盘复制延迟，等待更多从节点连接
repl-backlog-size 64mb                    # 复制缓冲区大小
repl-backlog-ttl 3600                     # 复制缓冲区TTL
min-slaves-to-write 1                     # 最少从节点数量
min-slaves-max-lag 10                     # 从节点最大延迟

# 性能优化
tcp-keepalive 60                          # TCP保活
timeout 300                               # 客户端超时
maxclients 10000                          # 最大客户端连接数

# 内存管理
maxmemory 8gb                             # 最大内存
maxmemory-policy allkeys-lru              # 内存淘汰策略
```

**从节点配置**：
```bash
# redis-slave.conf
slaveof 192.168.1.100 6379               # 主节点地址
slave-read-only yes                       # 只读模式
slave-serve-stale-data yes                # 连接断开时继续服务
slave-priority 100                        # 从节点优先级

# 复制优化
repl-disable-tcp-nodelay no               # 启用TCP_NODELAY
slave-lazy-flush yes                      # 异步删除大key
```

#### 2. 哨兵模式（Sentinel）企业级部署

##### 2.1 哨兵集群架构设计
```go
// 哨兵节点结构
type SentinelNode struct {
    ID              string
    IP              string
    Port            int
    RunID           string
    Epoch           int64
    MasterInstances map[string]*MasterInstance
    KnownSentinels  map[string]*SentinelNode
}

// 主实例监控信息
type MasterInstance struct {
    Name            string
    IP              string
    Port            int
    Quorum          int
    DownAfterPeriod time.Duration
    FailoverTimeout time.Duration
    ParallelSyncs   int
    Slaves          []*SlaveInstance
    SentinelNodes   []*SentinelNode
    LastPingTime    time.Time
    LastPongTime    time.Time
    Status          MasterStatus
}

type MasterStatus int

const (
    MASTER_STATUS_OK MasterStatus = iota
    MASTER_STATUS_SDOWN  // 主观下线
    MASTER_STATUS_ODOWN  // 客观下线
    MASTER_STATUS_FAILOVER_IN_PROGRESS
)

// 故障转移管理器
type FailoverManager struct {
    sentinel       *SentinelNode
    masterInstance *MasterInstance
    state          FailoverState
    startTime      time.Time
    stateChangeTime time.Time
}

type FailoverState int

const (
    FAILOVER_STATE_NONE FailoverState = iota
    FAILOVER_STATE_WAIT_START
    FAILOVER_STATE_SELECT_SLAVE
    FAILOVER_STATE_SEND_SLAVEOF_NOONE
    FAILOVER_STATE_WAIT_PROMOTION
    FAILOVER_STATE_RECONF_SLAVES
    FAILOVER_STATE_UPDATE_CONFIG
)

// 故障转移执行流程
func (fm *FailoverManager) ExecuteFailover() error {
    switch fm.state {
    case FAILOVER_STATE_WAIT_START:
        return fm.waitForFailoverStart()
    case FAILOVER_STATE_SELECT_SLAVE:
        return fm.selectBestSlave()
    case FAILOVER_STATE_SEND_SLAVEOF_NOONE:
        return fm.promoteSelectedSlave()
    case FAILOVER_STATE_WAIT_PROMOTION:
        return fm.waitForPromotion()
    case FAILOVER_STATE_RECONF_SLAVES:
        return fm.reconfigureSlaves()
    case FAILOVER_STATE_UPDATE_CONFIG:
        return fm.updateConfiguration()
    }
    return nil
}

// 选择最佳从节点算法
func (fm *FailoverManager) selectBestSlave() error {
    var bestSlave *SlaveInstance
    bestScore := -1
    
    for _, slave := range fm.masterInstance.Slaves {
        score := fm.calculateSlaveScore(slave)
        if score > bestScore {
            bestScore = score
            bestSlave = slave
        }
    }
    
    if bestSlave == nil {
        return errors.New("no suitable slave found")
    }
    
    fm.selectedSlave = bestSlave
    fm.state = FAILOVER_STATE_SEND_SLAVEOF_NOONE
    return nil
}

// 从节点评分算法
func (fm *FailoverManager) calculateSlaveScore(slave *SlaveInstance) int {
    score := 0
    
    // 1. 优先级权重（越小越好）
    score += (256 - slave.Priority) * 1000
    
    // 2. 复制偏移量权重（越大越好）
    score += int(slave.ReplOffset / 1000)
    
    // 3. 运行时间权重
    if slave.InfoRefresh.Sub(slave.LastAvailableTime) < time.Minute*5 {
        score += 100
    }
    
    // 4. 连接状态权重
    if slave.LastPingTime.Sub(time.Now()) < time.Second*5 {
        score += 50
    }
    
    return score
}
```

##### 2.2 哨兵配置最佳实践
```bash
# sentinel.conf
port 26379
dir /var/lib/redis-sentinel

# 监控主节点配置
sentinel monitor mymaster 192.168.1.100 6379 2
sentinel auth-pass mymaster your_password

# 故障检测配置
sentinel down-after-milliseconds mymaster 5000    # 5秒无响应认为主观下线
sentinel parallel-syncs mymaster 1                # 故障转移时同时重新配置的从节点数
sentinel failover-timeout mymaster 60000          # 故障转移超时时间

# 通知脚本
sentinel notification-script mymaster /opt/redis/notify.sh
sentinel client-reconfig-script mymaster /opt/redis/reconfig.sh

# 性能优化
sentinel deny-scripts-reconfig yes                # 禁止运行时修改脚本
sentinel resolve-hostnames yes                    # 解析主机名
sentinel announce-hostnames yes                   # 通告主机名
```

#### 3. Redis集群（Cluster）生产级部署

##### 3.1 集群拓扑设计
```go
// 集群节点管理
type ClusterManager struct {
    nodes          map[string]*ClusterNode
    slots          [16384]*ClusterNode  // 槽位映射
    myself         *ClusterNode
    currentEpoch   uint64
    lastVoteEpoch  uint64
    state          ClusterState
}

type ClusterNode struct {
    Name           string
    IP             string
    Port           int
    Flags          NodeFlags
    MasterID       string
    PingSent       time.Time
    PongReceived   time.Time
    ConfigEpoch    uint64
    Slots          []bool  // 16384位槽位标记
    NumSlots       int
    Slaves         []*ClusterNode
    FailReports    []*NodeFailReport
}

type NodeFlags int

const (
    NODE_MASTER NodeFlags = 1 << iota
    NODE_SLAVE
    NODE_PFAIL   // 疑似故障
    NODE_FAIL    // 确认故障
    NODE_MYSELF
    NODE_HANDSHAKE
    NODE_NOADDR
)

// 槽位迁移管理器
type SlotMigrationManager struct {
    cluster     *ClusterManager
    sourceNode  *ClusterNode
    targetNode  *ClusterNode
    slot        int
    state       MigrationState
    migratingKeys map[string]bool
}

type MigrationState int

const (
    MIGRATION_STATE_NONE MigrationState = iota
    MIGRATION_STATE_PREPARE
    MIGRATION_STATE_MIGRATING
    MIGRATION_STATE_IMPORTING
    MIGRATION_STATE_COMPLETE
)

// 在线槽位迁移实现
func (smm *SlotMigrationManager) MigrateSlot() error {
    // 1. 准备阶段：设置源节点为MIGRATING状态
    if err := smm.setSlotMigrating(); err != nil {
        return err
    }
    
    // 2. 设置目标节点为IMPORTING状态
    if err := smm.setSlotImporting(); err != nil {
        return err
    }
    
    // 3. 迁移槽位中的所有key
    if err := smm.migrateAllKeys(); err != nil {
        return err
    }
    
    // 4. 更新槽位归属
    if err := smm.updateSlotOwnership(); err != nil {
        return err
    }
    
    // 5. 通知集群节点更新配置
    return smm.broadcastSlotUpdate()
}

func (smm *SlotMigrationManager) migrateAllKeys() error {
    // 获取槽位中的所有key
    keys, err := smm.sourceNode.GetKeysInSlot(smm.slot, 100)
    if err != nil {
        return err
    }
    
    for len(keys) > 0 {
        // 批量迁移key
        if err := smm.migrateKeysBatch(keys); err != nil {
            return err
        }
        
        // 获取下一批key
        keys, err = smm.sourceNode.GetKeysInSlot(smm.slot, 100)
        if err != nil {
            return err
        }
    }
    
    return nil
}

// 智能客户端实现
type ClusterClient struct {
    nodes       map[string]*redis.Client
    slotCache   [16384]string  // 槽位到节点的映射缓存
    redirections int
    maxRedirections int
}

func (cc *ClusterClient) Get(key string) (string, error) {
    slot := crc16(key) % 16384
    nodeAddr := cc.slotCache[slot]
    
    if nodeAddr == "" {
        // 缓存未命中，随机选择一个节点
        nodeAddr = cc.getRandomNode()
    }
    
    client := cc.nodes[nodeAddr]
    result, err := client.Get(context.Background(), key).Result()
    
    if err != nil {
        if isMovedError(err) {
            // 处理MOVED重定向
            newNodeAddr := parseMovedError(err)
            cc.slotCache[slot] = newNodeAddr
            return cc.Get(key)  // 重试
        }
        
        if isAskError(err) {
            // 处理ASK重定向（槽位迁移中）
            tempNodeAddr := parseAskError(err)
            tempClient := cc.nodes[tempNodeAddr]
            tempClient.Do(context.Background(), "ASKING")
            return tempClient.Get(context.Background(), key).Result()
        }
    }
    
    return result, err
}
```

##### 3.2 集群部署配置
**集群节点配置**：
```bash
# redis-cluster.conf
port 7000
cluster-enabled yes
cluster-config-file nodes-7000.conf
cluster-node-timeout 15000
cluster-announce-ip 192.168.1.100
cluster-announce-port 7000
cluster-announce-bus-port 17000

# 故障转移配置
cluster-slave-validity-factor 10
cluster-migration-barrier 1
cluster-require-full-coverage no

# 性能优化
cluster-allow-reads-when-down no
cluster-slave-no-failover no
```

**集群初始化脚本**：
```bash
#!/bin/bash
# cluster-init.sh

# 启动所有节点
for port in {7000..7005}; do
    redis-server /etc/redis/redis-${port}.conf --daemonize yes
done

# 等待节点启动
sleep 5

# 创建集群
redis-cli --cluster create \
    192.168.1.100:7000 192.168.1.100:7001 192.168.1.100:7002 \
    192.168.1.101:7000 192.168.1.101:7001 192.168.1.101:7002 \
    --cluster-replicas 1 --cluster-yes

# 验证集群状态
redis-cli --cluster check 192.168.1.100:7000
```

#### 4. 混合部署架构

##### 4.1 多层级缓存架构
```go
// 混合部署管理器
type HybridDeploymentManager struct {
    localCache    *LocalCache      // L1: 本地缓存
    sentinelCache *SentinelCache   // L2: 哨兵模式Redis
    clusterCache  *ClusterCache    // L3: 集群模式Redis
    database      *Database        // L4: 数据库
}

func (hdm *HybridDeploymentManager) Get(key string) (interface{}, error) {
    // L1: 本地缓存
    if value, found := hdm.localCache.Get(key); found {
        return value, nil
    }
    
    // L2: 哨兵Redis（热点数据）
    if hdm.isHotKey(key) {
        if value, err := hdm.sentinelCache.Get(key); err == nil {
            hdm.localCache.Set(key, value, time.Minute*5)
            return value, nil
        }
    }
    
    // L3: 集群Redis（海量数据）
    if value, err := hdm.clusterCache.Get(key); err == nil {
        if hdm.isHotKey(key) {
            hdm.sentinelCache.Set(key, value, time.Hour)
        }
        hdm.localCache.Set(key, value, time.Minute*5)
        return value, nil
    }
    
    // L4: 数据库
    value, err := hdm.database.Query(key)
    if err != nil {
        return nil, err
    }
    
    // 异步更新缓存
    go hdm.updateCaches(key, value)
    
    return value, nil
}
```

##### 4.2 生产环境监控
```go
// 部署监控系统
type DeploymentMonitor struct {
    clusters   map[string]*ClusterMonitor
    sentinels  map[string]*SentinelMonitor
    alerter    *AlertManager
    metrics    *MetricsCollector
}

func (dm *DeploymentMonitor) StartMonitoring() {
    ticker := time.NewTicker(time.Second * 30)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            dm.collectMetrics()
            dm.checkHealth()
            dm.detectAnomalies()
        }
    }
}

func (dm *DeploymentMonitor) collectMetrics() {
    for name, cluster := range dm.clusters {
        metrics := cluster.GetMetrics()
        dm.metrics.Record(name, metrics)
        
        // 检查关键指标
        if metrics.MemoryUsage > 0.8 {
            dm.alerter.SendAlert("High memory usage", name)
        }
        
        if metrics.ConnectionCount > 8000 {
            dm.alerter.SendAlert("High connection count", name)
        }
    }
}
```