### Redis集群深度解析

#### 1. Redis Cluster工作原理

##### 1.1 核心架构设计
**背景**：单节点Redis受限于内存容量（通常<100GB）和单线程处理能力，无法满足大规模应用需求。

**解决方案**：Redis Cluster采用无中心化的分布式架构，通过数据分片实现水平扩展。

**核心特性**：
- **去中心化**：无需Proxy或配置服务器，节点间直接通信
- **自动分片**：数据按哈希槽（Hash Slot）自动分布到不同节点
- **高可用**：支持主从复制和自动故障转移
- **线性扩展**：可在线添加/删除节点，动态调整集群规模

##### 1.2 哈希槽（Hash Slot）机制
**核心原理**：
- Redis Cluster将整个数据空间划分为**16384个哈希槽**（0-16383）
- 每个主节点负责一部分槽位（如3主节点各负责约5461个槽）
- 客户端通过CRC16算法计算key的槽位：`slot = CRC16(key) % 16384`

**Go实现示例**：
```go
package main

import (
    "fmt"
    "hash/crc32"
)

// 计算key对应的哈希槽
func calculateSlot(key string) int {
    // Redis使用CRC16，这里用CRC32模拟
    hash := crc32.ChecksumIEEE([]byte(key))
    return int(hash % 16384)
}

// 集群节点信息
type ClusterNode struct {
    ID       string
    Address  string
    Slots    []SlotRange
    IsMaster bool
}

type SlotRange struct {
    Start int
    End   int
}

// 集群路由表
type ClusterRouter struct {
    nodes    map[string]*ClusterNode
    slotMap  [16384]*ClusterNode  // 槽位到节点的映射
}

func (r *ClusterRouter) GetNodeForKey(key string) *ClusterNode {
    slot := calculateSlot(key)
    return r.slotMap[slot]
}

func (r *ClusterRouter) AddNode(node *ClusterNode) {
    r.nodes[node.ID] = node
    // 更新槽位映射
    for _, slotRange := range node.Slots {
        for slot := slotRange.Start; slot <= slotRange.End; slot++ {
            r.slotMap[slot] = node
        }
    }
}
```

##### 1.3 集群通信协议（Gossip Protocol）
**目的**：节点间交换集群状态信息，实现最终一致性。

**工作机制**：
- 每个节点定期（每秒）向随机几个节点发送PING消息
- PING消息包含发送者的状态信息和部分其他节点信息
- 接收者回复PONG消息，确认收到并返回自己的状态
- 通过多轮传播，集群状态最终收敛一致

**Go实现示例**：
```go
type GossipMessage struct {
    SenderID    string
    MessageType string  // PING, PONG, FAIL
    ClusterInfo map[string]*NodeInfo
    Timestamp   int64
}

type NodeInfo struct {
    ID          string
    Address     string
    Status      string  // ONLINE, OFFLINE, SUSPECTED
    Slots       []SlotRange
    LastSeen    int64
}

type GossipProtocol struct {
    localNode   *NodeInfo
    clusterView map[string]*NodeInfo
    mutex       sync.RWMutex
}

func (g *GossipProtocol) SendPing() {
    // 随机选择几个节点发送PING
    targets := g.selectRandomNodes(3)
    
    for _, target := range targets {
        msg := &GossipMessage{
            SenderID:    g.localNode.ID,
            MessageType: "PING",
            ClusterInfo: g.getPartialClusterView(),
            Timestamp:   time.Now().Unix(),
        }
        
        go g.sendMessage(target, msg)
    }
}

func (g *GossipProtocol) HandleMessage(msg *GossipMessage) {
    g.mutex.Lock()
    defer g.mutex.Unlock()
    
    // 更新集群视图
    for nodeID, nodeInfo := range msg.ClusterInfo {
        if existing, exists := g.clusterView[nodeID]; exists {
            if nodeInfo.LastSeen > existing.LastSeen {
                g.clusterView[nodeID] = nodeInfo
            }
        } else {
            g.clusterView[nodeID] = nodeInfo
        }
    }
    
    // 如果是PING消息，回复PONG
    if msg.MessageType == "PING" {
        pong := &GossipMessage{
            SenderID:    g.localNode.ID,
            MessageType: "PONG",
            ClusterInfo: g.getPartialClusterView(),
            Timestamp:   time.Now().Unix(),
        }
        g.sendMessage(msg.SenderID, pong)
    }
}
```

#### 2. 分布式寻址算法对比

##### 2.1 哈希取模算法
**原理**：`node = hash(key) % N`（N为节点数）

**优点**：实现简单，分布均匀
**缺点**：扩容时需要重新分布大量数据（约N-1/N的数据需要迁移）

**Go实现**：
```go
type HashModRouter struct {
    nodes []string
}

func (r *HashModRouter) GetNode(key string) string {
    hash := crc32.ChecksumIEEE([]byte(key))
    return r.nodes[hash%uint32(len(r.nodes))]
}

// 扩容时的数据迁移成本
func (r *HashModRouter) CalculateMigrationCost(newNodeCount int) float64 {
    oldCount := len(r.nodes)
    if newNodeCount <= oldCount {
        return 0
    }
    // 迁移比例 = (新节点数 - 旧节点数) / 新节点数
    return float64(newNodeCount-oldCount) / float64(newNodeCount)
}
```

##### 2.2 一致性哈希算法
**原理**：将哈希空间组织成环形，节点和数据都映射到环上，数据存储在顺时针方向第一个节点。

**优点**：扩容时只需迁移少量数据（平均1/N）
**缺点**：可能出现数据倾斜，需要虚拟节点解决

**Go实现**：
```go
type ConsistentHashRouter struct {
    ring        map[uint32]string  // 哈希环
    sortedHashes []uint32           // 排序的哈希值
    virtualNodes int                // 虚拟节点数
    mutex        sync.RWMutex
}

func NewConsistentHashRouter(virtualNodes int) *ConsistentHashRouter {
    return &ConsistentHashRouter{
        ring:         make(map[uint32]string),
        virtualNodes: virtualNodes,
    }
}

func (r *ConsistentHashRouter) AddNode(node string) {
    r.mutex.Lock()
    defer r.mutex.Unlock()
    
    // 为每个物理节点创建多个虚拟节点
    for i := 0; i < r.virtualNodes; i++ {
        virtualKey := fmt.Sprintf("%s#%d", node, i)
        hash := crc32.ChecksumIEEE([]byte(virtualKey))
        r.ring[hash] = node
        r.sortedHashes = append(r.sortedHashes, hash)
    }
    
    sort.Slice(r.sortedHashes, func(i, j int) bool {
        return r.sortedHashes[i] < r.sortedHashes[j]
    })
}

func (r *ConsistentHashRouter) GetNode(key string) string {
    r.mutex.RLock()
    defer r.mutex.RUnlock()
    
    if len(r.ring) == 0 {
        return ""
    }
    
    hash := crc32.ChecksumIEEE([]byte(key))
    
    // 二分查找第一个大于等于hash的节点
    idx := sort.Search(len(r.sortedHashes), func(i int) bool {
        return r.sortedHashes[i] >= hash
    })
    
    // 如果没找到，说明应该分配给第一个节点（环形）
    if idx == len(r.sortedHashes) {
        idx = 0
    }
    
    return r.ring[r.sortedHashes[idx]]
}

func (r *ConsistentHashRouter) RemoveNode(node string) {
    r.mutex.Lock()
    defer r.mutex.Unlock()
    
    // 移除所有虚拟节点
    for i := 0; i < r.virtualNodes; i++ {
        virtualKey := fmt.Sprintf("%s#%d", node, i)
        hash := crc32.ChecksumIEEE([]byte(virtualKey))
        delete(r.ring, hash)
        
        // 从排序数组中移除
        for j, h := range r.sortedHashes {
            if h == hash {
                r.sortedHashes = append(r.sortedHashes[:j], r.sortedHashes[j+1:]...)
                break
            }
        }
    }
}
```

##### 2.3 Redis Cluster的哈希槽算法
**优势**：
- **预分片**：固定16384个槽位，避免动态哈希计算
- **迁移粒度小**：以槽为单位迁移，可精确控制
- **负载均衡**：可手动调整槽位分布，优化负载

**Go实现示例**：
```go
type RedisClusterRouter struct {
    slotToNode map[int]string     // 槽位到节点映射
    nodeSlots  map[string][]int   // 节点到槽位映射
    mutex      sync.RWMutex
}

func (r *RedisClusterRouter) MigrateSlot(slot int, fromNode, toNode string) error {
    r.mutex.Lock()
    defer r.mutex.Unlock()
    
    // 验证槽位当前归属
    if r.slotToNode[slot] != fromNode {
        return fmt.Errorf("slot %d is not owned by node %s", slot, fromNode)
    }
    
    // 更新映射关系
    r.slotToNode[slot] = toNode
    
    // 更新节点槽位列表
    r.removeSlotFromNode(fromNode, slot)
    r.addSlotToNode(toNode, slot)
    
    return nil
}

func (r *RedisClusterRouter) GetSlotDistribution() map[string]int {
    r.mutex.RLock()
    defer r.mutex.RUnlock()
    
    distribution := make(map[string]int)
    for _, node := range r.slotToNode {
        distribution[node]++
    }
    return distribution
}

// 自动负载均衡
func (r *RedisClusterRouter) Rebalance() {
    distribution := r.GetSlotDistribution()
    nodeCount := len(distribution)
    if nodeCount == 0 {
        return
    }
    
    avgSlots := 16384 / nodeCount
    
    // 找出负载过高和过低的节点
    var overloaded, underloaded []string
    for node, slots := range distribution {
        if slots > avgSlots+100 {  // 容忍100个槽位的差异
            overloaded = append(overloaded, node)
        } else if slots < avgSlots-100 {
            underloaded = append(underloaded, node)
        }
    }
    
    // 执行槽位迁移
    for len(overloaded) > 0 && len(underloaded) > 0 {
        fromNode := overloaded[0]
        toNode := underloaded[0]
        
        // 迁移一个槽位
        slots := r.nodeSlots[fromNode]
        if len(slots) > 0 {
            slot := slots[0]
            r.MigrateSlot(slot, fromNode, toNode)
            
            // 更新负载状态
            distribution[fromNode]--
            distribution[toNode]++
            
            if distribution[fromNode] <= avgSlots+100 {
                overloaded = overloaded[1:]
            }
            if distribution[toNode] >= avgSlots-100 {
                underloaded = underloaded[1:]
            }
        }
    }
}
```

#### 3. 集群故障处理机制

##### 3.1 故障检测
**检测方式**：
- **主观下线（PFAIL）**：单个节点认为某节点不可达
- **客观下线（FAIL）**：超过半数节点确认某节点不可达

**Go实现示例**：
```go
type FailureDetector struct {
    localNodeID     string
    clusterNodes    map[string]*ClusterNode
    suspectedNodes  map[string]time.Time  // 主观下线节点
    failedNodes     map[string]time.Time  // 客观下线节点
    failureReports  map[string]map[string]time.Time  // 故障报告
    mutex           sync.RWMutex
}

func (fd *FailureDetector) ReportFailure(reporterID, targetID string) {
    fd.mutex.Lock()
    defer fd.mutex.Unlock()
    
    if fd.failureReports[targetID] == nil {
        fd.failureReports[targetID] = make(map[string]time.Time)
    }
    
    fd.failureReports[targetID][reporterID] = time.Now()
    
    // 检查是否达到客观下线条件
    if len(fd.failureReports[targetID]) > len(fd.clusterNodes)/2 {
        fd.failedNodes[targetID] = time.Now()
        delete(fd.suspectedNodes, targetID)
        
        // 触发故障转移
        go fd.triggerFailover(targetID)
    }
}

func (fd *FailureDetector) triggerFailover(failedNodeID string) {
    failedNode := fd.clusterNodes[failedNodeID]
    if failedNode == nil || !failedNode.IsMaster {
        return
    }
    
    // 选择最佳从节点进行故障转移
    bestSlave := fd.selectBestSlave(failedNodeID)
    if bestSlave != nil {
        fd.promoteSlaveToMaster(bestSlave, failedNode.Slots)
    }
}

func (fd *FailureDetector) selectBestSlave(masterID string) *ClusterNode {
    var bestSlave *ClusterNode
    var maxReplicationOffset int64
    
    for _, node := range fd.clusterNodes {
        if !node.IsMaster && node.MasterID == masterID {
            if node.ReplicationOffset > maxReplicationOffset {
                maxReplicationOffset = node.ReplicationOffset
                bestSlave = node
            }
        }
    }
    
    return bestSlave
}
```

##### 3.2 槽位迁移机制
**迁移流程**：
1. **准备阶段**：标记槽位为迁移状态
2. **数据迁移**：逐个迁移槽位中的key
3. **完成阶段**：更新集群配置，完成迁移

**Go实现示例**：
```go
type SlotMigrator struct {
    sourceNode *redis.Client
    targetNode *redis.Client
    slot       int
    batchSize  int
}

func (sm *SlotMigrator) MigrateSlot() error {
    // 1. 设置槽位为迁移状态
    err := sm.sourceNode.ClusterSetSlot(context.Background(), sm.slot, "migrating", sm.targetNode.Options().Addr).Err()
    if err != nil {
        return fmt.Errorf("failed to set slot migrating: %v", err)
    }
    
    err = sm.targetNode.ClusterSetSlot(context.Background(), sm.slot, "importing", sm.sourceNode.Options().Addr).Err()
    if err != nil {
        return fmt.Errorf("failed to set slot importing: %v", err)
    }
    
    // 2. 迁移数据
    for {
        keys, err := sm.sourceNode.ClusterGetKeysInSlot(context.Background(), sm.slot, sm.batchSize).Result()
        if err != nil {
            return fmt.Errorf("failed to get keys in slot: %v", err)
        }
        
        if len(keys) == 0 {
            break  // 所有key已迁移完成
        }
        
        // 批量迁移key
        err = sm.migrateKeys(keys)
        if err != nil {
            return fmt.Errorf("failed to migrate keys: %v", err)
        }
    }
    
    // 3. 完成迁移
    err = sm.sourceNode.ClusterSetSlot(context.Background(), sm.slot, "node", sm.targetNode.Options().Addr).Err()
    if err != nil {
        return fmt.Errorf("failed to complete migration on source: %v", err)
    }
    
    err = sm.targetNode.ClusterSetSlot(context.Background(), sm.slot, "node", sm.targetNode.Options().Addr).Err()
    if err != nil {
        return fmt.Errorf("failed to complete migration on target: %v", err)
    }
    
    return nil
}

func (sm *SlotMigrator) migrateKeys(keys []string) error {
    pipe := sm.sourceNode.Pipeline()
    
    // 使用MIGRATE命令批量迁移
    for _, key := range keys {
        pipe.Migrate(context.Background(), sm.targetNode.Options().Addr, key, 0, 5*time.Second)
    }
    
    _, err := pipe.Exec(context.Background())
    return err
}
```

#### 4. 集群性能优化

##### 4.1 智能客户端实现
**核心功能**：
- **槽位缓存**：本地缓存槽位到节点的映射关系
- **重定向处理**：自动处理MOVED和ASK重定向
- **连接池管理**：为每个节点维护独立连接池

**Go实现示例**：
```go
type SmartClusterClient struct {
    nodeClients map[string]*redis.Client  // 节点连接池
    slotCache   [16384]string             // 槽位缓存
    mutex       sync.RWMutex
}

func (c *SmartClusterClient) Get(key string) (string, error) {
    slot := calculateSlot(key)
    
    for attempts := 0; attempts < 3; attempts++ {
        nodeAddr := c.getNodeForSlot(slot)
        if nodeAddr == "" {
            return "", fmt.Errorf("no node found for slot %d", slot)
        }
        
        client := c.getNodeClient(nodeAddr)
        result, err := client.Get(context.Background(), key).Result()
        
        if err == nil {
            return result, nil
        }
        
        // 处理重定向
        if strings.Contains(err.Error(), "MOVED") {
            newAddr := c.parseRedirection(err.Error())
            c.updateSlotCache(slot, newAddr)
            continue
        }
        
        if strings.Contains(err.Error(), "ASK") {
            askAddr := c.parseRedirection(err.Error())
            askClient := c.getNodeClient(askAddr)
            askClient.Asking(context.Background())
            return askClient.Get(context.Background(), key).Result()
        }
        
        return "", err
    }
    
    return "", fmt.Errorf("max retries exceeded")
}

func (c *SmartClusterClient) updateSlotCache(slot int, nodeAddr string) {
    c.mutex.Lock()
    defer c.mutex.Unlock()
    c.slotCache[slot] = nodeAddr
}

func (c *SmartClusterClient) parseRedirection(errMsg string) string {
    // 解析 "MOVED 3999 127.0.0.1:7002" 格式的错误消息
    parts := strings.Split(errMsg, " ")
    if len(parts) >= 3 {
        return parts[2]
    }
    return ""
}
```

##### 4.2 批量操作优化
**问题**：跨槽位的批量操作需要分发到不同节点，影响性能。

**解决方案**：按槽位分组，并行执行。

**Go实现示例**：
```go
func (c *SmartClusterClient) MGet(keys []string) (map[string]string, error) {
    // 按槽位分组
    slotGroups := make(map[int][]string)
    for _, key := range keys {
        slot := calculateSlot(key)
        slotGroups[slot] = append(slotGroups[slot], key)
    }
    
    // 并行执行
    results := make(map[string]string)
    var wg sync.WaitGroup
    var mutex sync.Mutex
    errChan := make(chan error, len(slotGroups))
    
    for slot, slotKeys := range slotGroups {
        wg.Add(1)
        go func(s int, keys []string) {
            defer wg.Done()
            
            nodeAddr := c.getNodeForSlot(s)
            client := c.getNodeClient(nodeAddr)
            
            values, err := client.MGet(context.Background(), keys...).Result()
            if err != nil {
                errChan <- err
                return
            }
            
            mutex.Lock()
            for i, key := range keys {
                if i < len(values) && values[i] != nil {
                    results[key] = values[i].(string)
                }
            }
            mutex.Unlock()
        }(slot, slotKeys)
    }
    
    wg.Wait()
    close(errChan)
    
    // 检查错误
    if len(errChan) > 0 {
        return nil, <-errChan
    }
    
    return results, nil
}
```
